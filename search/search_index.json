{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Home","text":"The operating system for autonomous systems"},{"location":"index.html#welcome","title":"Welcome","text":"<p>Welcome to the official documentation of the <code>Botnix</code> OS!</p> <p>Please see the navigation sidebar to the left for more information.</p>"},{"location":"index.html#repository","title":"Repository","text":"<p>Botnix on GitHub</p> <p> \"Accelerating the development of robotic general intelligence\"    TM 2024 \u00a9 Nervosys, LLC </p>"},{"location":"build-helpers.html","title":"Build helpers","text":"<p>A build helper is a function that produces derivations.</p> <p>:::{.warning} This is not to be confused with the <code>builder</code> argument of the Nix <code>derivation</code> primitive, which refers to the executable that produces the build result, or remote builder, which refers to a remote  machine that could run such an executable. :::</p> <p>Such a function is usually designed to abstract over a typical workflow for a given programming language or framework. This allows declaring a build recipe by setting a limited number of options relevant to the particular use case instead of using the <code>derivation</code> function directly.</p> <p><code>stdenv.mkDerivation</code> is the most widely used build helper, and serves as a basis for many others. In addition, it offers various options to customize parts of the builds.</p> <p>There is no uniform interface for build helpers. Trivial build helpers and fetchers have various input types for convenience. Language- or framework-specific build helpers usually follow the style of <code>stdenv.mkDerivation</code>, which accepts an attribute set or a fixed-point function taking an attribute set.</p> <p><code>{=include=} chapters build-helpers/fetchers.chapter.md build-helpers/trivial-build-helpers.chapter.md build-helpers/testers.chapter.md build-helpers/special.md build-helpers/images.md hooks/index.md languages-frameworks/index.md packages/index.md</code></p>"},{"location":"contributing.html","title":"Contributing to Nixpkgs","text":"<p><code>{=include=} chapters contributing/quick-start.chapter.md contributing/coding-conventions.chapter.md contributing/submitting-changes.chapter.md contributing/vulnerability-roundup.chapter.md contributing/reviewing-contributions.chapter.md contributing/contributing-to-documentation.chapter.md</code></p>"},{"location":"development.html","title":"Development of Nixpkgs","text":"<p>This section shows you how Nixpkgs is being developed and how you can interact with the contributors and the latest updates. If you are interested in contributing yourself, see CONTRIBUTING.md.</p> <p><code>{=include=} chapters development/opening-issues.chapter.md</code></p>"},{"location":"functions.html","title":"Functions reference","text":"<p>The nixpkgs repository has several utility functions to manipulate Nix expressions.</p> <p><code>{=include=} sections functions/library.md functions/generators.section.md functions/debug.section.md functions/prefer-remote-fetch.section.md functions/nix-gitignore.section.md</code></p>"},{"location":"lib.html","title":"Nixpkgs <code>lib</code>","text":"<p><code>{=include=} chapters functions.md module-system/module-system.chapter.md</code></p>"},{"location":"preface.chapter.html","title":"Preface","text":"<p>The Nix Packages collection (Nixpkgs) is a set of thousands of packages for the Nix package manager, released under a permissive MIT license. Packages are available for several platforms, and can be used with the Nix package manager on most GNU/Linux distributions as well as NixOS.</p> <p>This document is the user reference manual for Nixpkgs. It describes entire public interface of Nixpkgs in a concise and orderly manner, and all relevant behaviors, with examples and cross-references.</p> <p>To discover other kinds of documentation: - nix.dev: Tutorials and guides for getting things done with Nix - NixOS Option Search and reference documentation - Nixpkgs Package Search - NixOS manual: Reference documentation for the NixOS Linux distribution - <code>CONTRIBUTING.md</code>: Contributing to Nixpkgs, including this manual</p>"},{"location":"preface.chapter.html#overview-of-nixpkgs","title":"Overview of Nixpkgs","text":"<p>Nix expressions describe how to build packages from source and are collected in the nixpkgs repository. Also included in the collection are Nix expressions for NixOS modules. With these expressions the Nix package manager can build binary packages.</p> <p>Packages, including the Nix packages collection, are distributed through channels. The collection is distributed for users of Nix on non-NixOS distributions through the channel <code>nixpkgs-unstable</code>. Users of NixOS generally use one of the <code>nixos-*</code> channels, e.g. <code>nixos-22.11</code>, which includes all packages and modules for the stable NixOS 22.11. Stable NixOS releases are generally only given security updates. More up to date packages and modules are available via the <code>nixos-unstable</code> channel.</p> <p>Both <code>nixos-unstable</code> and <code>nixpkgs-unstable</code> follow the <code>master</code> branch of the nixpkgs repository, although both do lag the <code>master</code> branch by generally a couple of days. Updates to a channel are distributed as soon as all tests for that channel pass, e.g. this table shows the status of tests for the <code>nixpkgs-unstable</code> channel.</p> <p>The tests are conducted by a cluster called Hydra, which also builds binary packages from the Nix expressions in Nixpkgs for <code>x86_64-linux</code>, <code>i686-linux</code> and <code>x86_64-darwin</code>. The binaries are made available via a binary cache.</p> <p>The current Nix expressions of the channels are available in the nixpkgs repository in branches that correspond to the channel names (e.g. <code>nixos-22.11-small</code>).</p>"},{"location":"stdenv.html","title":"Standard environment","text":"<p><code>{=include=} chapters stdenv/stdenv.chapter.md stdenv/meta.chapter.md stdenv/multiple-output.chapter.md stdenv/cross-compilation.chapter.md stdenv/platform-notes.chapter.md</code></p>"},{"location":"using-nixpkgs.html","title":"Using Nixpkgs","text":"<p><code>{=include=} chapters using/platform-support.chapter.md using/configuration.chapter.md using/overlays.chapter.md using/overrides.chapter.md</code></p>"},{"location":"build-helpers/fetchers.chapter.html","title":"Fetchers","text":"<p>Building software with Nix often requires downloading source code and other files from the internet. To this end, Nixpkgs provides fetchers: functions to obtain remote sources via various protocols and services.</p> <p>Nixpkgs fetchers differ from built-in fetchers such as <code>builtins.fetchTarball</code>: - A built-in fetcher will download and cache files at evaluation time and produce a store path.   A Nixpkgs fetcher will create a (fixed-output) derivation, and files are downloaded at build time. - Built-in fetchers will invalidate their cache after <code>tarball-ttl</code> expires, and will require network activity to check if the cache entry is up to date.   Nixpkgs fetchers only re-download if the specified hash changes or the store object is not otherwise available. - Built-in fetchers do not use substituters.   Derivations produced by Nixpkgs fetchers will use any configured binary cache transparently.</p> <p>This significantly reduces the time needed to evaluate the entirety of Nixpkgs, and allows Hydra to retain and re-distribute sources used by Nixpkgs in the public binary cache. For these reasons, built-in fetchers are not allowed in Nixpkgs source code.</p> <p>The following table shows an overview of the differences:</p> Fetchers Download Output Cache Re-download when <code>builtins.fetch*</code> evaluation time store path <code>/nix/store</code>, <code>~/.cache/nix</code> <code>tarball-ttl</code> expires, cache miss in <code>~/.cache/nix</code>, output store object not in local store <code>pkgs.fetch*</code> build time derivation <code>/nix/store</code>, substituters output store object not available"},{"location":"build-helpers/fetchers.chapter.html#chap-pkgs-fetchers-caveats","title":"Caveats","text":"<p>The fact that the hash belongs to the Nix derivation output and not the file itself can lead to confusion. For example, consider the following fetcher:</p> <pre><code>fetchurl {\n  url = \"http://www.example.org/hello-1.0.tar.gz\";\n  hash = \"sha256-lTeyxzJNQeMdu1IVdovNMtgn77jRIhSybLdMbTkf2Ww=\";\n};\n</code></pre> <p>A common mistake is to update a fetcher\u2019s URL, or a version parameter, without updating the hash.</p> <pre><code>fetchurl {\n  url = \"http://www.example.org/hello-1.1.tar.gz\";\n  hash = \"sha256-lTeyxzJNQeMdu1IVdovNMtgn77jRIhSybLdMbTkf2Ww=\";\n};\n</code></pre> <p>This will reuse the old contents. Remember to invalidate the hash argument, in this case by setting the <code>hash</code> attribute to an empty string.</p> <pre><code>fetchurl {\n  url = \"http://www.example.org/hello-1.1.tar.gz\";\n  hash = \"\";\n};\n</code></pre> <p>Use the resulting error message to determine the correct hash.</p> <pre><code>error: hash mismatch in fixed-output derivation '/path/to/my.drv':\n         specified: sha256-AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=\n            got:    sha256-lTeyxzJNQeMdu1IVdovNMtgn77jRIhSybLdMbTkf2Ww=\n</code></pre> <p>A similar problem arises while testing changes to a fetcher's implementation. If the output of the derivation already exists in the Nix store, test failures can go undetected. The <code>invalidateFetcherByDrvHash</code> function helps prevent reusing cached derivations.</p>"},{"location":"build-helpers/fetchers.chapter.html#fetchurl","title":"<code>fetchurl</code> and <code>fetchzip</code>","text":"<p>Two basic fetchers are <code>fetchurl</code> and <code>fetchzip</code>. Both of these have two required arguments, a URL and a hash. The hash is typically <code>hash</code>, although many more hash algorithms are supported. Nixpkgs contributors are currently recommended to use <code>hash</code>. This hash will be used by Nix to identify your source. A typical usage of <code>fetchurl</code> is provided below.</p> <pre><code>{ stdenv, fetchurl }:\n\nstdenv.mkDerivation {\n  name = \"hello\";\n  src = fetchurl {\n    url = \"http://www.example.org/hello.tar.gz\";\n    hash = \"sha256-BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB=\";\n  };\n}\n</code></pre> <p>The main difference between <code>fetchurl</code> and <code>fetchzip</code> is in how they store the contents. <code>fetchurl</code> will store the unaltered contents of the URL within the Nix store. <code>fetchzip</code> on the other hand, will decompress the archive for you, making files and directories directly accessible in the future. <code>fetchzip</code> can only be used with archives. Despite the name, <code>fetchzip</code> is not limited to .zip files and can also be used with any tarball.</p>"},{"location":"build-helpers/fetchers.chapter.html#fetchpatch","title":"<code>fetchpatch</code>","text":"<p><code>fetchpatch</code> works very similarly to <code>fetchurl</code> with the same arguments expected. It expects patch files as a source and performs normalization on them before computing the checksum. For example, it will remove comments or other unstable parts that are sometimes added by version control systems and can change over time.</p> <ul> <li><code>relative</code>: Similar to using <code>git-diff</code>'s <code>--relative</code> flag, only keep changes inside the specified directory, making paths relative to it.</li> <li><code>stripLen</code>: Remove the first <code>stripLen</code> components of pathnames in the patch.</li> <li><code>decode</code>: Pipe the downloaded data through this command before processing it as a patch.</li> <li><code>extraPrefix</code>: Prefix pathnames by this string.</li> <li><code>excludes</code>: Exclude files matching these patterns (applies after the above arguments).</li> <li><code>includes</code>: Include only files matching these patterns (applies after the above arguments).</li> <li><code>revert</code>: Revert the patch.</li> </ul> <p>Note that because the checksum is computed after applying these effects, using or modifying these arguments will have no effect unless the <code>hash</code> argument is changed as well.</p> <p>Most other fetchers return a directory rather than a single file.</p>"},{"location":"build-helpers/fetchers.chapter.html#fetchdebianpatch","title":"<code>fetchDebianPatch</code>","text":"<p>A wrapper around <code>fetchpatch</code>, which takes: - <code>patch</code> and <code>hash</code>: the patch's filename,   and its hash after normalization by <code>fetchpatch</code> ; - <code>pname</code>: the Debian source package's name ; - <code>version</code>: the upstream version number ; - <code>debianRevision</code>: the Debian revision number if applicable ; - the <code>area</code> of the Debian archive: <code>main</code> (default), <code>contrib</code>, or <code>non-free</code>.</p> <p>Here is an example of <code>fetchDebianPatch</code> in action:</p> <pre><code>{ lib\n, fetchDebianPatch\n, buildPythonPackage\n}:\n\nbuildPythonPackage rec {\n  pname = \"pysimplesoap\";\n  version = \"1.16.2\";\n  src = ...;\n\n  patches = [\n    (fetchDebianPatch {\n      inherit pname version;\n      debianRevision = \"5\";\n      name = \"Add-quotes-to-SOAPAction-header-in-SoapClient.patch\";\n      hash = \"sha256-xA8Wnrpr31H8wy3zHSNfezFNjUJt1HbSXn3qUMzeKc0=\";\n    })\n  ];\n\n  ...\n}\n</code></pre> <p>Patches are fetched from <code>sources.debian.org</code>, and so must come from a package version that was uploaded to the Debian archive.  Packages may be removed from there once that specific version isn't in any suite anymore (stable, testing, unstable, etc.), so maintainers should use <code>copy-tarballs.pl</code> to archive the patch if it needs to be available longer-term.</p>"},{"location":"build-helpers/fetchers.chapter.html#fetchsvn","title":"<code>fetchsvn</code>","text":"<p>Used with Subversion. Expects <code>url</code> to a Subversion directory, <code>rev</code>, and <code>hash</code>.</p>"},{"location":"build-helpers/fetchers.chapter.html#fetchgit","title":"<code>fetchgit</code>","text":"<p>Used with Git. Expects <code>url</code> to a Git repo, <code>rev</code>, and <code>hash</code>. <code>rev</code> in this case can be full the git commit id (SHA1 hash) or a tag name like <code>refs/tags/v1.0</code>.</p> <p>Additionally, the following optional arguments can be given: <code>fetchSubmodules = true</code> makes <code>fetchgit</code> also fetch the submodules of a repository. If <code>deepClone</code> is set to true, the entire repository is cloned as opposing to just creating a shallow clone. <code>deepClone = true</code> also implies <code>leaveDotGit = true</code> which means that the <code>.git</code> directory of the clone won't be removed after checkout.</p> <p>If only parts of the repository are needed, <code>sparseCheckout</code> can be used. This will prevent git from fetching unnecessary blobs from server, see git sparse-checkout for more information:</p> <pre><code>{ stdenv, fetchgit }:\n\nstdenv.mkDerivation {\n  name = \"hello\";\n  src = fetchgit {\n    url = \"https://...\";\n    sparseCheckout = [\n      \"directory/to/be/included\"\n      \"another/directory\"\n    ];\n    hash = \"sha256-AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=\";\n  };\n}\n</code></pre>"},{"location":"build-helpers/fetchers.chapter.html#fetchfossil","title":"<code>fetchfossil</code>","text":"<p>Used with Fossil. Expects <code>url</code> to a Fossil archive, <code>rev</code>, and <code>hash</code>.</p>"},{"location":"build-helpers/fetchers.chapter.html#fetchcvs","title":"<code>fetchcvs</code>","text":"<p>Used with CVS. Expects <code>cvsRoot</code>, <code>tag</code>, and <code>hash</code>.</p>"},{"location":"build-helpers/fetchers.chapter.html#fetchhg","title":"<code>fetchhg</code>","text":"<p>Used with Mercurial. Expects <code>url</code>, <code>rev</code>, and <code>hash</code>.</p> <p>A number of fetcher functions wrap part of <code>fetchurl</code> and <code>fetchzip</code>. They are mainly convenience functions intended for commonly used destinations of source code in Nixpkgs. These wrapper fetchers are listed below.</p>"},{"location":"build-helpers/fetchers.chapter.html#fetchfromgitea","title":"<code>fetchFromGitea</code>","text":"<p><code>fetchFromGitea</code> expects five arguments. <code>domain</code> is the gitea server name. <code>owner</code> is a string corresponding to the Gitea user or organization that controls this repository. <code>repo</code> corresponds to the name of the software repository. These are located at the top of every Gitea HTML page as <code>owner</code>/<code>repo</code>. <code>rev</code> corresponds to the Git commit hash or tag (e.g <code>v1.0</code>) that will be downloaded from Git. Finally, <code>hash</code> corresponds to the hash of the extracted directory. Again, other hash algorithms are also available but <code>hash</code> is currently preferred.</p>"},{"location":"build-helpers/fetchers.chapter.html#fetchfromgithub","title":"<code>fetchFromGitHub</code>","text":"<p><code>fetchFromGitHub</code> expects four arguments. <code>owner</code> is a string corresponding to the GitHub user or organization that controls this repository. <code>repo</code> corresponds to the name of the software repository. These are located at the top of every GitHub HTML page as <code>owner</code>/<code>repo</code>. <code>rev</code> corresponds to the Git commit hash or tag (e.g <code>v1.0</code>) that will be downloaded from Git. Finally, <code>hash</code> corresponds to the hash of the extracted directory. Again, other hash algorithms are also available, but <code>hash</code> is currently preferred.</p> <p>To use a different GitHub instance, use <code>githubBase</code> (defaults to <code>\"github.com\"</code>).</p> <p><code>fetchFromGitHub</code> uses <code>fetchzip</code> to download the source archive generated by GitHub for the specified revision. If <code>leaveDotGit</code>, <code>deepClone</code> or <code>fetchSubmodules</code> are set to <code>true</code>, <code>fetchFromGitHub</code> will use <code>fetchgit</code> instead. Refer to its section for documentation of these options.</p>"},{"location":"build-helpers/fetchers.chapter.html#fetchfromgitlab","title":"<code>fetchFromGitLab</code>","text":"<p>This is used with GitLab repositories. It behaves similarly to <code>fetchFromGitHub</code>, and expects <code>owner</code>, <code>repo</code>, <code>rev</code>, and <code>hash</code>.</p> <p>To use a specific GitLab instance, use <code>domain</code> (defaults to <code>\"gitlab.com\"</code>).</p>"},{"location":"build-helpers/fetchers.chapter.html#fetchfromgitiles","title":"<code>fetchFromGitiles</code>","text":"<p>This is used with Gitiles repositories. The arguments expected are similar to <code>fetchgit</code>.</p>"},{"location":"build-helpers/fetchers.chapter.html#fetchfrombitbucket","title":"<code>fetchFromBitbucket</code>","text":"<p>This is used with BitBucket repositories. The arguments expected are very similar to <code>fetchFromGitHub</code> above.</p>"},{"location":"build-helpers/fetchers.chapter.html#fetchfromsavannah","title":"<code>fetchFromSavannah</code>","text":"<p>This is used with Savannah repositories. The arguments expected are very similar to <code>fetchFromGitHub</code> above.</p>"},{"location":"build-helpers/fetchers.chapter.html#fetchfromrepoorcz","title":"<code>fetchFromRepoOrCz</code>","text":"<p>This is used with repo.or.cz repositories. The arguments expected are very similar to <code>fetchFromGitHub</code> above.</p>"},{"location":"build-helpers/fetchers.chapter.html#fetchfromsourcehut","title":"<code>fetchFromSourcehut</code>","text":"<p>This is used with sourcehut repositories. Similar to <code>fetchFromGitHub</code> above, it expects <code>owner</code>, <code>repo</code>, <code>rev</code> and <code>hash</code>, but don't forget the tilde (~) in front of the username! Expected arguments also include <code>vc</code> (\"git\" (default) or \"hg\"), <code>domain</code> and <code>fetchSubmodules</code>.</p> <p>If <code>fetchSubmodules</code> is <code>true</code>, <code>fetchFromSourcehut</code> uses <code>fetchgit</code> or <code>fetchhg</code> with <code>fetchSubmodules</code> or <code>fetchSubrepos</code> set to <code>true</code>, respectively. Otherwise, the fetcher uses <code>fetchzip</code>.</p>"},{"location":"build-helpers/fetchers.chapter.html#requirefile","title":"<code>requireFile</code>","text":"<p><code>requireFile</code> allows requesting files that cannot be fetched automatically, but whose content is known. This is a useful last-resort workaround for license restrictions that prohibit redistribution, or for downloads that are only accessible after authenticating interactively in a browser. If the requested file is present in the Nix store, the resulting derivation will not be built, because its expected output is already available. Otherwise, the builder will run, but fail with a message explaining to the user how to provide the file. The following code, for example:</p> <p><pre><code>requireFile {\n  name = \"jdk-${version}_linux-x64_bin.tar.gz\";\n  url = \"https://www.oracle.com/java/technologies/javase-jdk11-downloads.html\";\n  hash = \"sha256-lL00+F7jjT71nlKJ7HRQuUQ7kkxVYlZh//5msD8sjeI=\";\n}\n</code></pre> results in this error message: <pre><code>***\nUnfortunately, we cannot download file jdk-11.0.10_linux-x64_bin.tar.gz automatically.\nPlease go to https://www.oracle.com/java/technologies/javase-jdk11-downloads.html to download it yourself, and add it to the Nix store\nusing either\n  nix-store --add-fixed sha256 jdk-11.0.10_linux-x64_bin.tar.gz\nor\n  nix-prefetch-url --type sha256 file:///path/to/jdk-11.0.10_linux-x64_bin.tar.gz\n\n***\n</code></pre></p>"},{"location":"build-helpers/fetchers.chapter.html#fetchtorrent","title":"<code>fetchtorrent</code>","text":"<p><code>fetchtorrent</code> expects two arguments. <code>url</code> which can either be a Magnet URI (Magnet Link) such as <code>magnet:?xt=urn:btih:dd8255ecdc7ca55fb0bbf81323d87062db1f6d1c</code> or an HTTP URL pointing to a <code>.torrent</code> file. It can also take a <code>config</code> argument which will craft a <code>settings.json</code> configuration file and give it to <code>transmission</code>, the underlying program that is performing the fetch. The available config options for <code>transmission</code> can be found here</p> <pre><code>{ fetchtorrent }:\n\nfetchtorrent {\n  config = { peer-limit-global = 100; };\n  url = \"magnet:?xt=urn:btih:dd8255ecdc7ca55fb0bbf81323d87062db1f6d1c\";\n  sha256 = \"\";\n}\n</code></pre>"},{"location":"build-helpers/fetchers.chapter.html#fetchtorrent-parameters","title":"Parameters","text":"<ul> <li> <p><code>url</code>: Magnet URI (Magnet Link) such as <code>magnet:?xt=urn:btih:dd8255ecdc7ca55fb0bbf81323d87062db1f6d1c</code> or an HTTP URL pointing to a <code>.torrent</code> file.</p> </li> <li> <p><code>backend</code>: Which bittorrent program to use. Default: <code>\"transmission\"</code>. Valid values are <code>\"rqbit\"</code> or <code>\"transmission\"</code>. These are the two most suitable torrent clients for fetching in a fixed-output derivation at the time of writing, as they can be easily exited after usage. <code>rqbit</code> is written in Rust and has a smaller closure size than <code>transmission</code>, and the performance and peer discovery properties differs between these clients, requiring experimentation to decide upon which is the best.</p> </li> <li> <p><code>config</code>: When using <code>transmission</code> as the <code>backend</code>, a json configuration can   be supplied to transmission. Refer to the upstream documentation for information on how to configure.</p> </li> </ul>"},{"location":"build-helpers/images.html","title":"Images","text":"<p>This chapter describes tools for creating various types of images.</p> <p><code>{=include=} sections images/appimagetools.section.md images/dockertools.section.md images/ocitools.section.md images/snaptools.section.md images/portableservice.section.md images/makediskimage.section.md images/binarycache.section.md</code></p>"},{"location":"build-helpers/special.html","title":"Special build helpers","text":"<p>This chapter describes several special build helpers.</p> <p><code>{=include=} sections special/fakenss.section.md special/fhs-environments.section.md special/makesetuphook.section.md special/mkshell.section.md special/vm-tools.section.md special/checkpoint-build.section.md</code></p>"},{"location":"build-helpers/testers.chapter.html","title":"Testers","text":"<p>This chapter describes several testing builders which are available in the <code>testers</code> namespace.</p>"},{"location":"build-helpers/testers.chapter.html#tester-hasPkgConfigModules","title":"<code>hasPkgConfigModules</code>","text":"<p>[]{#tester-hasPkgConfigModule} Checks whether a package exposes a given list of <code>pkg-config</code> modules. If the <code>moduleNames</code> argument is omitted, <code>hasPkgConfigModules</code> will use <code>meta.pkgConfigModules</code>.</p> <p>:::{.example #ex-haspkgconfigmodules-defaultvalues}</p>"},{"location":"build-helpers/testers.chapter.html#check-that-pkg-config-modules-are-exposed-using-default-values","title":"Check that <code>pkg-config</code> modules are exposed using default values","text":"<pre><code>passthru.tests.pkg-config = testers.hasPkgConfigModules {\n  package = finalAttrs.finalPackage;\n};\n\nmeta.pkgConfigModules = [ \"libfoo\" ];\n</code></pre> <p>:::</p> <p>:::{.example #ex-haspkgconfigmodules-explicitmodules}</p>"},{"location":"build-helpers/testers.chapter.html#check-that-pkg-config-modules-are-exposed-using-explicit-module-names","title":"Check that <code>pkg-config</code> modules are exposed using explicit module names","text":"<pre><code>passthru.tests.pkg-config = testers.hasPkgConfigModules {\n  package = finalAttrs.finalPackage;\n  moduleNames = [ \"libfoo\" ];\n};\n</code></pre> <p>:::</p>"},{"location":"build-helpers/testers.chapter.html#tester-testVersion","title":"<code>testVersion</code>","text":"<p>Checks that the output from running a command contains the specified version string in it as a whole word.</p> <p>Although simplistic, this test assures that the main program can run. While there's no substitute for a real test case, it does catch dynamic linking errors and such. It also provides some protection against accidentally building the wrong version, for example when using an \"old\" hash in a fixed-output derivation.</p> <p>By default, the command to be run will be inferred from the given <code>package</code> attribute: it will check <code>meta.mainProgram</code> first, and fall back to <code>pname</code> or <code>name</code>. The default argument to the command is <code>--version</code>, and the version to be checked will be inferred from the given <code>package</code> attribute as well.</p> <p>:::{.example #ex-testversion-hello}</p>"},{"location":"build-helpers/testers.chapter.html#check-a-program-version-using-all-the-default-values","title":"Check a program version using all the default values","text":"<p>This example will run the command <code>hello --version</code>, and then check that the version of the <code>hello</code> package is in the output of the command.</p> <pre><code>passthru.tests.version = testers.testVersion { package = hello; };\n</code></pre> <p>:::</p> <p>:::{.example #ex-testversion-different-commandversion}</p>"},{"location":"build-helpers/testers.chapter.html#check-the-program-version-using-a-specified-command-and-expected-version-string","title":"Check the program version using a specified command and expected version string","text":"<p>This example will run the command <code>leetcode -V</code>, and then check that <code>leetcode 0.4.2</code> is in the output of the command as a whole word (separated by whitespaces). This means that an output like \"leetcode 0.4.21\" would fail the tests, and an output like \"You're running leetcode 0.4.2\" would pass the tests.</p> <p>A common usage of the <code>version</code> attribute is to specify <code>version = \"v${version}\"</code>.</p> <pre><code>version = \"0.4.2\";\n\npassthru.tests.version = testers.testVersion {\n  package = leetcode-cli;\n  command = \"leetcode -V\";\n  version = \"leetcode ${version}\";\n};\n</code></pre> <p>:::</p>"},{"location":"build-helpers/testers.chapter.html#tester-testBuildFailure","title":"<code>testBuildFailure</code>","text":"<p>Make sure that a build does not succeed. This is useful for testing testers.</p> <p>This returns a derivation with an override on the builder, with the following effects:</p> <ul> <li>Fail the build when the original builder succeeds</li> <li>Move <code>$out</code> to <code>$out/result</code>, if it exists (assuming <code>out</code> is the default output)</li> <li>Save the build log to <code>$out/testBuildFailure.log</code> (same)</li> </ul> <p>While <code>testBuildFailure</code> is designed to keep changes to the original builder's environment to a minimum, some small changes are inevitable:</p> <ul> <li>The file <code>$TMPDIR/testBuildFailure.log</code> is present. It should not be deleted.</li> <li><code>stdout</code> and <code>stderr</code> are a pipe instead of a tty. This could be improved.</li> <li>One or two extra processes are present in the sandbox during the original builder's execution.</li> <li>The derivation and output hashes are different, but not unusual.</li> <li>The derivation includes a dependency on <code>buildPackages.bash</code> and <code>expect-failure.sh</code>, which is built to include a transitive dependency on <code>buildPackages.coreutils</code> and possibly more.    These are not added to <code>PATH</code> or any other environment variable, so they should be hard to observe.</li> </ul> <p>:::{.example #ex-testBuildFailure-showingenvironmentchanges}</p>"},{"location":"build-helpers/testers.chapter.html#check-that-a-build-fails-and-verify-the-changes-made-during-build","title":"Check that a build fails, and verify the changes made during build","text":"<pre><code>runCommand \"example\" {\n  failed = testers.testBuildFailure (runCommand \"fail\" {} ''\n    echo ok-ish &gt;$out\n    echo failing though\n    exit 3\n  '');\n} ''\n  grep -F 'ok-ish' $failed/result\n  grep -F 'failing though' $failed/testBuildFailure.log\n  [[ 3 = $(cat $failed/testBuildFailure.exit) ]]\n  touch $out\n'';\n</code></pre> <p>:::</p>"},{"location":"build-helpers/testers.chapter.html#tester-equalContents","title":"<code>testEqualContents</code>","text":"<p>Check that two paths have the same contents.</p> <p>:::{.example #ex-testEqualContents-toyexample}</p>"},{"location":"build-helpers/testers.chapter.html#check-that-two-paths-have-the-same-contents","title":"Check that two paths have the same contents","text":"<pre><code>testers.testEqualContents {\n  assertion = \"sed -e performs replacement\";\n  expected = writeText \"expected\" ''\n    foo baz baz\n  '';\n  actual = runCommand \"actual\" {\n    # not really necessary for a package that's in stdenv\n    nativeBuildInputs = [ gnused ];\n    base = writeText \"base\" ''\n      foo bar baz\n    '';\n  } ''\n    sed -e 's/bar/baz/g' $base &gt;$out\n  '';\n}\n</code></pre> <p>:::</p>"},{"location":"build-helpers/testers.chapter.html#tester-testEqualDerivation","title":"<code>testEqualDerivation</code>","text":"<p>Checks that two packages produce the exact same build instructions.</p> <p>This can be used to make sure that a certain difference of configuration, such as the presence of an overlay does not cause a cache miss.</p> <p>When the derivations are equal, the return value is an empty file. Otherwise, the build log explains the difference via <code>nix-diff</code>.</p> <p>:::{.example #ex-testEqualDerivation-hello}</p>"},{"location":"build-helpers/testers.chapter.html#check-that-two-packages-produce-the-same-derivation","title":"Check that two packages produce the same derivation","text":"<pre><code>testers.testEqualDerivation\n  \"The hello package must stay the same when enabling checks.\"\n  hello\n  (hello.overrideAttrs(o: { doCheck = true; }))\n</code></pre> <p>:::</p>"},{"location":"build-helpers/testers.chapter.html#tester-invalidateFetcherByDrvHash","title":"<code>invalidateFetcherByDrvHash</code>","text":"<p>Use the derivation hash to invalidate the output via name, for testing.</p> <p>Type: <code>(a@{ name, ... } -&gt; Derivation) -&gt; a -&gt; Derivation</code></p> <p>Normally, fixed output derivations can and should be cached by their output hash only, but for testing we want to re-fetch everytime the fetcher changes.</p> <p>Changes to the fetcher become apparent in the drvPath, which is a hash of how to fetch, rather than a fixed store path. By inserting this hash into the name, we can make sure to re-run the fetcher every time the fetcher changes.</p> <p>This relies on the assumption that Nix isn't clever enough to reuse its database of local store contents to optimize fetching.</p> <p>You might notice that the \"salted\" name derives from the normal invocation, not the final derivation. <code>invalidateFetcherByDrvHash</code> has to invoke the fetcher function twice: once to get a derivation hash, and again to produce the final fixed output derivation.</p> <p>:::{.example #ex-invalidateFetcherByDrvHash-nix}</p>"},{"location":"build-helpers/testers.chapter.html#prevent-nix-from-reusing-the-output-of-a-fetcher","title":"Prevent nix from reusing the output of a fetcher","text":"<pre><code>tests.fetchgit = testers.invalidateFetcherByDrvHash fetchgit {\n  name = \"nix-source\";\n  url = \"https://github.com/NixOS/nix\";\n  rev = \"9d9dbe6ed05854e03811c361a3380e09183f4f4a\";\n  hash = \"sha256-7DszvbCNTjpzGRmpIVAWXk20P0/XTrWZ79KSOGLrUWY=\";\n};\n</code></pre> <p>:::</p>"},{"location":"build-helpers/testers.chapter.html#tester-runNixOSTest","title":"<code>runNixOSTest</code>","text":"<p>A helper function that behaves exactly like the NixOS <code>runTest</code>, except it also assigns this Nixpkgs package set as the <code>pkgs</code> of the test and makes the <code>nixpkgs.*</code> options read-only.</p> <p>If your test is part of the Nixpkgs repository, or if you need a more general entrypoint, see \"Calling a test\" in the NixOS manual.</p> <p>:::{.example #ex-runNixOSTest-hello}</p>"},{"location":"build-helpers/testers.chapter.html#run-a-nixos-test-using-runnixostest","title":"Run a NixOS test using <code>runNixOSTest</code>","text":"<pre><code>pkgs.testers.runNixOSTest ({ lib, ... }: {\n  name = \"hello\";\n  nodes.machine = { pkgs, ... }: {\n    environment.systemPackages = [ pkgs.hello ];\n  };\n  testScript = ''\n    machine.succeed(\"hello\")\n  '';\n})\n</code></pre> <p>:::</p>"},{"location":"build-helpers/testers.chapter.html#tester-nixosTest","title":"<code>nixosTest</code>","text":"<p>Run a NixOS VM network test using this evaluation of Nixpkgs.</p> <p>NOTE: This function is primarily for external use. NixOS itself uses <code>make-test-python.nix</code> directly. Packages defined in Nixpkgs reuse NixOS tests via <code>nixosTests</code>, plural.</p> <p>It is mostly equivalent to the function <code>import ./make-test-python.nix</code> from the NixOS manual, except that the current application of Nixpkgs (<code>pkgs</code>) will be used, instead of letting NixOS invoke Nixpkgs anew.</p> <p>If a test machine needs to set NixOS options under <code>nixpkgs</code>, it must set only the <code>nixpkgs.pkgs</code> option.</p>"},{"location":"build-helpers/testers.chapter.html#tester-nixosTest-parameter","title":"Parameter","text":"<p>A NixOS VM test network, or path to it. Example:</p> <pre><code>{\n  name = \"my-test\";\n  nodes = {\n    machine1 = { lib, pkgs, nodes, ... }: {\n      environment.systemPackages = [ pkgs.hello ];\n      services.foo.enable = true;\n    };\n    # machine2 = ...;\n  };\n  testScript = ''\n    start_all()\n    machine1.wait_for_unit(\"foo.service\")\n    machine1.succeed(\"hello | foo-send\")\n  '';\n}\n</code></pre>"},{"location":"build-helpers/testers.chapter.html#tester-nixosTest-result","title":"Result","text":"<p>A derivation that runs the VM test.</p> <p>Notable attributes:</p> <ul> <li> <p><code>nodes</code>: the evaluated NixOS configurations. Useful for debugging and exploring the configuration.</p> </li> <li> <p><code>driverInteractive</code>: a script that launches an interactive Python session in the context of the <code>testScript</code>.</p> </li> </ul>"},{"location":"build-helpers/trivial-build-helpers.chapter.html","title":"Trivial build helpers","text":"<p>Nixpkgs provides a variety of wrapper functions that help build commonly useful derivations. Like <code>stdenv.mkDerivation</code>, each of these build helpers creates a derivation, but the arguments passed are different (usually simpler) from those required by <code>stdenv.mkDerivation</code>.</p>"},{"location":"build-helpers/trivial-build-helpers.chapter.html#trivial-builder-runCommand","title":"<code>runCommand</code>","text":"<p><code>runCommand :: String -&gt; AttrSet -&gt; String -&gt; Derivation</code></p> <p><code>runCommand name drvAttrs buildCommand</code> returns a derivation that is built by running the specified shell commands.</p> <p><code>name :: String</code> :   The name that Nix will append to the store path in the same way that <code>stdenv.mkDerivation</code> uses its <code>name</code> attribute.</p> <p><code>drvAttr :: AttrSet</code> :   Attributes to pass to the underlying call to <code>stdenv.mkDerivation</code>.</p> <p><code>buildCommand :: String</code> :   Shell commands to run in the derivation builder.</p> <pre><code>::: {.note}\nYou have to create a file or directory `$out` for Nix to be able to run the builder successfully.\n:::\n</code></pre> <p>::: {.example #ex-runcommand-simple}</p>"},{"location":"build-helpers/trivial-build-helpers.chapter.html#invocation-of-runcommand","title":"Invocation of <code>runCommand</code>","text":"<p><pre><code>(import &lt;nixpkgs&gt; {}).runCommand \"my-example\" {} ''\n  echo My example command is running\n\n  mkdir $out\n\n  echo I can write data to the Nix store &gt; $out/message\n\n  echo I can also run basic commands like:\n\n  echo ls\n  ls\n\n  echo whoami\n  whoami\n\n  echo date\n  date\n''\n</code></pre> :::</p>"},{"location":"build-helpers/trivial-build-helpers.chapter.html#trivial-builder-runCommandCC","title":"<code>runCommandCC</code>","text":"<p>This works just like <code>runCommand</code>. The only difference is that it also provides a C compiler in <code>buildCommand</code>'s environment. To minimize your dependencies, you should only use this if you are sure you will need a C compiler as part of running your command.</p>"},{"location":"build-helpers/trivial-build-helpers.chapter.html#trivial-builder-runCommandLocal","title":"<code>runCommandLocal</code>","text":"<p>Variant of <code>runCommand</code> that forces the derivation to be built locally, it is not substituted. This is intended for very cheap commands (&lt;1s execution time). It saves on the network round-trip and can speed up a build.</p> <p>::: {.note} This sets <code>allowSubstitutes</code> to <code>false</code>, so only use <code>runCommandLocal</code> if you are certain the user will always have a builder for the <code>system</code> of the derivation. This should be true for most trivial use cases (e.g., just copying some files to a different location or adding symlinks) because there the <code>system</code> is usually the same as <code>builtins.currentSystem</code>. :::</p>"},{"location":"build-helpers/trivial-build-helpers.chapter.html#trivial-builder-text-writing","title":"Writing text files","text":"<p>Nixpkgs provides the following functions for producing derivations which write text files or executable scripts into the Nix store. They are useful for creating files from Nix expression, and are all implemented as convenience wrappers around <code>writeTextFile</code>.</p> <p>Each of these functions will cause a derivation to be produced. When you coerce the result of each of these functions to a string with string interpolation or <code>builtins.toString</code>, it will evaluate to the store path of this derivation.</p> <p>:::: {.note} Some of these functions will put the resulting files within a directory inside the derivation output. If you need to refer to the resulting files somewhere else in a Nix expression, append their path to the derivation's store path.</p> <p>For example, if the file destination is a directory:</p> <pre><code>my-file = writeTextFile {\n  name = \"my-file\";\n  text = ''\n    Contents of File\n  '';\n  destination = \"/share/my-file\";\n}\n</code></pre> <p>Remember to append \"/share/my-file\" to the resulting store path when using it elsewhere:</p> <p><pre><code>writeShellScript \"evaluate-my-file.sh\" ''\n  cat ${my-file}/share/my-file\n'';\n</code></pre> ::::</p>"},{"location":"build-helpers/trivial-build-helpers.chapter.html#trivial-builder-writeTextFile","title":"<code>writeTextFile</code>","text":"<p>Write a text file to the Nix store.</p> <p><code>writeTextFile</code> takes an attribute set with the following possible attributes:</p> <p><code>name</code> (String)</p> <p>: Corresponds to the name used in the Nix store path identifier.</p> <p><code>text</code> (String)</p> <p>: The contents of the file.</p> <p><code>executable</code> (Bool, optional)</p> <p>: Make this file have the executable bit set.</p> <p>Default: <code>false</code></p> <p><code>destination</code> (String, optional)</p> <p>: A subpath under the derivation's output path into which to put the file.   Subdirectories are created automatically when the derivation is realised.</p> <p>By default, the store path itself will be a file containing the text contents.</p> <p>Default: <code>\"\"</code></p> <p><code>checkPhase</code> (String, optional)</p> <p>: Commands to run after generating the file.</p> <p>Default: <code>\"\"</code></p> <p><code>meta</code> (Attribute set, optional)</p> <p>: Additional metadata for the derivation.</p> <p>Default: <code>{}</code></p> <p><code>allowSubstitutes</code> (Bool, optional)</p> <p>: Whether to allow substituting from a binary cache.   Passed through to <code>allowSubsitutes</code> of the underlying call to <code>builtins.derivation</code>.</p> <p>It defaults to <code>false</code>, as running the derivation's simple <code>builder</code> executable locally is assumed to be faster than network operations.   Set it to true if the <code>checkPhase</code> step is expensive.</p> <p>Default: <code>false</code></p> <p><code>preferLocalBuild</code> (Bool, optional)</p> <p>: Whether to prefer building locally, even if faster remote build machines are available.</p> <p>Passed through to <code>preferLocalBuild</code> of the underlying call to <code>builtins.derivation</code>.</p> <p>It defaults to <code>true</code> for the same reason <code>allowSubstitutes</code> defaults to <code>false</code>.</p> <p>Default: <code>true</code></p> <p>The resulting store path will include some variation of the name, and it will be a file unless <code>destination</code> is used, in which case it will be a directory.</p> <p>::: {.example #ex-writeTextFile}</p>"},{"location":"build-helpers/trivial-build-helpers.chapter.html#usage-1-of-writetextfile","title":"Usage 1 of <code>writeTextFile</code>","text":"<p>Write <code>my-file</code> to <code>/nix/store/&lt;store path&gt;/some/subpath/my-cool-script</code>, making it executable. Also run a check on the resulting file in a <code>checkPhase</code>, and supply values for the less-used options.</p> <p><pre><code>writeTextFile {\n  name = \"my-cool-script\";\n  text = ''\n    #!/bin/sh\n    echo \"This is my cool script!\"\n  '';\n  executable = true;\n  destination = \"/some/subpath/my-cool-script\";\n  checkPhase = ''\n    ${pkgs.shellcheck}/bin/shellcheck $out/some/subpath/my-cool-script\n  '';\n  meta = {\n    license = pkgs.lib.licenses.cc0;\n  };\n  allowSubstitutes = true;\n  preferLocalBuild = false;\n};\n</code></pre> :::</p> <p>::: {.example #ex2-writeTextFile}</p>"},{"location":"build-helpers/trivial-build-helpers.chapter.html#usage-2-of-writetextfile","title":"Usage 2 of <code>writeTextFile</code>","text":"<p>Write the string <code>Contents of File</code> to <code>/nix/store/&lt;store path&gt;</code>. See also the  helper function.</p> <p><pre><code>writeTextFile {\n  name = \"my-file\";\n  text = ''\n    Contents of File\n  '';\n}\n</code></pre> :::</p> <p>::: {.example #ex3-writeTextFile}</p>"},{"location":"build-helpers/trivial-build-helpers.chapter.html#usage-3-of-writetextfile","title":"Usage 3 of <code>writeTextFile</code>","text":"<p>Write an executable script <code>my-script</code> to <code>/nix/store/&lt;store path&gt;/bin/my-script</code>. See also the  helper function.</p> <p><pre><code>writeTextFile {\n  name = \"my-script\";\n  text = ''\n    echo \"hi\"\n  '';\n  executable = true;\n  destination = \"/bin/my-script\";\n}\n</code></pre> :::</p>"},{"location":"build-helpers/trivial-build-helpers.chapter.html#trivial-builder-writeText","title":"<code>writeText</code>","text":"<p>Write a text file to the Nix store</p> <p><code>writeText</code> takes the following arguments: a string.</p> <p><code>name</code> (String)</p> <p>: The name used in the Nix store path.</p> <p><code>text</code> (String)</p> <p>: The contents of the file.</p> <p>The store path will include the name, and it will be a file.</p> <p>::: {.example #ex-writeText}</p>"},{"location":"build-helpers/trivial-build-helpers.chapter.html#usage-of-writetext","title":"Usage of <code>writeText</code>","text":"<p>Write the string <code>Contents of File</code> to <code>/nix/store/&lt;store path&gt;</code>:</p> <p><pre><code>writeText \"my-file\"\n  ''\n  Contents of File\n  '';\n</code></pre> :::</p> <p>This is equivalent to:</p> <pre><code>writeTextFile {\n  name = \"my-file\";\n  text = ''\n    Contents of File\n  '';\n}\n</code></pre>"},{"location":"build-helpers/trivial-build-helpers.chapter.html#trivial-builder-writeTextDir","title":"<code>writeTextDir</code>","text":"<p>Write a text file within a subdirectory of the Nix store.</p> <p><code>writeTextDir</code> takes the following arguments:</p> <p><code>path</code> (String)</p> <p>: The destination within the Nix store path under which to create the file.</p> <p><code>text</code> (String)</p> <p>: The contents of the file.</p> <p>The store path will be a directory.</p> <p>::: {.example #ex-writeTextDir}</p>"},{"location":"build-helpers/trivial-build-helpers.chapter.html#usage-of-writetextdir","title":"Usage of <code>writeTextDir</code>","text":"<p>Write the string <code>Contents of File</code> to <code>/nix/store/&lt;store path&gt;/share/my-file</code>:</p> <p><pre><code>writeTextDir \"share/my-file\"\n  ''\n  Contents of File\n  '';\n</code></pre> :::</p> <p>This is equivalent to:</p> <pre><code>writeTextFile {\n  name = \"my-file\";\n  text = ''\n    Contents of File\n  '';\n  destination = \"share/my-file\";\n}\n</code></pre>"},{"location":"build-helpers/trivial-build-helpers.chapter.html#trivial-builder-writeScript","title":"<code>writeScript</code>","text":"<p>Write an executable script file to the Nix store.</p> <p><code>writeScript</code> takes the following arguments:</p> <p><code>name</code> (String)</p> <p>: The name used in the Nix store path.</p> <p><code>text</code> (String)</p> <p>: The contents of the file.</p> <p>The created file is marked as executable. The store path will include the name, and it will be a file.</p> <p>::: {.example #ex-writeScript}</p>"},{"location":"build-helpers/trivial-build-helpers.chapter.html#usage-of-writescript","title":"Usage of <code>writeScript</code>","text":"<p>Write the string <code>Contents of File</code> to <code>/nix/store/&lt;store path&gt;</code> and make the file executable.</p> <p><pre><code>writeScript \"my-file\"\n  ''\n  Contents of File\n  '';\n</code></pre> :::</p> <p>This is equivalent to:</p> <pre><code>writeTextFile {\n  name = \"my-file\";\n  text = ''\n    Contents of File\n  '';\n  executable = true;\n}\n</code></pre>"},{"location":"build-helpers/trivial-build-helpers.chapter.html#trivial-builder-writeScriptBin","title":"<code>writeScriptBin</code>","text":"<p>Write a script within a <code>bin</code> subirectory of a directory in the Nix store. This is for consistency with the convention of software packages placing executables under <code>bin</code>.</p> <p><code>writeScriptBin</code> takes the following arguments:</p> <p><code>name</code> (String)</p> <p>: The name used in the Nix store path and within the file created under the store path.</p> <p><code>text</code> (String)</p> <p>: The contents of the file.</p> <p>The created file is marked as executable. The file's contents will be put into <code>/nix/store/&lt;store path&gt;/bin/&lt;name&gt;</code>. The store path will include the the name, and it will be a directory.</p> <p>::: {.example #ex-writeScriptBin}</p>"},{"location":"build-helpers/trivial-build-helpers.chapter.html#usage-of-writescriptbin","title":"Usage of <code>writeScriptBin</code>","text":"<p><pre><code>writeScriptBin \"my-script\"\n  ''\n  echo \"hi\"\n  '';\n</code></pre> :::</p> <p>This is equivalent to:</p> <pre><code>writeTextFile {\n  name = \"my-script\";\n  text = ''\n    echo \"hi\"\n  '';\n  executable = true;\n  destination = \"bin/my-script\"\n}\n</code></pre>"},{"location":"build-helpers/trivial-build-helpers.chapter.html#trivial-builder-writeShellScript","title":"<code>writeShellScript</code>","text":"<p>Write a Bash script to the store.</p> <p><code>writeShellScript</code> takes the following arguments:</p> <p><code>name</code> (String)</p> <p>: The name used in the Nix store path.</p> <p><code>text</code> (String)</p> <p>: The contents of the file.</p> <p>The created file is marked as executable. The store path will include the name, and it will be a file.</p> <p>This function is almost exactly like , except that it prepends to the file a shebang line that points to the version of Bash used in Nixpkgs.</p> <p>::: {.example #ex-writeShellScript}</p>"},{"location":"build-helpers/trivial-build-helpers.chapter.html#usage-of-writeshellscript","title":"Usage of <code>writeShellScript</code>","text":"<p><pre><code>writeShellScript \"my-script\"\n  ''\n  echo \"hi\"\n  '';\n</code></pre> :::</p> <p>This is equivalent to:</p> <pre><code>writeTextFile {\n  name = \"my-script\";\n  text = ''\n    #! ${pkgs.runtimeShell}\n    echo \"hi\"\n  '';\n  executable = true;\n}\n</code></pre>"},{"location":"build-helpers/trivial-build-helpers.chapter.html#trivial-builder-writeShellScriptBin","title":"<code>writeShellScriptBin</code>","text":"<p>Write a Bash script to a \"bin\" subdirectory of a directory in the Nix store.</p> <p><code>writeShellScriptBin</code> takes the following arguments:</p> <p><code>name</code> (String)</p> <p>: The name used in the Nix store path and within the file generated under the store path.</p> <p><code>text</code> (String)</p> <p>: The contents of the file.</p> <p>The file's contents will be put into <code>/nix/store/&lt;store path&gt;/bin/&lt;name&gt;</code>. The store path will include the the name, and it will be a directory.</p> <p>This function is a combination of  and .</p> <p>::: {.example #ex-writeShellScriptBin}</p>"},{"location":"build-helpers/trivial-build-helpers.chapter.html#usage-of-writeshellscriptbin","title":"Usage of <code>writeShellScriptBin</code>","text":"<p><pre><code>writeShellScriptBin \"my-script\"\n  ''\n  echo \"hi\"\n  '';\n</code></pre> :::</p> <p>This is equivalent to:</p> <pre><code>writeTextFile {\n  name = \"my-script\";\n  text = ''\n    #! ${pkgs.runtimeShell}\n    echo \"hi\"\n  '';\n  executable = true;\n  destination = \"bin/my-script\"\n}\n</code></pre>"},{"location":"build-helpers/trivial-build-helpers.chapter.html#trivial-builder-concatText","title":"<code>concatTextFile</code>, <code>concatText</code>, <code>concatScript</code>","text":"<p>These functions concatenate <code>files</code> to the Nix store in a single file. This is useful for configuration files structured in lines of text. <code>concatTextFile</code> takes an attribute set and expects two arguments, <code>name</code> and <code>files</code>. <code>name</code> corresponds to the name used in the Nix store path. <code>files</code> will be the files to be concatenated. You can also set <code>executable</code> to true to make this file have the executable bit set. <code>concatText</code> and<code>concatScript</code> are simple wrappers over <code>concatTextFile</code>.</p> <p>Here are a few examples: <pre><code># Writes my-file to /nix/store/&lt;store path&gt;\nconcatTextFile {\n  name = \"my-file\";\n  files = [ drv1 \"${drv2}/path/to/file\" ];\n}\n# See also the `concatText` helper function below.\n\n# Writes executable my-file to /nix/store/&lt;store path&gt;/bin/my-file\nconcatTextFile {\n  name = \"my-file\";\n  files = [ drv1 \"${drv2}/path/to/file\" ];\n  executable = true;\n  destination = \"/bin/my-file\";\n}\n# Writes contents of files to /nix/store/&lt;store path&gt;\nconcatText \"my-file\" [ file1 file2 ]\n\n# Writes contents of files to /nix/store/&lt;store path&gt;\nconcatScript \"my-file\" [ file1 file2 ]\n</code></pre></p>"},{"location":"build-helpers/trivial-build-helpers.chapter.html#trivial-builder-writeShellApplication","title":"<code>writeShellApplication</code>","text":"<p><code>writeShellApplication</code> is similar to <code>writeShellScriptBin</code> and <code>writeScriptBin</code> but supports runtime dependencies with <code>runtimeInputs</code>. Writes an executable shell script to <code>/nix/store/&lt;store path&gt;/bin/&lt;name&gt;</code> and checks its syntax with <code>shellcheck</code> and the <code>bash</code>'s <code>-n</code> option. Some basic Bash options are set by default (<code>errexit</code>, <code>nounset</code>, and <code>pipefail</code>), but can be overridden with <code>bashOptions</code>.</p> <p>Extra arguments may be passed to <code>stdenv.mkDerivation</code> by setting <code>derivationArgs</code>; note that variables set in this manner will be set when the shell script is built, not when it's run. Runtime environment variables can be set with the <code>runtimeEnv</code> argument.</p> <p>For example, the following shell application can refer to <code>curl</code> directly, rather than needing to write <code>${curl}/bin/curl</code>:</p> <pre><code>writeShellApplication {\n  name = \"show-nixos-org\";\n\n  runtimeInputs = [ curl w3m ];\n\n  text = ''\n    curl -s 'https://nixos.org' | w3m -dump -T text/html\n  '';\n}\n</code></pre>"},{"location":"build-helpers/trivial-build-helpers.chapter.html#trivial-builder-symlinkJoin","title":"<code>symlinkJoin</code>","text":"<p>This can be used to put many derivations into the same directory structure. It works by creating a new derivation and adding symlinks to each of the paths listed. It expects two arguments, <code>name</code>, and <code>paths</code>. <code>name</code> is the name used in the Nix store path for the created derivation. <code>paths</code> is a list of paths that will be symlinked. These paths can be to Nix store derivations or any other subdirectory contained within. Here is an example: <pre><code># adds symlinks of hello and stack to current build and prints \"links added\"\nsymlinkJoin { name = \"myexample\"; paths = [ pkgs.hello pkgs.stack ]; postBuild = \"echo links added\"; }\n</code></pre> This creates a derivation with a directory structure like the following: <pre><code>/nix/store/sglsr5g079a5235hy29da3mq3hv8sjmm-myexample\n|-- bin\n|   |-- hello -&gt; /nix/store/qy93dp4a3rqyn2mz63fbxjg228hffwyw-hello-2.10/bin/hello\n|   `-- stack -&gt; /nix/store/6lzdpxshx78281vy056lbk553ijsdr44-stack-2.1.3.1/bin/stack\n`-- share\n    |-- bash-completion\n    |   `-- completions\n    |       `-- stack -&gt; /nix/store/6lzdpxshx78281vy056lbk553ijsdr44-stack-2.1.3.1/share/bash-completion/completions/stack\n    |-- fish\n    |   `-- vendor_completions.d\n    |       `-- stack.fish -&gt; /nix/store/6lzdpxshx78281vy056lbk553ijsdr44-stack-2.1.3.1/share/fish/vendor_completions.d/stack.fish\n...\n</code></pre></p>"},{"location":"build-helpers/trivial-build-helpers.chapter.html#trivial-builder-writeReferencesToFile","title":"<code>writeReferencesToFile</code>","text":"<p>Writes the closure of transitive dependencies to a file.</p> <p>This produces the equivalent of <code>nix-store -q --requisites</code>.</p> <p>For example,</p> <pre><code>writeReferencesToFile (writeScriptBin \"hi\" ''${hello}/bin/hello'')\n</code></pre> <p>produces an output path <code>/nix/store/&lt;hash&gt;-runtime-deps</code> containing</p> <pre><code>/nix/store/&lt;hash&gt;-hello-2.10\n/nix/store/&lt;hash&gt;-hi\n/nix/store/&lt;hash&gt;-libidn2-2.3.0\n/nix/store/&lt;hash&gt;-libunistring-0.9.10\n/nix/store/&lt;hash&gt;-glibc-2.32-40\n</code></pre> <p>You can see that this includes <code>hi</code>, the original input path, <code>hello</code>, which is a direct reference, but also the other paths that are indirectly required to run <code>hello</code>.</p>"},{"location":"build-helpers/trivial-build-helpers.chapter.html#trivial-builder-writeDirectReferencesToFile","title":"<code>writeDirectReferencesToFile</code>","text":"<p>Writes the set of references to the output file, that is, their immediate dependencies.</p> <p>This produces the equivalent of <code>nix-store -q --references</code>.</p> <p>For example,</p> <pre><code>writeDirectReferencesToFile (writeScriptBin \"hi\" ''${hello}/bin/hello'')\n</code></pre> <p>produces an output path <code>/nix/store/&lt;hash&gt;-runtime-references</code> containing</p> <pre><code>/nix/store/&lt;hash&gt;-hello-2.10\n</code></pre> <p>but none of <code>hello</code>'s dependencies because those are not referenced directly by <code>hi</code>'s output.</p>"},{"location":"build-helpers/images/appimagetools.section.html","title":"pkgs.appimageTools","text":"<p><code>pkgs.appimageTools</code> is a set of functions for extracting and wrapping AppImage files. They are meant to be used if traditional packaging from source is infeasible, or if it would take too long. To quickly run an AppImage file, <code>pkgs.appimage-run</code> can be used as well.</p> <p>::: {.warning} The <code>appimageTools</code> API is unstable and may be subject to backwards-incompatible changes in the future. :::</p>"},{"location":"build-helpers/images/appimagetools.section.html#ssec-pkgs-appimageTools-wrapping","title":"Wrapping","text":"<p>Use <code>wrapType2</code> to wrap any AppImage. This will create a FHS environment with many packages expected to exist for the AppImage to work. <code>wrapType2</code> expects an argument with the <code>src</code> attribute, and either a <code>name</code> attribute or <code>pname</code> and <code>version</code> attributes.</p> <p>It will eventually call into <code>buildFHSEnv</code>, and any extra attributes in the argument to <code>wrapType2</code> will be passed through to it. This means that you can pass the <code>extraInstallCommands</code> attribute, for example, and it will have the same effect as described in <code>buildFHSEnv</code>.</p> <p>::: {.note} In the past, <code>appimageTools</code> provided both <code>wrapType1</code> and <code>wrapType2</code>, to be used depending on the type of AppImage that was being wrapped. However, those were unified early 2020, meaning that both <code>wrapType1</code> and <code>wrapType2</code> have the same behaviour now. :::</p> <p>:::{.example #ex-wrapping-appimage-from-github}</p>"},{"location":"build-helpers/images/appimagetools.section.html#wrapping-an-appimage-from-github","title":"Wrapping an AppImage from GitHub","text":"<pre><code>{ appimageTools, fetchurl }:\nlet\n  pname = \"nuclear\";\n  version = \"0.6.30\";\n\n  src = fetchurl {\n    url = \"https://github.com/nukeop/nuclear/releases/download/v${version}/${pname}-v${version}.AppImage\";\n    hash = \"sha256-he1uGC1M/nFcKpMM9JKY4oeexJcnzV0ZRxhTjtJz6xw=\";\n  };\nin\nappimageTools.wrapType2 {\n  inherit pname version src;\n}\n</code></pre> <p>:::</p> <p>The argument passed to <code>wrapType2</code> can also contain an <code>extraPkgs</code> attribute, which allows you to include additional packages inside the FHS environment your AppImage is going to run in. <code>extraPkgs</code> must be a function that returns a list of packages. There are a few ways to learn which dependencies an application needs:</p> <ul> <li>Looking through the extracted AppImage files, reading its scripts and running <code>patchelf</code> and <code>ldd</code> on its executables.     This can also be done in <code>appimage-run</code>, by setting <code>APPIMAGE_DEBUG_EXEC=bash</code>.</li> <li>Running <code>strace -vfefile</code> on the wrapped executable, looking for libraries that can't be found.</li> </ul> <p>:::{.example #ex-wrapping-appimage-with-extrapkgs}</p>"},{"location":"build-helpers/images/appimagetools.section.html#wrapping-an-appimage-with-extra-packages","title":"Wrapping an AppImage with extra packages","text":"<pre><code>{ appimageTools, fetchurl }:\nlet\n  pname = \"irccloud\";\n  version = \"0.16.0\";\n\n  src = fetchurl {\n    url = \"https://github.com/irccloud/irccloud-desktop/releases/download/v${version}/IRCCloud-${version}-linux-x86_64.AppImage\";\n    sha256 = \"sha256-/hMPvYdnVB1XjKgU2v47HnVvW4+uC3rhRjbucqin4iI=\";\n  };\nin appimageTools.wrapType2 {\n  inherit pname version src;\n  extraPkgs = pkgs: [ pkgs.at-spi2-core ];\n}\n</code></pre> <p>:::</p>"},{"location":"build-helpers/images/appimagetools.section.html#ssec-pkgs-appimageTools-extracting","title":"Extracting","text":"<p>Use <code>extract</code> if you need to extract the contents of an AppImage. This is usually used in Nixpkgs to install extra files in addition to wrapping the AppImage. <code>extract</code> expects an argument with the <code>src</code> attribute, and either a <code>name</code> attribute or <code>pname</code> and <code>version</code> attributes.</p> <p>::: {.note} In the past, <code>appimageTools</code> provided both <code>extractType1</code> and <code>extractType2</code>, to be used depending on the type of AppImage that was being extracted. However, those were unified early 2020, meaning that both <code>extractType1</code> and <code>extractType2</code> have the same behaviour as <code>extract</code> now. :::</p> <p>:::{.example #ex-extracting-appimage}</p>"},{"location":"build-helpers/images/appimagetools.section.html#extracting-an-appimage-to-install-extra-files","title":"Extracting an AppImage to install extra files","text":"<p>This example was adapted from a real package in Nixpkgs to show how <code>extract</code> is usually used in combination with <code>wrapType2</code>. Note how <code>appimageContents</code> is used in <code>extraInstallCommands</code> to install additional files that were extracted from the AppImage.</p> <pre><code>{ appimageTools, fetchurl }:\nlet\n  pname = \"irccloud\";\n  version = \"0.16.0\";\n\n  src = fetchurl {\n    url = \"https://github.com/irccloud/irccloud-desktop/releases/download/v${version}/IRCCloud-${version}-linux-x86_64.AppImage\";\n    sha256 = \"sha256-/hMPvYdnVB1XjKgU2v47HnVvW4+uC3rhRjbucqin4iI=\";\n  };\n\n  appimageContents = appimageTools.extract {\n    inherit pname version src;\n  };\nin appimageTools.wrapType2 {\n  inherit pname version src;\n\n  extraPkgs = pkgs: [ pkgs.at-spi2-core ];\n\n  extraInstallCommands = ''\n    mv $out/bin/${pname}-${version} $out/bin/${pname}\n    install -m 444 -D ${appimageContents}/irccloud.desktop $out/share/applications/irccloud.desktop\n    install -m 444 -D ${appimageContents}/usr/share/icons/hicolor/512x512/apps/irccloud.png \\\n      $out/share/icons/hicolor/512x512/apps/irccloud.png\n    substituteInPlace $out/share/applications/irccloud.desktop \\\n      --replace 'Exec=AppRun' 'Exec=${pname}'\n  '';\n}\n</code></pre> <p>:::</p> <p>The argument passed to <code>extract</code> can also contain a <code>postExtract</code> attribute, which allows you to execute additional commands after the files are extracted from the AppImage. <code>postExtract</code> must be a string with commands to run.</p> <p>:::{.example #ex-extracting-appimage-with-postextract}</p>"},{"location":"build-helpers/images/appimagetools.section.html#extracting-an-appimage-to-install-extra-files-using-postextract","title":"Extracting an AppImage to install extra files, using <code>postExtract</code>","text":"<p>This is a rewrite of  to use <code>postExtract</code>.</p> <pre><code>{ appimageTools, fetchurl }:\nlet\n  pname = \"irccloud\";\n  version = \"0.16.0\";\n\n  src = fetchurl {\n    url = \"https://github.com/irccloud/irccloud-desktop/releases/download/v${version}/IRCCloud-${version}-linux-x86_64.AppImage\";\n    sha256 = \"sha256-/hMPvYdnVB1XjKgU2v47HnVvW4+uC3rhRjbucqin4iI=\";\n  };\n\n  appimageContents = appimageTools.extract {\n    inherit pname version src;\n    postExtract = ''\n      substituteInPlace $out/irccloud.desktop --replace 'Exec=AppRun' 'Exec=${pname}'\n    '';\n  };\nin appimageTools.wrapType2 {\n  inherit pname version src;\n\n  extraPkgs = pkgs: [ pkgs.at-spi2-core ];\n\n  extraInstallCommands = ''\n    mv $out/bin/${pname}-${version} $out/bin/${pname}\n    install -m 444 -D ${appimageContents}/irccloud.desktop $out/share/applications/irccloud.desktop\n    install -m 444 -D ${appimageContents}/usr/share/icons/hicolor/512x512/apps/irccloud.png \\\n      $out/share/icons/hicolor/512x512/apps/irccloud.png\n  '';\n}\n</code></pre> <p>:::</p>"},{"location":"build-helpers/images/binarycache.section.html","title":"pkgs.mkBinaryCache","text":"<p><code>pkgs.mkBinaryCache</code> is a function for creating Nix flat-file binary caches. Such a cache exists as a directory on disk, and can be used as a Nix substituter by passing <code>--substituter file:///path/to/cache</code> to Nix commands.</p> <p>Nix packages are most commonly shared between machines using HTTP, SSH, or S3, but a flat-file binary cache can still be useful in some situations. For example, you can copy it directly to another machine, or make it available on a network file system. It can also be a convenient way to make some Nix packages available inside a container via bind-mounting.</p> <p><code>mkBinaryCache</code> expects an argument with the <code>rootPaths</code> attribute. <code>rootPaths</code> must be a list of derivations. The transitive closure of these derivations' outputs will be copied into the cache.</p> <p>::: {.note} This function is meant for advanced use cases. The more idiomatic way to work with flat-file binary caches is via the nix-copy-closure command. You may also want to consider dockerTools for your containerization needs. :::</p> <p>[]{#sec-pkgs-binary-cache-example} :::{.example #ex-mkbinarycache-copying-package-closure}</p>"},{"location":"build-helpers/images/binarycache.section.html#copying-a-package-and-its-closure-to-another-machine-with-mkbinarycache","title":"Copying a package and its closure to another machine with <code>mkBinaryCache</code>","text":"<p>The following derivation will construct a flat-file binary cache containing the closure of <code>hello</code>.</p> <pre><code>{ mkBinaryCache, hello }:\nmkBinaryCache {\n  rootPaths = [hello];\n}\n</code></pre> <p>Build the cache on a machine. Note that the command still builds the exact nix package above, but adds some boilerplate to build it directly from an expression.</p> <pre><code>$ nix-build -E 'let pkgs = import &lt;nixpkgs&gt; {}; in pkgs.callPackage ({ mkBinaryCache, hello }: mkBinaryCache { rootPaths = [hello]; }) {}'\n/nix/store/azf7xay5xxdnia4h9fyjiv59wsjdxl0g-binary-cache\n</code></pre> <p>Copy the resulting directory to another machine, which we'll call <code>host2</code>:</p> <pre><code>$ scp result host2:/tmp/hello-cache\n</code></pre> <p>At this point, the cache can be used as a substituter when building derivations on <code>host2</code>:</p> <pre><code>$ nix-build -A hello '&lt;nixpkgs&gt;' \\\n  --option require-sigs false \\\n  --option trusted-substituters file:///tmp/hello-cache \\\n  --option substituters file:///tmp/hello-cache\n/nix/store/zhl06z4lrfrkw5rp0hnjjfrgsclzvxpm-hello-2.12.1\n</code></pre> <p>:::</p>"},{"location":"build-helpers/images/dockertools.section.html","title":"pkgs.dockerTools","text":"<p><code>pkgs.dockerTools</code> is a set of functions for creating and manipulating Docker images according to the Docker Image Specification v1.3.0. Docker itself is not used to perform any of the operations done by these functions.</p>"},{"location":"build-helpers/images/dockertools.section.html#ssec-pkgs-dockerTools-buildImage","title":"buildImage","text":"<p>This function builds a Docker-compatible repository tarball containing a single image. As such, the result is suitable for being loaded in Docker with <code>docker load</code> (see  for how to do this).</p> <p>This function will create a single layer for all files (and dependencies) that are specified in its argument. Only new dependencies that are not already in the existing layers will be copied. If you prefer to create multiple layers for the files and dependencies you want to add to the image, see  or  instead.</p> <p>This function allows a script to be run during the layer generation process, allowing custom behaviour to affect the final results of the image (see the documentation of the <code>runAsRoot</code> and <code>extraCommands</code> attributes).</p> <p>The resulting repository tarball will list a single image as specified by the <code>name</code> and <code>tag</code> attributes. By default, that image will use a static creation date (see documentation for the <code>created</code> attribute). This allows <code>buildImage</code> to produce reproducible images.</p> <p>:::{.tip} When running an image built with <code>buildImage</code>, you might encounter certain errors depending on what you included in the image, especially if you did not start with any base image.</p> <p>If you encounter errors similar to <code>getProtocolByName: does not exist (no such protocol name: tcp)</code>, you may need to add the contents of <code>pkgs.iana-etc</code> in the <code>copyToRoot</code> attribute. Similarly, if you encounter errors similar to <code>Error_Protocol (\"certificate has unknown CA\",True,UnknownCa)</code>, you may need to add the contents of <code>pkgs.cacert</code> in the <code>copyToRoot</code> attribute. :::</p>"},{"location":"build-helpers/images/dockertools.section.html#ssec-pkgs-dockerTools-buildImage-inputs","title":"Inputs","text":"<p><code>buildImage</code> expects an argument with the following attributes:</p> <p><code>name</code> (String)</p> <p>: The name of the generated image.</p> <p><code>tag</code> (String or Null; optional)</p> <p>: Tag of the generated image.   If <code>null</code>, the hash of the nix derivation will be used as the tag.</p> <p>Default value: <code>null</code>.</p> <p><code>fromImage</code> (Path or Null; optional)</p> <p>: The repository tarball of an image to be used as the base for the generated image.   It must be a valid Docker image, such as one exported by <code>docker save</code>, or another image built with the <code>dockerTools</code> utility functions.   This can be seen as an equivalent of <code>FROM fromImage</code> in a <code>Dockerfile</code>.   A value of <code>null</code> can be seen as an equivalent of <code>FROM scratch</code>.</p> <p>If specified, the layer created by <code>buildImage</code> will be appended to the layers defined in the base image, resulting in an image with at least two layers (one or more layers from the base image, and the layer created by <code>buildImage</code>).   Otherwise, the resulting image with contain the single layer created by <code>buildImage</code>.</p> <p>Default value: <code>null</code>.</p> <p><code>fromImageName</code> (String or Null; optional)</p> <p>: Used to specify the image within the repository tarball in case it contains multiple images.   A value of <code>null</code> means that <code>buildImage</code> will use the first image available in the repository.</p> <p>:::{.note}   This must be used with <code>fromImageTag</code>. Using only <code>fromImageName</code> without <code>fromImageTag</code> will make <code>buildImage</code> use the first image available in the repository.   :::</p> <p>Default value: <code>null</code>.</p> <p><code>fromImageTag</code> (String or Null; optional)</p> <p>: Used to specify the image within the repository tarball in case it contains multiple images.   A value of <code>null</code> means that <code>buildImage</code> will use the first image available in the repository.</p> <p>:::{.note}   This must be used with <code>fromImageName</code>. Using only <code>fromImageTag</code> without <code>fromImageName</code> will make <code>buildImage</code> use the first image available in the repository   :::</p> <p>Default value: <code>null</code>.</p> <p><code>copyToRoot</code> (Path, List of Paths, or Null; optional)</p> <p>: Files to add to the generated image.   Anything that coerces to a path (e.g. a derivation) can also be used.   This can be seen as an equivalent of <code>ADD contents/ /</code> in a <code>Dockerfile</code>.</p> <p>Default value: <code>null</code>.</p> <p><code>keepContentsDirlinks</code> (Boolean; optional)</p> <p>: When adding files to the generated image (as specified by <code>copyToRoot</code>), this attribute controls whether to preserve symlinks to directories.   If <code>false</code>, the symlinks will be transformed into directories.   This behaves the same as <code>rsync -k</code> when <code>keepContentsDirlinks</code> is <code>false</code>, and the same as <code>rsync -K</code> when <code>keepContentsDirlinks</code> is <code>true</code>.</p> <p>Default value: <code>false</code>.</p> <p><code>runAsRoot</code> (String or Null; optional)</p> <p>: A bash script that will run as root inside a VM that contains the existing layers of the base image and the new generated layer (including the files from <code>copyToRoot</code>).   The script will be run with a working directory of <code>/</code>.   This can be seen as an equivalent of <code>RUN ...</code> in a <code>Dockerfile</code>.   A value of <code>null</code> means that this step in the image generation process will be skipped.</p> <p>See  for how to work with this attribute.</p> <p>:::{.caution}   Using this attribute requires the <code>kvm</code> device to be available, see <code>system-features</code>.   If the <code>kvm</code> device isn't available, you should consider using <code>buildLayeredImage</code> or <code>streamLayeredImage</code> instead.   Those functions allow scripts to be run as root without access to the <code>kvm</code> device.   :::</p> <p>:::{.note}   At the time the script in <code>runAsRoot</code> is run, the files specified directly in <code>copyToRoot</code> will be present in the VM, but their dependencies might not be there yet.   Copying their dependencies into the generated image is a step that happens after <code>runAsRoot</code> finishes running.   :::</p> <p>Default value: <code>null</code>.</p> <p><code>extraCommands</code> (String; optional)</p> <p>: A bash script that will run before the layer created by <code>buildImage</code> is finalised.   The script will be run on some (opaque) working directory which will become <code>/</code> once the layer is created.   This is similar to <code>runAsRoot</code>, but the script specified in <code>extraCommands</code> is not run as root, and does not involve creating a VM.   It is simply run as part of building the derivation that outputs the layer created by <code>buildImage</code>.</p> <p>See  for how to work with this attribute, and subtle differences compared to <code>runAsRoot</code>.</p> <p>Default value: <code>\"\"</code>.</p> <p><code>config</code> (Attribute Set; optional)</p> <p>: Used to specify the configuration of the containers that will be started off the generated image.   Must be an attribute set, with each attribute as listed in the Docker Image Specification v1.3.0.</p> <p>Default value: <code>null</code>.</p> <p><code>architecture</code> (String; optional)</p> <p>: Used to specify the image architecture.   This is useful for multi-architecture builds that don't need cross compiling.   If specified, its value should follow the OCI Image Configuration Specification, which should still be compatible with Docker.   According to the linked specification, all possible values for <code>$GOARCH</code> in the Go docs should be valid, but will commonly be one of <code>386</code>, <code>amd64</code>, <code>arm</code>, or <code>arm64</code>.</p> <p>Default value: the same value from <code>pkgs.go.GOARCH</code>.</p> <p><code>diskSize</code> (Number; optional)</p> <p>: Controls the disk size (in megabytes) of the VM used to run the script specified in <code>runAsRoot</code>.   This attribute is ignored if <code>runAsRoot</code> is <code>null</code>.</p> <p>Default value: 1024.</p> <p><code>buildVMMemorySize</code> (Number; optional)</p> <p>: Controls the amount of memory (in megabytes) provisioned for the VM used to run the script specified in <code>runAsRoot</code>.   This attribute is ignored if <code>runAsRoot</code> is <code>null</code>.</p> <p>Default value: 512.</p> <p><code>created</code> (String; optional)</p> <p>: Specifies the time of creation of the generated image.   This should be either a date and time formatted according to ISO-8601 or <code>\"now\"</code>, in which case <code>buildImage</code> will use the current date.</p> <p>See  for how to use <code>\"now\"</code>.</p> <p>:::{.caution}   Using <code>\"now\"</code> means that the generated image will not be reproducible anymore (because the date will always change whenever it's built).   :::</p> <p>Default value: <code>\"1970-01-01T00:00:01Z\"</code>.</p> <p><code>uid</code> (Number; optional)</p> <p>: The uid of the user that will own the files packed in the new layer built by <code>buildImage</code>.</p> <p>Default value: 0.</p> <p><code>gid</code> (Number; optional)</p> <p>: The gid of the group that will own the files packed in the new layer built by <code>buildImage</code>.</p> <p>Default value: 0.</p> <p><code>contents</code> DEPRECATED</p> <p>: This attribute is deprecated, and users are encouraged to use <code>copyToRoot</code> instead.</p>"},{"location":"build-helpers/images/dockertools.section.html#ssec-pkgs-dockerTools-buildImage-passthru-outputs","title":"Passthru outputs","text":"<p><code>buildImage</code> defines a few <code>passthru</code> attributes:</p> <p><code>buildArgs</code> (Attribute Set)</p> <p>: The argument passed to <code>buildImage</code> itself.   This allows you to inspect all attributes specified in the argument, as described above.</p> <p><code>layer</code> (Attribute Set)</p> <p>: The derivation with the layer created by <code>buildImage</code>.   This allows easier inspection of the contents added by <code>buildImage</code> in the generated image.</p> <p><code>imageTag</code> (String)</p> <p>: The tag of the generated image.   This is useful if no tag was specified in the attributes of the argument to <code>buildImage</code>, because an automatic tag will be used instead.   <code>imageTag</code> allows you to retrieve the value of the tag used in this case.</p>"},{"location":"build-helpers/images/dockertools.section.html#ssec-pkgs-dockerTools-buildImage-examples","title":"Examples","text":"<p>:::{.example #ex-dockerTools-buildImage}</p>"},{"location":"build-helpers/images/dockertools.section.html#building-a-docker-image","title":"Building a Docker image","text":"<p>The following package builds a Docker image that runs the <code>redis-server</code> executable from the <code>redis</code> package. The Docker image will have name <code>redis</code> and tag <code>latest</code>.</p> <pre><code>{ dockerTools, buildEnv, redis }:\ndockerTools.buildImage {\n  name = \"redis\";\n  tag = \"latest\";\n\n  copyToRoot = buildEnv {\n    name = \"image-root\";\n    paths = [ redis ];\n    pathsToLink = [ \"/bin\" ];\n  };\n\n  runAsRoot = ''\n    mkdir -p /data\n  '';\n\n  config = {\n    Cmd = [ \"/bin/redis-server\" ];\n    WorkingDir = \"/data\";\n    Volumes = { \"/data\" = { }; };\n  };\n}\n</code></pre> <p>The result of building this package is a <code>.tar.gz</code> file that can be loaded into Docker:</p> <p><pre><code>$ nix-build\n(some output removed for clarity)\nbuilding '/nix/store/yw0adm4wpsw1w6j4fb5hy25b3arr9s1v-docker-image-redis.tar.gz.drv'...\nAdding layer...\ntar: Removing leading `/' from member names\nAdding meta...\nCooking the image...\nFinished.\n/nix/store/p4dsg62inh9d2ksy3c7bv58xa851dasr-docker-image-redis.tar.gz\n\n$ docker load -i /nix/store/p4dsg62inh9d2ksy3c7bv58xa851dasr-docker-image-redis.tar.gz\n(some output removed for clarity)\nLoaded image: redis:latest\n</code></pre> :::</p> <p>:::{.example #ex-dockerTools-buildImage-runAsRoot}</p>"},{"location":"build-helpers/images/dockertools.section.html#building-a-docker-image-with-runasroot","title":"Building a Docker image with <code>runAsRoot</code>","text":"<p>The following package builds a Docker image with the <code>hello</code> executable from the <code>hello</code> package. It uses <code>runAsRoot</code> to create a directory and a file inside the image.</p> <p>This works the same as , but uses <code>runAsRoot</code> instead of <code>extraCommands</code>.</p> <p><pre><code>{ dockerTools, buildEnv, hello }:\ndockerTools.buildImage {\n  name = \"hello\";\n  tag = \"latest\";\n\n  copyToRoot = buildEnv {\n    name = \"image-root\";\n    paths = [ hello ];\n    pathsToLink = [ \"/bin\" ];\n  };\n\n  runAsRoot = ''\n    mkdir -p /data\n    echo \"some content\" &gt; my-file\n  '';\n\n  config = {\n    Cmd = [ \"/bin/hello\" ];\n    WorkingDir = \"/data\";\n  };\n}\n</code></pre> :::</p> <p>:::{.example #ex-dockerTools-buildImage-extraCommands}</p>"},{"location":"build-helpers/images/dockertools.section.html#building-a-docker-image-with-extracommands","title":"Building a Docker image with <code>extraCommands</code>","text":"<p>The following package builds a Docker image with the <code>hello</code> executable from the <code>hello</code> package. It uses <code>extraCommands</code> to create a directory and a file inside the image.</p> <p>This works the same as , but uses <code>extraCommands</code> instead of <code>runAsRoot</code>. Note that with <code>extraCommands</code>, we can't directly reference <code>/</code> and must create files and directories as if we were already on <code>/</code>.</p> <p><pre><code>{ dockerTools, buildEnv, hello }:\ndockerTools.buildImage {\n  name = \"hello\";\n  tag = \"latest\";\n\n  copyToRoot = buildEnv {\n    name = \"image-root\";\n    paths = [ hello ];\n    pathsToLink = [ \"/bin\" ];\n  };\n\n  extraCommands = ''\n    mkdir -p data\n    echo \"some content\" &gt; my-file\n  '';\n\n  config = {\n    Cmd = [ \"/bin/hello\" ];\n    WorkingDir = \"/data\";\n  };\n}\n</code></pre> :::</p> <p>:::{.example #ex-dockerTools-buildImage-creatednow}</p>"},{"location":"build-helpers/images/dockertools.section.html#building-a-docker-image-with-a-creation-date-set-to-the-current-time","title":"Building a Docker image with a creation date set to the current time","text":"<p>Note that using a value of <code>\"now\"</code> in the <code>created</code> attribute will break reproducibility.</p> <pre><code>{ dockerTools, buildEnv, hello }:\ndockerTools.buildImage {\n  name = \"hello\";\n  tag = \"latest\";\n\n  created = \"now\";\n\n  copyToRoot = buildEnv {\n    name = \"image-root\";\n    paths = [ hello ];\n    pathsToLink = [ \"/bin\" ];\n  };\n\n  config.Cmd = [ \"/bin/hello\" ];\n}\n</code></pre> <p>After importing the generated repository tarball with Docker, its CLI will display a reasonable date and sort the images as expected:</p> <p><pre><code>$ docker images\nREPOSITORY   TAG      IMAGE ID       CREATED              SIZE\nhello        latest   de2bf4786de6   About a minute ago   25.2MB\n</code></pre> :::</p>"},{"location":"build-helpers/images/dockertools.section.html#ssec-pkgs-dockerTools-buildLayeredImage","title":"buildLayeredImage","text":"<p><code>buildLayeredImage</code> uses <code>streamLayeredImage</code> underneath to build a compressed Docker-compatible repository tarball. Basically, <code>buildLayeredImage</code> runs the script created by <code>streamLayeredImage</code> to save the compressed image in the Nix store. <code>buildLayeredImage</code> supports the same options as <code>streamLayeredImage</code>, see <code>streamLayeredImage</code> for details.</p> <p>:::{.note} Despite the similar name, <code>buildImage</code> works completely differently from <code>buildLayeredImage</code> and <code>streamLayeredImage</code>.</p> <p>Even though some of the arguments may seem related, they cannot be interchanged. :::</p> <p>You can use this function to load an image in Docker with <code>docker load</code>. See  to see how to do that.</p>"},{"location":"build-helpers/images/dockertools.section.html#ssec-pkgs-dockerTools-buildLayeredImage-examples","title":"Examples","text":"<p>:::{.example #ex-dockerTools-buildLayeredImage-hello}</p>"},{"location":"build-helpers/images/dockertools.section.html#building-a-layered-docker-image","title":"Building a layered Docker image","text":"<p>The following package builds a layered Docker image that runs the <code>hello</code> executable from the <code>hello</code> package. The Docker image will have name <code>hello</code> and tag <code>latest</code>.</p> <pre><code>{ dockerTools, hello }:\ndockerTools.buildLayeredImage {\n  name = \"hello\";\n  tag = \"latest\";\n\n  contents = [ hello ];\n\n  config.Cmd = [ \"/bin/hello\" ];\n}\n</code></pre> <p>The result of building this package is a <code>.tar.gz</code> file that can be loaded into Docker:</p> <p><pre><code>$ nix-build\n(some output removed for clarity)\nbuilding '/nix/store/bk8bnrbw10nq7p8pvcmdr0qf57y6scha-hello.tar.gz.drv'...\nNo 'fromImage' provided\nCreating layer 1 from paths: ['/nix/store/i93s7xxblavsacpy82zdbn4kplsyq48l-libunistring-1.1']\nCreating layer 2 from paths: ['/nix/store/ji01n9vinnj22nbrb86nx8a1ssgpilx8-libidn2-2.3.4']\nCreating layer 3 from paths: ['/nix/store/ldrslljw4rg026nw06gyrdwl78k77vyq-xgcc-12.3.0-libgcc']\nCreating layer 4 from paths: ['/nix/store/9y8pmvk8gdwwznmkzxa6pwyah52xy3nk-glibc-2.38-27']\nCreating layer 5 from paths: ['/nix/store/zhl06z4lrfrkw5rp0hnjjfrgsclzvxpm-hello-2.12.1']\nCreating layer 6 with customisation...\nAdding manifests...\nDone.\n/nix/store/hxcz7snvw7f8rzhbh6mv8jq39d992905-hello.tar.gz\n\n$ docker load -i /nix/store/hxcz7snvw7f8rzhbh6mv8jq39d992905-hello.tar.gz\n(some output removed for clarity)\nLoaded image: hello:latest\n</code></pre> :::</p>"},{"location":"build-helpers/images/dockertools.section.html#ssec-pkgs-dockerTools-streamLayeredImage","title":"streamLayeredImage","text":"<p><code>streamLayeredImage</code> builds a script which, when run, will stream to stdout a Docker-compatible repository tarball containing a single image, using multiple layers to improve sharing between images. This means that <code>streamLayeredImage</code> does not output an image into the Nix store, but only a script that builds the image, saving on IO and disk/cache space, particularly with large images.</p> <p>You can use this function to load an image in Docker with <code>docker load</code>. See  to see how to do that.</p> <p>For this function, you specify a store path or a list of store paths to be added to the image, and the functions will automatically include any dependencies of those paths in the image. The function will attempt to create one layer per object in the Nix store that needs to be added to the image. In case there are more objects to include than available layers, the function will put the most \"popular\" objects in their own layers, and group all remaining objects into a single layer.</p> <p>An additional layer will be created with symlinks to the store paths you specified to be included in the image. These symlinks are built with <code>symlinkJoin</code>, so they will be included in the root of the image. See  to understand how these symlinks are laid out in the generated image.</p> <p><code>streamLayeredImage</code> allows scripts to be run when creating the additional layer with symlinks, allowing custom behaviour to affect the final results of the image (see the documentation of the <code>extraCommands</code> and <code>fakeRootCommands</code> attributes).</p> <p>The resulting repository tarball will list a single image as specified by the <code>name</code> and <code>tag</code> attributes. By default, that image will use a static creation date (see documentation for the <code>created</code> attribute). This allows the function to produce reproducible images.</p>"},{"location":"build-helpers/images/dockertools.section.html#ssec-pkgs-dockerTools-streamLayeredImage-inputs","title":"Inputs","text":"<p><code>streamLayeredImage</code> expects one argument with the following attributes:</p> <p><code>name</code> (String)</p> <p>: The name of the generated image.</p> <p><code>tag</code> (String; optional)</p> <p>: Tag of the generated image.   If <code>null</code>, the hash of the nix derivation will be used as the tag.</p> <p>Default value: <code>null</code>.</p> <p><code>fromImage</code>(Path or Null; optional)</p> <p>: The repository tarball of an image to be used as the base for the generated image.   It must be a valid Docker image, such as one exported by <code>docker save</code>, or another image built with the <code>dockerTools</code> utility functions.   This can be seen as an equivalent of <code>FROM fromImage</code> in a <code>Dockerfile</code>.   A value of <code>null</code> can be seen as an equivalent of <code>FROM scratch</code>.</p> <p>If specified, the created layers will be appended to the layers defined in the base image.</p> <p>Default value: <code>null</code>.</p> <p><code>contents</code> (Path or List of Paths; optional) []{#dockerTools-buildLayeredImage-arg-contents}</p> <p>: Directories whose contents will be added to the generated image.   Things that coerce to paths (e.g. a derivation) can also be used.   This can be seen as an equivalent of <code>ADD contents/ /</code> in a <code>Dockerfile</code>.</p> <p>All the contents specified by <code>contents</code> will be added as a final layer in the generated image.   They will be added as links to the actual files (e.g. links to the store paths).   The actual files will be added in previous layers.</p> <p>Default value: <code>[]</code></p> <p><code>config</code> (Attribute Set; optional) []{#dockerTools-buildLayeredImage-arg-config}</p> <p>: Used to specify the configuration of the containers that will be started off the generated image.   Must be an attribute set, with each attribute as listed in the Docker Image Specification v1.3.0.</p> <p>If any packages are used directly in <code>config</code>, they will be automatically included in the generated image.   See  for an example.</p> <p>Default value: <code>null</code>.</p> <p><code>architecture</code> (String; optional)</p> <p>: Used to specify the image architecture.   This is useful for multi-architecture builds that don't need cross compiling.   If specified, its value should follow the OCI Image Configuration Specification, which should still be compatible with Docker.   According to the linked specification, all possible values for <code>$GOARCH</code> in the Go docs should be valid, but will commonly be one of <code>386</code>, <code>amd64</code>, <code>arm</code>, or <code>arm64</code>.</p> <p>Default value: the same value from <code>pkgs.go.GOARCH</code>.</p> <p><code>created</code> (String; optional)</p> <p>: Specifies the time of creation of the generated image.   This should be either a date and time formatted according to ISO-8601 or <code>\"now\"</code>, in which case the current date will be used.</p> <p>:::{.caution}   Using <code>\"now\"</code> means that the generated image will not be reproducible anymore (because the date will always change whenever it's built).   :::</p> <p>Default value: <code>\"1970-01-01T00:00:01Z\"</code>.</p> <p><code>maxLayers</code> (Number; optional) []{#dockerTools-buildLayeredImage-arg-maxLayers}</p> <p>: The maximum number of layers that will be used by the generated image.   If a <code>fromImage</code> was specified, the number of layers used by <code>fromImage</code> will be subtracted from <code>maxLayers</code> to ensure that the image generated will have at most <code>maxLayers</code>.</p> <p>:::{.caution}   Depending on the tool/runtime where the image will be used, there might be a limit to the number of layers that an image can have.   For Docker, see this issue on GitHub.   :::</p> <p>Default value: 100.</p> <p><code>extraCommands</code> (String; optional)</p> <p>: A bash script that will run in the context of the layer created with the contents specified by <code>contents</code>.   At the moment this script runs, only the contents directly specified by <code>contents</code> will be available as links.</p> <p>Default value: <code>\"\"</code>.</p> <p><code>fakeRootCommands</code> (String; optional)</p> <p>: A bash script that will run in the context of the layer created with the contents specified by <code>contents</code>.   During the process to generate that layer, the script in <code>extraCommands</code> will be run first, if specified.   After that, a {manpage}<code>fakeroot(1)</code> environment will be entered.   The script specified in <code>fakeRootCommands</code> runs inside the fakeroot environment, and the layer is then generated from the view of the files inside the fakeroot environment.</p> <p>This is useful to change the owners of the files in the layer (by running <code>chown</code>, for example), or performing any other privileged operations related to file manipulation (by default, all files in the layer will be owned by root, and the build environment doesn't have enough privileges to directly perform privileged operations on these files).</p> <p>For more details, see the manpage for {manpage}<code>fakeroot(1)</code>.</p> <p>:::{.caution}   Due to how fakeroot works, static binaries cannot perform privileged file operations in <code>fakeRootCommands</code>, unless <code>enableFakechroot</code> is set to <code>true</code>.   :::</p> <p>Default value: <code>\"\"</code>.</p> <p><code>enableFakechroot</code> (Boolean; optional)</p> <p>: By default, the script specified in <code>fakeRootCommands</code> only runs inside a fakeroot environment.   If <code>enableFakechroot</code> is <code>true</code>, a more complete chroot environment will be created using <code>proot</code> before running the script in <code>fakeRootCommands</code>.   Files in the Nix store will be available.   This allows scripts that perform installation in <code>/</code> to work as expected.   This can be seen as an equivalent of <code>RUN ...</code> in a <code>Dockerfile</code>.</p> <p>Default value: <code>false</code></p> <p><code>includeStorePaths</code> (Boolean; optional)</p> <p>: The files specified in <code>contents</code> are put into layers in the generated image.   If <code>includeStorePaths</code> is <code>false</code>, the actual files will not be included in the generated image, and only links to them will be added instead.   It is not recommended to set this to <code>false</code> unless you have other tooling to insert the store paths via other means (such as bind mounting the host store) when running containers with the generated image.   If you don't provide any extra tooling, the generated image won't run properly.</p> <p>See  to understand the impact of setting <code>includeStorePaths</code> to <code>false</code>.</p> <p>Default value: <code>true</code></p> <p><code>passthru</code> (Attribute Set; optional)</p> <p>: Use this to pass any attributes as passthru for the resulting derivation.</p> <p>Default value: <code>{}</code></p>"},{"location":"build-helpers/images/dockertools.section.html#ssec-pkgs-dockerTools-streamLayeredImage-passthru-outputs","title":"Passthru outputs","text":"<p><code>streamLayeredImage</code> also defines its own <code>passthru</code> attributes:</p> <p><code>imageTag</code> (String)</p> <p>: The tag of the generated image.   This is useful if no tag was specified in the attributes of the argument to the function, because an automatic tag will be used instead.   <code>imageTag</code> allows you to retrieve the value of the tag used in this case.</p>"},{"location":"build-helpers/images/dockertools.section.html#ssec-pkgs-dockerTools-streamLayeredImage-examples","title":"Examples","text":"<p>:::{.example #ex-dockerTools-streamLayeredImage-hello}</p>"},{"location":"build-helpers/images/dockertools.section.html#streaming-a-layered-docker-image","title":"Streaming a layered Docker image","text":"<p>The following package builds a script which, when run, will stream a layered Docker image that runs the <code>hello</code> executable from the <code>hello</code> package. The Docker image will have name <code>hello</code> and tag <code>latest</code>.</p> <pre><code>{ dockerTools, hello }:\ndockerTools.streamLayeredImage {\n  name = \"hello\";\n  tag = \"latest\";\n\n  contents = [ hello ];\n\n  config.Cmd = [ \"/bin/hello\" ];\n}\n</code></pre> <p>The result of building this package is a script. Running this script and piping it into <code>docker load</code> gives you the same image that was built in . Note that in this case, the image is never added to the Nix store, but instead streamed directly into Docker.</p> <p><pre><code>$ nix-build\n(output removed for clarity)\n/nix/store/wsz2xl8ckxnlb769irvq6jv1280dfvxd-stream-hello\n\n$ /nix/store/wsz2xl8ckxnlb769irvq6jv1280dfvxd-stream-hello | docker load\nNo 'fromImage' provided\nCreating layer 1 from paths: ['/nix/store/i93s7xxblavsacpy82zdbn4kplsyq48l-libunistring-1.1']\nCreating layer 2 from paths: ['/nix/store/ji01n9vinnj22nbrb86nx8a1ssgpilx8-libidn2-2.3.4']\nCreating layer 3 from paths: ['/nix/store/ldrslljw4rg026nw06gyrdwl78k77vyq-xgcc-12.3.0-libgcc']\nCreating layer 4 from paths: ['/nix/store/9y8pmvk8gdwwznmkzxa6pwyah52xy3nk-glibc-2.38-27']\nCreating layer 5 from paths: ['/nix/store/zhl06z4lrfrkw5rp0hnjjfrgsclzvxpm-hello-2.12.1']\nCreating layer 6 with customisation...\nAdding manifests...\nDone.\n(some output removed for clarity)\nLoaded image: hello:latest\n</code></pre> :::</p> <p>:::{.example #ex-dockerTools-streamLayeredImage-exploringlayers}</p>"},{"location":"build-helpers/images/dockertools.section.html#exploring-the-layers-in-an-image-built-with-streamlayeredimage","title":"Exploring the layers in an image built with <code>streamLayeredImage</code>","text":"<p>Assume the following package, which builds a layered Docker image with the <code>hello</code> package.</p> <pre><code>{ dockerTools, hello }:\ndockerTools.streamLayeredImage {\n  name = \"hello\";\n  contents = [ hello ];\n}\n</code></pre> <p>The <code>hello</code> package depends on 4 other packages:</p> <pre><code>$ nix-store --query -R $(nix-build -A hello)\n/nix/store/i93s7xxblavsacpy82zdbn4kplsyq48l-libunistring-1.1\n/nix/store/ji01n9vinnj22nbrb86nx8a1ssgpilx8-libidn2-2.3.4\n/nix/store/ldrslljw4rg026nw06gyrdwl78k77vyq-xgcc-12.3.0-libgcc\n/nix/store/9y8pmvk8gdwwznmkzxa6pwyah52xy3nk-glibc-2.38-27\n/nix/store/zhl06z4lrfrkw5rp0hnjjfrgsclzvxpm-hello-2.12.1\n</code></pre> <p>This means that all these packages will be included in the image generated by <code>streamLayeredImage</code>. It will put each package in its own layer, for a total of 5 layers with actual files in them. A final layer will be created only with symlinks for the <code>hello</code> package.</p> <p>The image generated will have the following directory structure (some directories were collapsed for readability):</p> <pre><code>\u251c\u2500\u2500 bin\n\u2502   \u2514\u2500\u2500 hello \u2192 /nix/store/zhl06z4lrfrkw5rp0hnjjfrgsclzvxpm-hello-2.12.1/bin/hello\n\u251c\u2500\u2500 nix\n\u2502   \u2514\u2500\u2500 store\n\u2502       \u251c\u2500\u2295 9y8pmvk8gdwwznmkzxa6pwyah52xy3nk-glibc-2.38-27\n\u2502       \u251c\u2500\u2295 i93s7xxblavsacpy82zdbn4kplsyq48l-libunistring-1.1\n\u2502       \u251c\u2500\u2295 ji01n9vinnj22nbrb86nx8a1ssgpilx8-libidn2-2.3.4\n\u2502       \u251c\u2500\u2295 ldrslljw4rg026nw06gyrdwl78k77vyq-xgcc-12.3.0-libgcc\n\u2502       \u2514\u2500\u2295 zhl06z4lrfrkw5rp0hnjjfrgsclzvxpm-hello-2.12.1\n\u2514\u2500\u2500 share\n    \u251c\u2500\u2500 info\n    \u2502   \u2514\u2500\u2500 hello.info \u2192 /nix/store/zhl06z4lrfrkw5rp0hnjjfrgsclzvxpm-hello-2.12.1/share/info/hello.info\n    \u251c\u2500\u2295 locale\n    \u2514\u2500\u2500 man\n        \u2514\u2500\u2500 man1\n            \u2514\u2500\u2500 hello.1.gz \u2192 /nix/store/zhl06z4lrfrkw5rp0hnjjfrgsclzvxpm-hello-2.12.1/share/man/man1/hello.1.gz\n</code></pre> <p>Each of the packages in <code>/nix/store</code> comes from a layer in the image. The final layer adds the <code>/bin</code> and <code>/share</code> directories, but they only contain links to the actual files in <code>/nix/store</code>.</p> <p>If our package sets <code>includeStorePaths</code> to <code>false</code>, we'll end up with only the final layer with the links, but the actual files won't exist in the image:</p> <pre><code>{ dockerTools, hello }:\ndockerTools.streamLayeredImage {\n  name = \"hello\";\n  contents = [ hello ];\n  includeStorePaths = false;\n}\n</code></pre> <p>After building this package, the image will have the following directory structure:</p> <pre><code>\u251c\u2500\u2500 bin\n\u2502   \u2514\u2500\u2500 hello \u2192 /nix/store/zhl06z4lrfrkw5rp0hnjjfrgsclzvxpm-hello-2.12.1/bin/hello\n\u2514\u2500\u2500 share\n    \u251c\u2500\u2500 info\n    \u2502   \u2514\u2500\u2500 hello.info \u2192 /nix/store/zhl06z4lrfrkw5rp0hnjjfrgsclzvxpm-hello-2.12.1/share/info/hello.info\n    \u251c\u2500\u2295 locale\n    \u2514\u2500\u2500 man\n        \u2514\u2500\u2500 man1\n            \u2514\u2500\u2500 hello.1.gz \u2192 /nix/store/zhl06z4lrfrkw5rp0hnjjfrgsclzvxpm-hello-2.12.1/share/man/man1/hello.1.gz\n</code></pre> <p>Note how the links point to paths in <code>/nix/store</code>, but they're not included in the image itself. This is why you need extra tooling when using <code>includeStorePaths</code>: a container created from such image won't find any of the files it needs to run otherwise. :::</p> <p>::: {.example #ex-dockerTools-streamLayeredImage-configclosure}</p>"},{"location":"build-helpers/images/dockertools.section.html#building-a-layered-docker-image-with-packages-directly-in-config","title":"Building a layered Docker image with packages directly in <code>config</code>","text":"<p>The closure of <code>config</code> is automatically included in the generated image. The following package shows a more compact way to create the same output generated in .</p> <p><pre><code>{ dockerTools, hello, lib }:\ndockerTools.streamLayeredImage {\n  name = \"hello\";\n  tag = \"latest\";\n  config.Cmd = [ \"${lib.getExe hello}\" ];\n}\n</code></pre> :::</p> <p>[]{#ssec-pkgs-dockerTools-fetchFromRegistry}</p>"},{"location":"build-helpers/images/dockertools.section.html#ssec-pkgs-dockerTools-pullImage","title":"pullImage","text":"<p>This function is similar to the <code>docker pull</code> command, which means it can be used to pull a Docker image from a registry that implements the Docker Registry HTTP API V2. By default, the <code>docker.io</code> registry is used.</p> <p>The image will be downloaded as an uncompressed Docker-compatible repository tarball, which is suitable for use with other <code>dockerTools</code> functions such as <code>buildImage</code>, <code>buildLayeredImage</code>, and <code>streamLayeredImage</code>.</p> <p>This function requires two different types of hashes/digests to be specified:</p> <ul> <li>One of them is used to identify a unique image within the registry (see the documentation for the <code>imageDigest</code> attribute).</li> <li>The other is used by Nix to ensure the contents of the output haven't changed (see the documentation for the <code>sha256</code> attribute).</li> </ul> <p>Both hashes are required because they must uniquely identify some content in two completely different systems (the Docker registry and the Nix store), but their values will not be the same. See  for a tool that can help gather these values.</p>"},{"location":"build-helpers/images/dockertools.section.html#ssec-pkgs-dockerTools-pullImage-inputs","title":"Inputs","text":"<p><code>pullImage</code> expects a single argument with the following attributes:</p> <p><code>imageName</code> (String)</p> <p>: Specifies the name of the image to be downloaded, as well as the registry endpoint.   By default, the <code>docker.io</code> registry is used.   To specify a different registry, prepend the endpoint to <code>imageName</code>, separated by a slash (<code>/</code>).   See  for how to do that.</p> <p><code>imageDigest</code> (String)</p> <p>: Specifies the digest of the image to be downloaded.</p> <p>:::{.tip}   Why can't I specify a tag to pull from, and have to use a digest instead?</p> <p>Tags are often updated to point to different image contents.   The most common example is the <code>latest</code> tag, which is usually updated whenever a newer image version is available.</p> <p>An image tag isn't enough to guarantee the contents of an image won't change, but a digest guarantees this.   Providing a digest helps ensure that you will still be able to build the same Nix code and get the same output even if newer versions of an image are released.   :::</p> <p><code>sha256</code> (String)</p> <p>: The hash of the image after it is downloaded.   Internally, this is passed to the <code>outputHash</code> attribute of the resulting derivation.   This is needed to provide a guarantee to Nix that the contents of the image haven't changed, because Nix doesn't support the value in <code>imageDigest</code>.</p> <p><code>finalImageName</code> (String; optional)</p> <p>: Specifies the name that will be used for the image after it has been downloaded.   This only applies after the image is downloaded, and is not used to identify the image to be downloaded in the registry.   Use <code>imageName</code> for that instead.</p> <p>Default value: the same value specified in <code>imageName</code>.</p> <p><code>finalImageTag</code> (String; optional)</p> <p>: Specifies the tag that will be used for the image after it has been downloaded.   This only applies after the image is downloaded, and is not used to identify the image to be downloaded in the registry.</p> <p>Default value: <code>\"latest\"</code>.</p> <p><code>os</code> (String; optional)</p> <p>: Specifies the operating system of the image to pull.   If specified, its value should follow the OCI Image Configuration Specification, which should still be compatible with Docker.   According to the linked specification, all possible values for <code>$GOOS</code> in the Go docs should be valid, but will commonly be one of <code>darwin</code> or <code>linux</code>.</p> <p>Default value: <code>\"linux\"</code>.</p> <p><code>arch</code> (String; optional)</p> <p>: Specifies the architecture of the image to pull.   If specified, its value should follow the OCI Image Configuration Specification, which should still be compatible with Docker.   According to the linked specification, all possible values for <code>$GOARCH</code> in the Go docs should be valid, but will commonly be one of <code>386</code>, <code>amd64</code>, <code>arm</code>, or <code>arm64</code>.</p> <p>Default value: the same value from <code>pkgs.go.GOARCH</code>.</p> <p><code>tlsVerify</code> (Boolean; optional)</p> <p>: Used to enable or disable HTTPS and TLS certificate verification when communicating with the chosen Docker registry.   Setting this to <code>false</code> will make <code>pullImage</code> connect to the registry through HTTP.</p> <p>Default value: <code>true</code>.</p> <p><code>name</code> (String; optional)</p> <p>: The name used for the output in the Nix store path.</p> <p>Default value: a value derived from <code>finalImageName</code> and <code>finalImageTag</code>, with some symbols replaced.   It is recommended to treat the default as an opaque value.</p>"},{"location":"build-helpers/images/dockertools.section.html#ssec-pkgs-dockerTools-pullImage-examples","title":"Examples","text":"<p>::: {.example #ex-dockerTools-pullImage-niximage}</p>"},{"location":"build-helpers/images/dockertools.section.html#pulling-the-nixosnix-docker-image-from-the-default-registry","title":"Pulling the nixos/nix Docker image from the default registry","text":"<p>This example pulls the <code>nixos/nix</code> image and saves it in the Nix store.</p> <p><pre><code>{ dockerTools }:\ndockerTools.pullImage {\n  imageName = \"nixos/nix\";\n  imageDigest = \"sha256:b8ea88f763f33dfda2317b55eeda3b1a4006692ee29e60ee54ccf6d07348c598\";\n  finalImageName = \"nix\";\n  finalImageTag = \"2.19.3\";\n  sha256 = \"zRwlQs1FiKrvHPaf8vWOR/Tlp1C5eLn1d9pE4BZg3oA=\";\n}\n</code></pre> :::</p> <p>::: {.example #ex-dockerTools-pullImage-differentregistry}</p>"},{"location":"build-helpers/images/dockertools.section.html#pulling-the-nixosnix-docker-image-from-a-specific-registry","title":"Pulling the nixos/nix Docker image from a specific registry","text":"<p>This example pulls the <code>coreos/etcd</code> image from the <code>quay.io</code> registry.</p> <p><pre><code>{ dockerTools }:\ndockerTools.pullImage {\n  imageName = \"quay.io/coreos/etcd\";\n  imageDigest = \"sha256:24a23053f29266fb2731ebea27f915bb0fb2ae1ea87d42d890fe4e44f2e27c5d\";\n  finalImageName = \"etcd\";\n  finalImageTag = \"v3.5.11\";\n  sha256 = \"Myw+85f2/EVRyMB3axECdmQ5eh9p1q77FWYKy8YpRWU=\";\n}\n</code></pre> :::</p> <p>::: {.example #ex-dockerTools-pullImage-nixprefetchdocker}</p>"},{"location":"build-helpers/images/dockertools.section.html#finding-the-digest-and-hash-values-to-use-for-dockertoolspullimage","title":"Finding the digest and hash values to use for <code>dockerTools.pullImage</code>","text":"<p>Since <code>dockerTools.pullImage</code> requires two different hashes, one can run the <code>nix-prefetch-docker</code> tool to find out the values for the hashes. The tool outputs some text for an attribute set which you can pass directly to <code>pullImage</code>.</p> <pre><code>$ nix run nixpkgs#nix-prefetch-docker -- --image-name nixos/nix --image-tag 2.19.3 --arch amd64 --os linux\n(some output removed for clarity)\nWriting manifest to image destination\n-&gt; ImageName: nixos/nix\n-&gt; ImageDigest: sha256:498fa2d7f2b5cb3891a4edf20f3a8f8496e70865099ba72540494cd3e2942634\n-&gt; FinalImageName: nixos/nix\n-&gt; FinalImageTag: latest\n-&gt; ImagePath: /nix/store/4mxy9mn6978zkvlc670g5703nijsqc95-docker-image-nixos-nix-latest.tar\n-&gt; ImageHash: 1q6cf2pdrasa34zz0jw7pbs6lvv52rq2aibgxccbwcagwkg2qj1q\n{\n  imageName = \"nixos/nix\";\n  imageDigest = \"sha256:498fa2d7f2b5cb3891a4edf20f3a8f8496e70865099ba72540494cd3e2942634\";\n  sha256 = \"1q6cf2pdrasa34zz0jw7pbs6lvv52rq2aibgxccbwcagwkg2qj1q\";\n  finalImageName = \"nixos/nix\";\n  finalImageTag = \"latest\";\n}\n</code></pre> <p>It is important to supply the <code>--arch</code> and <code>--os</code> arguments to <code>nix-prefetch-docker</code> to filter to a single image, in case there are multiple architectures and/or operating systems supported by the image name and tags specified. By default, <code>nix-prefetch-docker</code> will set <code>os</code> to <code>linux</code> and <code>arch</code> to <code>amd64</code>.</p> <p>Run <code>nix-prefetch-docker --help</code> for a list of all supported arguments: <pre><code>$ nix run nixpkgs#nix-prefetch-docker -- --help\n(output removed for clarity)\n</code></pre> :::</p>"},{"location":"build-helpers/images/dockertools.section.html#ssec-pkgs-dockerTools-exportImage","title":"exportImage","text":"<p>This function is similar to the <code>docker container export</code> command, which means it can be used to export an image's filesystem as an uncompressed tarball archive. The difference is that <code>docker container export</code> is applied to containers, but <code>dockerTools.exportImage</code> applies to Docker images. The resulting archive will not contain any image metadata (such as command to run with <code>docker container run</code>), only the filesystem contents.</p> <p>You can use this function to import an archive in Docker with <code>docker image import</code>. See  to understand how to do that.</p> <p>:::{.caution} <code>exportImage</code> works by unpacking the given image inside a VM. Because of this, using this function requires the <code>kvm</code> device to be available, see <code>system-features</code>. :::</p>"},{"location":"build-helpers/images/dockertools.section.html#ssec-pkgs-dockerTools-exportImage-inputs","title":"Inputs","text":"<p><code>exportImage</code> expects an argument with the following attributes:</p> <p><code>fromImage</code> (Attribute Set or String)</p> <p>: The repository tarball of the image whose filesystem will be exported.   It must be a valid Docker image, such as one exported by <code>docker image save</code>, or another image built with the <code>dockerTools</code> utility functions.</p> <p>If <code>name</code> is not specified, <code>fromImage</code> must be an Attribute Set corresponding to a derivation, i.e. it can't be a path to a tarball.   If <code>name</code> is specified, <code>fromImage</code> can be either an Attribute Set corresponding to a derivation or simply a path to a tarball.</p> <p>See  and  to understand the connection between <code>fromImage</code>, <code>name</code>, and the name used for the output of <code>exportImage</code>.</p> <p><code>fromImageName</code> (String or Null; optional)</p> <p>: Used to specify the image within the repository tarball in case it contains multiple images.   A value of <code>null</code> means that <code>exportImage</code> will use the first image available in the repository.</p> <p>:::{.note}   This must be used with <code>fromImageTag</code>. Using only <code>fromImageName</code> without <code>fromImageTag</code> will make <code>exportImage</code> use the first image available in the repository.   :::</p> <p>Default value: <code>null</code>.</p> <p><code>fromImageTag</code> (String or Null; optional)</p> <p>: Used to specify the image within the repository tarball in case it contains multiple images.   A value of <code>null</code> means that <code>exportImage</code> will use the first image available in the repository.</p> <p>:::{.note}   This must be used with <code>fromImageName</code>. Using only <code>fromImageTag</code> without <code>fromImageName</code> will make <code>exportImage</code> use the first image available in the repository   :::</p> <p>Default value: <code>null</code>.</p> <p><code>diskSize</code> (Number; optional)</p> <p>: Controls the disk size (in megabytes) of the VM used to unpack the image.</p> <p>Default value: 1024.</p> <p><code>name</code> (String; optional)</p> <p>: The name used for the output in the Nix store path.</p> <p>Default value: the value of <code>fromImage.name</code>.</p>"},{"location":"build-helpers/images/dockertools.section.html#ssec-pkgs-dockerTools-exportImage-examples","title":"Examples","text":"<p>:::{.example #ex-dockerTools-exportImage-hello}</p>"},{"location":"build-helpers/images/dockertools.section.html#exporting-a-docker-image-with-dockertoolsexportimage","title":"Exporting a Docker image with <code>dockerTools.exportImage</code>","text":"<p>This example first builds a layered image with <code>dockerTools.buildLayeredImage</code>, and then exports its filesystem with <code>dockerTools.exportImage</code>.</p> <pre><code>{ dockerTools, hello }:\ndockerTools.exportImage {\n  name = \"hello\";\n  fromImage = dockerTools.buildLayeredImage {\n    name = \"hello\";\n    contents = [ hello ];\n  };\n}\n</code></pre> <p>When building the package above, we can see the layers of the Docker image being unpacked to produce the final output:</p> <pre><code>$ nix-build\n(some output removed for clarity)\nUnpacking base image...\nFrom-image name or tag wasn't set. Reading the first ID.\nUnpacking layer 5731199219418f175d1580dbca05677e69144425b2d9ecb60f416cd57ca3ca42/layer.tar\ntar: Removing leading `/' from member names\nUnpacking layer e2897bf34bb78c4a65736510204282d9f7ca258ba048c183d665bd0f3d24c5ec/layer.tar\ntar: Removing leading `/' from member names\nUnpacking layer 420aa5876dca4128cd5256da7dea0948e30ef5971712f82601718cdb0a6b4cda/layer.tar\ntar: Removing leading `/' from member names\nUnpacking layer ea5f4e620e7906c8ecbc506b5e6f46420e68d4b842c3303260d5eb621b5942e5/layer.tar\ntar: Removing leading `/' from member names\nUnpacking layer 65807b9abe8ab753fa97da8fb74a21fcd4725cc51e1b679c7973c97acd47ebcf/layer.tar\ntar: Removing leading `/' from member names\nUnpacking layer b7da2076b60ebc0ea6824ef641978332b8ac908d47b2d07ff31b9cc362245605/layer.tar\nExecuting post-mount steps...\nPacking raw image...\n[    1.660036] reboot: Power down\n/nix/store/x6a5m7c6zdpqz1d8j7cnzpx9glzzvd2h-hello\n</code></pre> <p>The following command lists some of the contents of the output to verify that the structure of the archive is as expected:</p> <p><pre><code>$ tar --exclude '*/share/*' --exclude 'nix/store/*/*' -tvf /nix/store/x6a5m7c6zdpqz1d8j7cnzpx9glzzvd2h-hello\ndrwxr-xr-x root/0            0 1979-12-31 16:00 ./\ndrwxr-xr-x root/0            0 1979-12-31 16:00 ./bin/\nlrwxrwxrwx root/0            0 1979-12-31 16:00 ./bin/hello -&gt; /nix/store/h92a9jd0lhhniv2q417hpwszd4jhys7q-hello-2.12.1/bin/hello\ndr-xr-xr-x root/0            0 1979-12-31 16:00 ./nix/\ndr-xr-xr-x root/0            0 1979-12-31 16:00 ./nix/store/\ndr-xr-xr-x root/0            0 1979-12-31 16:00 ./nix/store/05zbwhz8a7i2v79r9j21pl6m6cj0xi8k-libunistring-1.1/\ndr-xr-xr-x root/0            0 1979-12-31 16:00 ./nix/store/ayg5rhjhi9ic73hqw33mjqjxwv59ndym-xgcc-13.2.0-libgcc/\ndr-xr-xr-x root/0            0 1979-12-31 16:00 ./nix/store/h92a9jd0lhhniv2q417hpwszd4jhys7q-hello-2.12.1/\ndr-xr-xr-x root/0            0 1979-12-31 16:00 ./nix/store/m59xdgkgnjbk8kk6k6vbxmqnf82mk9s0-libidn2-2.3.4/\ndr-xr-xr-x root/0            0 1979-12-31 16:00 ./nix/store/p3jshbwxiwifm1py0yq544fmdyy98j8a-glibc-2.38-27/\ndrwxr-xr-x root/0            0 1979-12-31 16:00 ./share/\n</code></pre> :::</p> <p>:::{.example #ex-dockerTools-exportImage-importingDocker}</p>"},{"location":"build-helpers/images/dockertools.section.html#importing-an-archive-built-with-dockertoolsexportimage-in-docker","title":"Importing an archive built with <code>dockerTools.exportImage</code> in Docker","text":"<p>We will use the same package from  and import it into Docker.</p> <pre><code>{ dockerTools, hello }:\ndockerTools.exportImage {\n  name = \"hello\";\n  fromImage = dockerTools.buildLayeredImage {\n    name = \"hello\";\n    contents = [ hello ];\n  };\n}\n</code></pre> <p>Building and importing it into Docker:</p> <p><pre><code>$ nix-build\n(output removed for clarity)\n/nix/store/x6a5m7c6zdpqz1d8j7cnzpx9glzzvd2h-hello\n$ docker image import /nix/store/x6a5m7c6zdpqz1d8j7cnzpx9glzzvd2h-hello\nsha256:1d42dba415e9b298ea0decf6497fbce954de9b4fcb2984f91e307c8fedc1f52f\n$ docker image ls\nREPOSITORY                              TAG                IMAGE ID       CREATED         SIZE\n&lt;none&gt;                                  &lt;none&gt;             1d42dba415e9   4 seconds ago   32.6MB\n</code></pre> :::</p> <p>:::{.example #ex-dockerTools-exportImage-naming}</p>"},{"location":"build-helpers/images/dockertools.section.html#exploring-output-naming-with-dockertoolsexportimage","title":"Exploring output naming with <code>dockerTools.exportImage</code>","text":"<p><code>exportImage</code> does not require a <code>name</code> attribute if <code>fromImage</code> is a derivation, which means that the following works:</p> <pre><code>{ dockerTools, hello }:\ndockerTools.exportImage {\n  fromImage = dockerTools.buildLayeredImage {\n    name = \"hello\";\n    contents = [ hello ];\n  };\n}\n</code></pre> <p>However, since <code>dockerTools.buildLayeredImage</code>'s output ends with <code>.tar.gz</code>, the output of <code>exportImage</code> will also end with <code>.tar.gz</code>, even though the archive created with <code>exportImage</code> is uncompressed:</p> <pre><code>$ nix-build\n(output removed for clarity)\n/nix/store/by3f40xvc4l6bkis74l0fj4zsy0djgkn-hello.tar.gz\n$ file /nix/store/by3f40xvc4l6bkis74l0fj4zsy0djgkn-hello.tar.gz\n/nix/store/by3f40xvc4l6bkis74l0fj4zsy0djgkn-hello.tar.gz: POSIX tar archive (GNU)\n</code></pre> <p>If the archive was actually compressed, the output of file would've mentioned that fact. Because of this, it may be important to set a proper <code>name</code> attribute when using <code>exportImage</code> with other functions from <code>dockerTools</code>. :::</p> <p>:::{.example #ex-dockerTools-exportImage-fromImagePath}</p>"},{"location":"build-helpers/images/dockertools.section.html#using-dockertoolsexportimage-with-a-path-as-fromimage","title":"Using <code>dockerTools.exportImage</code> with a path as <code>fromImage</code>","text":"<p>It is possible to use a path as the value of the <code>fromImage</code> attribute when calling <code>dockerTools.exportImage</code>. However, when doing so, a <code>name</code> attribute MUST be specified, or you'll encounter an error when evaluating the Nix code.</p> <p>For this example, we'll assume a Docker tarball image named <code>image.tar.gz</code> exists in the same directory where our package is defined:</p> <pre><code>{ dockerTools }:\ndockerTools.exportImage {\n  name = \"filesystem.tar\";\n  fromImage = ./image.tar.gz;\n}\n</code></pre> <p>Building this will give us the expected output:</p> <pre><code>$ nix-build\n(output removed for clarity)\n/nix/store/w13l8h3nlkg0zv56k7rj0ai0l2zlf7ss-filesystem.tar\n</code></pre> <p>If you don't specify a <code>name</code> attribute, you'll encounter an evaluation error and the package won't build. :::</p>"},{"location":"build-helpers/images/dockertools.section.html#ssec-pkgs-dockerTools-helpers","title":"Environment Helpers","text":"<p>When building Docker images with Nix, you might also want to add certain files that are expected to be available globally by the software you're packaging. Simple examples are the <code>env</code> utility in <code>/usr/bin/env</code>, or trusted root TLS/SSL certificates. Such files will most likely not be included if you're building a Docker image from scratch with Nix, and they might also not be included if you're starting from a Docker image that doesn't include them. The helpers in this section are packages that provide some of these commonly-needed global files.</p> <p>Most of these helpers are packages, which means you have to add them to the list of contents to be included in the image (this changes depending on the function you're using to build the image).  and  show how to include these packages on <code>dockerTools</code> functions that build an image. For more details on how that works, see the documentation for the function you're using.</p>"},{"location":"build-helpers/images/dockertools.section.html#sssec-pkgs-dockerTools-helpers-usrBinEnv","title":"usrBinEnv","text":"<p>This provides the <code>env</code> utility at <code>/usr/bin/env</code>. This is currently implemented by linking to the <code>env</code> binary from the <code>coreutils</code> package, but is considered an implementation detail that could change in the future.</p>"},{"location":"build-helpers/images/dockertools.section.html#sssec-pkgs-dockerTools-helpers-binSh","title":"binSh","text":"<p>This provides a <code>/bin/sh</code> link to the <code>bash</code> binary from the <code>bashInteractive</code> package. Because of this, it supports cases such as running a command interactively in a container (for example by running <code>docker run -it &lt;image_name&gt;</code>).</p>"},{"location":"build-helpers/images/dockertools.section.html#sssec-pkgs-dockerTools-helpers-caCertificates","title":"caCertificates","text":"<p>This adds trusted root TLS/SSL certificates from the <code>cacert</code> package in multiple locations in an attempt to be compatible with binaries built for multiple Linux distributions. The locations currently used are:</p> <ul> <li><code>/etc/ssl/certs/ca-bundle.crt</code></li> <li><code>/etc/ssl/certs/ca-certificates.crt</code></li> <li><code>/etc/pki/tls/certs/ca-bundle.crt</code></li> </ul> <p>[]{#ssec-pkgs-dockerTools-fakeNss}</p>"},{"location":"build-helpers/images/dockertools.section.html#sssec-pkgs-dockerTools-helpers-fakeNss","title":"fakeNss","text":"<p>This is a re-export of the <code>fakeNss</code> package from Nixpkgs. See .</p>"},{"location":"build-helpers/images/dockertools.section.html#ssec-pkgs-dockerTools-shadowSetup","title":"shadowSetup","text":"<p>This is a string containing a script that sets up files needed for <code>shadow</code> to work (using the <code>shadow</code> package from Nixpkgs), and alters <code>PATH</code> to make all its utilities available in the same script. It is intended to be used with other dockerTools functions in attributes that expect scripts. After the script in <code>shadowSetup</code> runs, you'll then be able to add more commands that make use of the utilities in <code>shadow</code>, such as adding any extra users and/or groups. See  and  to better understand how to use it.</p> <p><code>shadowSetup</code> achieves a result similar to <code>fakeNss</code>, but only sets up a <code>root</code> user with different values for the home directory and the shell to use, in addition to setting up files for PAM and a {manpage}<code>login.defs(5)</code> file.</p> <p>:::{.caution} Using both <code>fakeNss</code> and <code>shadowSetup</code> at the same time will either cause your build to break or produce unexpected results. Use either <code>fakeNss</code> or <code>shadowSetup</code> depending on your use case, but avoid using both. :::</p> <p>:::{.note} When used with <code>buildLayeredImage</code> or <code>streamLayeredImage</code>, you will have to set the <code>enableFakechroot</code> attribute to <code>true</code>, or else the script in <code>shadowSetup</code> won't run properly. See . :::</p>"},{"location":"build-helpers/images/dockertools.section.html#ssec-pkgs-dockerTools-helpers-examples","title":"Examples","text":"<p>:::{.example #ex-dockerTools-helpers-buildImage}</p>"},{"location":"build-helpers/images/dockertools.section.html#using-dockertoolss-environment-helpers-with-buildimage","title":"Using <code>dockerTools</code>'s environment helpers with <code>buildImage</code>","text":"<p>This example adds the <code>binSh</code> helper to a basic Docker image built with <code>dockerTools.buildImage</code>. This helper makes it possible to enter a shell inside the container. This is the <code>buildImage</code> equivalent of .</p> <pre><code>{ dockerTools, hello }:\ndockerTools.buildImage {\n  name = \"env-helpers\";\n  tag = \"latest\";\n\n  copyToRoot = [\n    hello\n    dockerTools.binSh\n  ];\n</code></pre> <p>After building the image and loading it in Docker, we can create a container based on it and enter a shell inside the container. This is made possible by <code>binSh</code>.</p> <p><pre><code>$ nix-build\n(some output removed for clarity)\n/nix/store/2p0i3i04cgjlk71hsn7ll4kxaxxiv4qg-docker-image-env-helpers.tar.gz\n$ docker load -i /nix/store/2p0i3i04cgjlk71hsn7ll4kxaxxiv4qg-docker-image-env-helpers.tar.gz\n(output removed for clarity)\n$ docker run --rm -it env-helpers:latest /bin/sh\nsh-5.2# help\nGNU bash, version 5.2.21(1)-release (x86_64-pc-linux-gnu)\n(rest of output removed for clarity)\n</code></pre> :::</p> <p>:::{.example #ex-dockerTools-helpers-buildLayeredImage}</p>"},{"location":"build-helpers/images/dockertools.section.html#using-dockertoolss-environment-helpers-with-buildlayeredimage","title":"Using <code>dockerTools</code>'s environment helpers with <code>buildLayeredImage</code>","text":"<p>This example adds the <code>binSh</code> helper to a basic Docker image built with <code>dockerTools.buildLayeredImage</code>. This helper makes it possible to enter a shell inside the container. This is the <code>buildLayeredImage</code> equivalent of .</p> <pre><code>{ dockerTools, hello }:\ndockerTools.buildLayeredImage {\n  name = \"env-helpers\";\n  tag = \"latest\";\n\n  contents = [\n    hello\n    dockerTools.binSh\n  ];\n\n  config = {\n    Cmd = [ \"/bin/hello\" ];\n  };\n}\n</code></pre> <p>After building the image and loading it in Docker, we can create a container based on it and enter a shell inside the container. This is made possible by <code>binSh</code>.</p> <p><pre><code>$ nix-build\n(some output removed for clarity)\n/nix/store/rpf47f4z5b9qr4db4ach9yr4b85hjhxq-env-helpers.tar.gz\n$ docker load -i /nix/store/rpf47f4z5b9qr4db4ach9yr4b85hjhxq-env-helpers.tar.gz\n(output removed for clarity)\n$ docker run --rm -it env-helpers:latest /bin/sh\nsh-5.2# help\nGNU bash, version 5.2.21(1)-release (x86_64-pc-linux-gnu)\n(rest of output removed for clarity)\n</code></pre> :::</p> <p>:::{.example #ex-dockerTools-shadowSetup-buildImage}</p>"},{"location":"build-helpers/images/dockertools.section.html#using-dockertoolsshadowsetup-with-dockertoolsbuildimage","title":"Using <code>dockerTools.shadowSetup</code> with <code>dockerTools.buildImage</code>","text":"<p>This is an example that shows how to use <code>shadowSetup</code> with <code>dockerTools.buildImage</code>. Note that the extra script in <code>runAsRoot</code> uses <code>groupadd</code> and <code>useradd</code>, which are binaries provided by the <code>shadow</code> package. These binaries are added to the <code>PATH</code> by the <code>shadowSetup</code> script, but only for the duration of <code>runAsRoot</code>.</p> <p><pre><code>{ dockerTools, hello }:\ndockerTools.buildImage {\n  name = \"shadow-basic\";\n  tag = \"latest\";\n\n  copyToRoot = [ hello ];\n\n  runAsRoot = ''\n    ${dockerTools.shadowSetup}\n    groupadd -r hello\n    useradd -r -g hello hello\n    mkdir /data\n    chown hello:hello /data\n  '';\n\n  config = {\n    Cmd = [ \"/bin/hello\" ];\n    WorkingDir = \"/data\";\n  };\n}\n</code></pre> :::</p> <p>:::{.example #ex-dockerTools-shadowSetup-buildLayeredImage}</p>"},{"location":"build-helpers/images/dockertools.section.html#using-dockertoolsshadowsetup-with-dockertoolsbuildlayeredimage","title":"Using <code>dockerTools.shadowSetup</code> with <code>dockerTools.buildLayeredImage</code>","text":"<p>It accomplishes the same thing as , but using <code>buildLayeredImage</code> instead.</p> <p>Note that the extra script in <code>fakeRootCommands</code> uses <code>groupadd</code> and <code>useradd</code>, which are binaries provided by the <code>shadow</code> package. These binaries are added to the <code>PATH</code> by the <code>shadowSetup</code> script, but only for the duration of <code>fakeRootCommands</code>.</p> <p><pre><code>{ dockerTools, hello }:\ndockerTools.buildLayeredImage {\n  name = \"shadow-basic\";\n  tag = \"latest\";\n\n  contents = [ hello ];\n\n  fakeRootCommands = ''\n    ${dockerTools.shadowSetup}\n    groupadd -r hello\n    useradd -r -g hello hello\n    mkdir /data\n    chown hello:hello /data\n  '';\n  enableFakechroot = true;\n\n  config = {\n    Cmd = [ \"/bin/hello\" ];\n    WorkingDir = \"/data\";\n  };\n}\n</code></pre> :::</p> <p>[]{#ssec-pkgs-dockerTools-buildNixShellImage-arguments}</p>"},{"location":"build-helpers/images/dockertools.section.html#ssec-pkgs-dockerTools-buildNixShellImage","title":"buildNixShellImage","text":"<p><code>buildNixShellImage</code> uses <code>streamNixShellImage</code> underneath to build a compressed Docker-compatible repository tarball of an image that sets up an environment similar to that of running <code>nix-shell</code> on a derivation. Basically, <code>buildNixShellImage</code> runs the script created by <code>streamNixShellImage</code> to save the compressed image in the Nix store.</p> <p><code>buildNixShellImage</code> supports the same options as <code>streamNixShellImage</code>, see <code>streamNixShellImage</code> for details.</p> <p>[]{#ssec-pkgs-dockerTools-buildNixShellImage-example}</p>"},{"location":"build-helpers/images/dockertools.section.html#ssec-pkgs-dockerTools-buildNixShellImage-examples","title":"Examples","text":"<p>:::{.example #ex-dockerTools-buildNixShellImage-hello}</p>"},{"location":"build-helpers/images/dockertools.section.html#building-a-docker-image-with-buildnixshellimage-with-the-build-environment-for-the-hello-package","title":"Building a Docker image with <code>buildNixShellImage</code> with the build environment for the <code>hello</code> package","text":"<p>This example shows how to build the <code>hello</code> package inside a Docker container built with <code>buildNixShellImage</code>. The Docker image generated will have a name like <code>hello-&lt;version&gt;-env</code> and tag <code>latest</code>. This example is the <code>buildNixShellImage</code> equivalent of .</p> <pre><code>{ dockerTools, hello }:\ndockerTools.buildNixShellImage {\n  drv = hello;\n  tag = \"latest\";\n}\n</code></pre> <p>The result of building this package is a <code>.tar.gz</code> file that can be loaded into Docker:</p> <pre><code>$ nix-build\n(some output removed for clarity)\n/nix/store/pkj1sgzaz31wl0pbvbg3yp5b3kxndqms-hello-2.12.1-env.tar.gz\n\n$ docker load -i /nix/store/pkj1sgzaz31wl0pbvbg3yp5b3kxndqms-hello-2.12.1-env.tar.gz\n(some output removed for clarity)\nLoaded image: hello-2.12.1-env:latest\n</code></pre> <p>After starting an interactive container, the derivation can be built by running <code>buildDerivation</code>, and the output can be executed as expected:</p> <p><pre><code>$ docker run -it hello-2.12.1-env:latest\n[nix-shell:~]$ buildDerivation\nRunning phase: unpackPhase\nunpacking source archive /nix/store/pa10z4ngm0g83kx9mssrqzz30s84vq7k-hello-2.12.1.tar.gz\nsource root is hello-2.12.1\n(some output removed for clarity)\nRunning phase: fixupPhase\nshrinking RPATHs of ELF executables and libraries in /nix/store/f2vs29jibd7lwxyj35r9h87h6brgdysz-hello-2.12.1\nshrinking /nix/store/f2vs29jibd7lwxyj35r9h87h6brgdysz-hello-2.12.1/bin/hello\nchecking for references to /build/ in /nix/store/f2vs29jibd7lwxyj35r9h87h6brgdysz-hello-2.12.1...\ngzipping man pages under /nix/store/f2vs29jibd7lwxyj35r9h87h6brgdysz-hello-2.12.1/share/man/\npatching script interpreter paths in /nix/store/f2vs29jibd7lwxyj35r9h87h6brgdysz-hello-2.12.1\nstripping (with command strip and flags -S -p) in  /nix/store/f2vs29jibd7lwxyj35r9h87h6brgdysz-hello-2.12.1/bin\n\n[nix-shell:~]$ $out/bin/hello\nHello, world!\n</code></pre> :::</p>"},{"location":"build-helpers/images/dockertools.section.html#ssec-pkgs-dockerTools-streamNixShellImage","title":"streamNixShellImage","text":"<p><code>streamNixShellImage</code> builds a script which, when run, will stream to stdout a Docker-compatible repository tarball of an image that sets up an environment similar to that of running <code>nix-shell</code> on a derivation. This means that <code>streamNixShellImage</code> does not output an image into the Nix store, but only a script that builds the image, saving on IO and disk/cache space, particularly with large images. See  to understand how to load in Docker the image generated by this script.</p> <p>The environment set up by <code>streamNixShellImage</code> somewhat resembles the Nix sandbox typically used by <code>nix-build</code>, with a major difference being that access to the internet is allowed. It also behaves like an interactive <code>nix-shell</code>, running things like <code>shellHook</code> (see ) and setting an interactive prompt. If the derivation is buildable (i.e. <code>nix-build</code> can be used on it), running <code>buildDerivation</code> in the container will build the derivation, with all its outputs being available in the correct <code>/nix/store</code> paths, pointed to by the respective environment variables (e.g. <code>$out</code>).</p> <p>::: {.caution} The environment in the image doesn't match <code>nix-shell</code> or <code>nix-build</code> exactly, and this function is known not to work correctly for fixed-output derivations, content-addressed derivations, impure derivations and other special types of derivations. :::</p>"},{"location":"build-helpers/images/dockertools.section.html#ssec-pkgs-dockerTools-streamNixShellImage-inputs","title":"Inputs","text":"<p><code>streamNixShellImage</code> expects one argument with the following attributes:</p> <p><code>drv</code> (Attribute Set)</p> <p>: The derivation for which the environment in the image will be set up.   Adding packages to the Docker image is possible by extending the list of <code>nativeBuildInputs</code> of this derivation.   See  for how to do that.   Similarly, you can extend the image initialization script by extending <code>shellHook</code>.    shows how to do that.</p> <p><code>name</code> (String; optional)</p> <p>: The name of the generated image.</p> <p>Default value: the value of <code>drv.name + \"-env\"</code>.</p> <p><code>tag</code> (String or Null; optional)</p> <p>: Tag of the generated image.   If <code>null</code>, the hash of the nix derivation that builds the Docker image will be used as the tag.</p> <p>Default value: <code>null</code>.</p> <p><code>uid</code> (Number; optional)</p> <p>: The user ID to run the container as.   This can be seen as a <code>nixbld</code> build user.</p> <p>Default value: 1000.</p> <p><code>gid</code> (Number; optional)</p> <p>: The group ID to run the container as.   This can be seen as a <code>nixbld</code> build group.</p> <p>Default value: 1000.</p> <p><code>homeDirectory</code> (String; optional)</p> <p>: The home directory of the user the container is running as.</p> <p>Default value: <code>/build</code>.</p> <p><code>shell</code> (String; optional)</p> <p>: The path to the <code>bash</code> binary to use as the shell.   This shell is started when running the image.   This can be seen as an equivalent of the <code>NIX_BUILD_SHELL</code> environment variable for {manpage}<code>nix-shell(1)</code>.</p> <p>Default value: the <code>bash</code> binary from the <code>bashInteractive</code> package.</p> <p><code>command</code> (String or Null; optional)</p> <p>: If specified, this command will be run in the environment of the derivation in an interactive shell.   A call to <code>exit</code> will be added after the command if it is specified, so the shell will exit after it's finished running.   This can be seen as an equivalent of the <code>--command</code> option in {manpage}<code>nix-shell(1)</code>.</p> <p>Default value: <code>null</code>.</p> <p><code>run</code> (String or Null; optional)</p> <p>: Similar to the <code>command</code> attribute, but runs the command in a non-interactive shell instead.   A call to <code>exit</code> will be added after the command if it is specified, so the shell will exit after it's finished running.   This can be seen as an equivalent of the <code>--run</code> option in {manpage}<code>nix-shell(1)</code>.</p> <p>Default value: <code>null</code>.</p>"},{"location":"build-helpers/images/dockertools.section.html#ssec-pkgs-dockerTools-streamNixShellImage-examples","title":"Examples","text":"<p>:::{.example #ex-dockerTools-streamNixShellImage-hello}</p>"},{"location":"build-helpers/images/dockertools.section.html#building-a-docker-image-with-streamnixshellimage-with-the-build-environment-for-the-hello-package","title":"Building a Docker image with <code>streamNixShellImage</code> with the build environment for the <code>hello</code> package","text":"<p>This example shows how to build the <code>hello</code> package inside a Docker container built with <code>streamNixShellImage</code>. The Docker image generated will have a name like <code>hello-&lt;version&gt;-env</code> and tag <code>latest</code>. This example is the <code>streamNixShellImage</code> equivalent of .</p> <pre><code>{ dockerTools, hello }:\ndockerTools.streamNixShellImage {\n  drv = hello;\n  tag = \"latest\";\n}\n</code></pre> <p>The result of building this package is a script. Running this script and piping it into <code>docker load</code> gives you the same image that was built in .</p> <pre><code>$ nix-build\n(some output removed for clarity)\n/nix/store/8vhznpz2frqazxnd8pgdvf38jscdypax-stream-hello-2.12.1-env\n\n$ /nix/store/8vhznpz2frqazxnd8pgdvf38jscdypax-stream-hello-2.12.1-env | docker load\n(some output removed for clarity)\nLoaded image: hello-2.12.1-env:latest\n</code></pre> <p>After starting an interactive container, the derivation can be built by running <code>buildDerivation</code>, and the output can be executed as expected:</p> <p><pre><code>$ docker run -it hello-2.12.1-env:latest\n[nix-shell:~]$ buildDerivation\nRunning phase: unpackPhase\nunpacking source archive /nix/store/pa10z4ngm0g83kx9mssrqzz30s84vq7k-hello-2.12.1.tar.gz\nsource root is hello-2.12.1\n(some output removed for clarity)\nRunning phase: fixupPhase\nshrinking RPATHs of ELF executables and libraries in /nix/store/f2vs29jibd7lwxyj35r9h87h6brgdysz-hello-2.12.1\nshrinking /nix/store/f2vs29jibd7lwxyj35r9h87h6brgdysz-hello-2.12.1/bin/hello\nchecking for references to /build/ in /nix/store/f2vs29jibd7lwxyj35r9h87h6brgdysz-hello-2.12.1...\ngzipping man pages under /nix/store/f2vs29jibd7lwxyj35r9h87h6brgdysz-hello-2.12.1/share/man/\npatching script interpreter paths in /nix/store/f2vs29jibd7lwxyj35r9h87h6brgdysz-hello-2.12.1\nstripping (with command strip and flags -S -p) in  /nix/store/f2vs29jibd7lwxyj35r9h87h6brgdysz-hello-2.12.1/bin\n\n[nix-shell:~]$ $out/bin/hello\nHello, world!\n</code></pre> :::</p> <p>:::{.example #ex-dockerTools-streamNixShellImage-extendingBuildInputs}</p>"},{"location":"build-helpers/images/dockertools.section.html#adding-extra-packages-to-a-docker-image-built-with-streamnixshellimage","title":"Adding extra packages to a Docker image built with <code>streamNixShellImage</code>","text":"<p>This example shows how to add extra packages to an image built with <code>streamNixShellImage</code>. In this case, we'll add the <code>cowsay</code> package. The Docker image generated will have a name like <code>hello-&lt;version&gt;-env</code> and tag <code>latest</code>. This example uses  as a starting point.</p> <pre><code>{ dockerTools, cowsay, hello }:\ndockerTools.streamNixShellImage {\n  tag = \"latest\";\n  drv = hello.overrideAttrs (old: {\n    nativeBuildInputs = old.nativeBuildInputs or [] ++ [\n      cowsay\n    ];\n  });\n}\n</code></pre> <p>The result of building this package is a script which can be run and piped into <code>docker load</code> to load the generated image.</p> <pre><code>$ nix-build\n(some output removed for clarity)\n/nix/store/h5abh0vljgzg381lna922gqknx6yc0v7-stream-hello-2.12.1-env\n\n$ /nix/store/h5abh0vljgzg381lna922gqknx6yc0v7-stream-hello-2.12.1-env | docker load\n(some output removed for clarity)\nLoaded image: hello-2.12.1-env:latest\n</code></pre> <p>After starting an interactive container, we can verify the extra package is available by running <code>cowsay</code>:</p> <p><pre><code>$ docker run -it hello-2.12.1-env:latest\n[nix-shell:~]$ cowsay \"Hello, world!\"\n _______________\n&lt; Hello, world! &gt;\n ---------------\n        \\   ^__^\n         \\  (oo)\\_______\n            (__)\\       )\\/\\\n                ||----w |\n                ||     ||\n</code></pre> :::</p> <p>:::{.example #ex-dockerTools-streamNixShellImage-addingShellHook}</p>"},{"location":"build-helpers/images/dockertools.section.html#adding-a-shellhook-to-a-docker-image-built-with-streamnixshellimage","title":"Adding a <code>shellHook</code> to a Docker image built with <code>streamNixShellImage</code>","text":"<p>This example shows how to add a <code>shellHook</code> command to an image built with <code>streamNixShellImage</code>. In this case, we'll simply output the string <code>Hello, world!</code>. The Docker image generated will have a name like <code>hello-&lt;version&gt;-env</code> and tag <code>latest</code>. This example uses  as a starting point.</p> <pre><code>{ dockerTools, hello }:\ndockerTools.streamNixShellImage {\n  tag = \"latest\";\n  drv = hello.overrideAttrs (old: {\n    shellHook = ''\n      ${old.shellHook or \"\"}\n      echo \"Hello, world!\"\n    '';\n  });\n}\n</code></pre> <p>The result of building this package is a script which can be run and piped into <code>docker load</code> to load the generated image.</p> <pre><code>$ nix-build\n(some output removed for clarity)\n/nix/store/iz4dhdvgzazl5vrgyz719iwjzjy6xlx1-stream-hello-2.12.1-env\n\n$ /nix/store/iz4dhdvgzazl5vrgyz719iwjzjy6xlx1-stream-hello-2.12.1-env | docker load\n(some output removed for clarity)\nLoaded image: hello-2.12.1-env:latest\n</code></pre> <p>After starting an interactive container, we can see the result of the <code>shellHook</code>:</p> <p><pre><code>$ docker run -it hello-2.12.1-env:latest\nHello, world!\n\n[nix-shell:~]$\n</code></pre> :::</p>"},{"location":"build-helpers/images/makediskimage.section.html","title":"<code>&lt;nixpkgs/nixos/lib/make-disk-image.nix&gt;</code>","text":"<p><code>&lt;nixpkgs/nixos/lib/make-disk-image.nix&gt;</code> is a function to create disk images in multiple formats: raw, QCOW2 (QEMU), QCOW2-Compressed (compressed version), VDI (VirtualBox), VPC (VirtualPC).</p> <p>This function can create images in two ways:</p> <ul> <li>using <code>cptofs</code> without any virtual machine to create a Nix store disk image,</li> <li>using a virtual machine to create a full NixOS installation.</li> </ul> <p>When testing early-boot or lifecycle parts of NixOS such as a bootloader or multiple generations, it is necessary to opt for a full NixOS system installation. Whereas for many web servers, applications, it is possible to work with a Nix store only disk image and is faster to build.</p> <p>NixOS tests also use this function when preparing the VM. The <code>cptofs</code> method is used when <code>virtualisation.useBootLoader</code> is false (the default). Otherwise the second method is used.</p>"},{"location":"build-helpers/images/makediskimage.section.html#sec-make-disk-image-features","title":"Features","text":"<p>For reference, read the function signature source code for documentation on arguments: https://github.com/NixOS/nixpkgs/blob/master/nixos/lib/make-disk-image.nix. Features are separated in various sections depending on if you opt for a Nix-store only image or a full NixOS image.</p>"},{"location":"build-helpers/images/makediskimage.section.html#sec-make-disk-image-features-common","title":"Common","text":"<ul> <li>arbitrary NixOS configuration</li> <li>automatic or bound disk size: <code>diskSize</code> parameter, <code>additionalSpace</code> can be set when <code>diskSize</code> is <code>auto</code> to add a constant of disk space</li> <li>multiple partition table layouts: EFI, legacy, legacy + GPT, hybrid, none through <code>partitionTableType</code> parameter</li> <li>OVMF or EFI firmwares and variables templates can be customized</li> <li>root filesystem <code>fsType</code> can be customized to whatever <code>mkfs.${fsType}</code> exist during operations</li> <li>root filesystem label can be customized, defaults to <code>nix-store</code> if it's a Nix store image, otherwise <code>nixpkgs/nixos</code></li> <li>arbitrary code can be executed after disk image was produced with <code>postVM</code></li> <li>the current nixpkgs can be realized as a channel in the disk image, which will change the hash of the image when the sources are updated</li> <li>additional store paths can be provided through <code>additionalPaths</code></li> </ul>"},{"location":"build-helpers/images/makediskimage.section.html#sec-make-disk-image-features-full-image","title":"Full NixOS image","text":"<ul> <li>arbitrary contents with permissions can be placed in the target filesystem using <code>contents</code></li> <li>a <code>/etc/nixpkgs/nixos/configuration.nix</code> can be provided through <code>configFile</code></li> <li>bootloaders are supported</li> <li>EFI variables can be mutated during image production and the result is exposed in <code>$out</code></li> <li>boot partition size when partition table is <code>efi</code> or <code>hybrid</code></li> </ul>"},{"location":"build-helpers/images/makediskimage.section.html#sec-make-disk-image-features-reproducibility","title":"On bit-to-bit reproducibility","text":"<p>Images are NOT deterministic, please do not hesitate to try to fix this, source of determinisms are (not exhaustive) :</p> <ul> <li>bootloader installation have timestamps</li> <li>SQLite Nix store database contain registration times</li> <li><code>/etc/shadow</code> is in a non-deterministic order</li> </ul> <p>A <code>deterministic</code> flag is available for best efforts determinism.</p>"},{"location":"build-helpers/images/makediskimage.section.html#sec-make-disk-image-usage","title":"Usage","text":"<p>To produce a Nix-store only image: <pre><code>let\n  pkgs = import &lt;nixpkgs&gt; {};\n  lib = pkgs.lib;\n  make-disk-image = import &lt;nixpkgs/nixos/lib/make-disk-image.nix&gt;;\nin\n  make-disk-image {\n    inherit pkgs lib;\n    config = {};\n    additionalPaths = [ ];\n    format = \"qcow2\";\n    onlyNixStore = true;\n    partitionTableType = \"none\";\n    installBootLoader = false;\n    touchEFIVars = false;\n    diskSize = \"auto\";\n    additionalSpace = \"0M\"; # Defaults to 512M.\n    copyChannel = false;\n  }\n</code></pre></p> <p>Some arguments can be left out, they are shown explicitly for the sake of the example.</p> <p>Building this derivation will provide a QCOW2 disk image containing only the Nix store and its registration information.</p> <p>To produce a NixOS installation image disk with UEFI and bootloader installed: <pre><code>let\n  pkgs = import &lt;nixpkgs&gt; {};\n  lib = pkgs.lib;\n  make-disk-image = import &lt;nixpkgs/nixos/lib/make-disk-image.nix&gt;;\n  evalConfig = import &lt;nixpkgs/nixos/lib/eval-config.nix&gt;;\nin\n  make-disk-image {\n    inherit pkgs lib;\n    config = evalConfig {\n      modules = [\n        {\n          fileSystems.\"/\" = { device = \"/dev/vda\"; fsType = \"ext4\"; autoFormat = true; };\n          boot.grub.device = \"/dev/vda\";\n        }\n      ];\n    };\n    format = \"qcow2\";\n    onlyNixStore = false;\n    partitionTableType = \"legacy+gpt\";\n    installBootLoader = true;\n    touchEFIVars = true;\n    diskSize = \"auto\";\n    additionalSpace = \"0M\"; # Defaults to 512M.\n    copyChannel = false;\n    memSize = 2048; # Qemu VM memory size in megabytes. Defaults to 1024M.\n  }\n</code></pre></p>"},{"location":"build-helpers/images/ocitools.section.html","title":"pkgs.ociTools","text":"<p><code>pkgs.ociTools</code> is a set of functions for creating containers according to the OCI container specification v1.0.0. Beyond that, it makes no assumptions about the container runner you choose to use to run the created container.</p>"},{"location":"build-helpers/images/ocitools.section.html#ssec-pkgs-ociTools-buildContainer","title":"buildContainer","text":"<p>This function creates a simple OCI container that runs a single command inside of it. An OCI container consists of a <code>config.json</code> and a rootfs directory. The nix store of the container will contain all referenced dependencies of the given command.</p> <p>The parameters of <code>buildContainer</code> with an example value are described below:</p> <pre><code>buildContainer {\n  args = [\n    (with pkgs;\n      writeScript \"run.sh\" ''\n        #!${bash}/bin/bash\n        exec ${bash}/bin/bash\n      '').outPath\n  ];\n\n  mounts = {\n    \"/data\" = {\n      type = \"none\";\n      source = \"/var/lib/mydata\";\n      options = [ \"bind\" ];\n    };\n  };\n\n  readonly = false;\n}\n</code></pre> <ul> <li> <p><code>args</code> specifies a set of arguments to run inside the container. This is the only required argument for <code>buildContainer</code>. All referenced packages inside the derivation will be made available inside the container.</p> </li> <li> <p><code>mounts</code> specifies additional mount points chosen by the user. By default only a minimal set of necessary filesystems are mounted into the container (e.g procfs, cgroupfs)</p> </li> <li> <p><code>readonly</code> makes the container's rootfs read-only if it is set to true. The default value is false <code>false</code>.</p> </li> </ul>"},{"location":"build-helpers/images/portableservice.section.html","title":"pkgs.portableService","text":"<p><code>pkgs.portableService</code> is a function to create portable service images, as read-only, immutable, <code>squashfs</code> archives.</p> <p>systemd supports a concept of Portable Services. Portable Services are a delivery method for system services that uses two specific features of container management:</p> <ul> <li>Applications are bundled. I.e. multiple services, their binaries and   all their dependencies are packaged in an image, and are run directly from it.</li> <li>Stricter default security policies, i.e. sandboxing of applications.</li> </ul> <p>This allows using Nix to build images which can be run on many recent Linux distributions.</p> <p>The primary tool for interacting with Portable Services is <code>portablectl</code>, and they are managed by the <code>systemd-portabled</code> system service.</p> <p>::: {.note} Portable services are supported starting with systemd 239 (released on 2018-06-22). :::</p> <p>A very simple example of using <code>portableService</code> is described below:</p> <p>[]{#ex-pkgs-portableService}</p> <pre><code>pkgs.portableService {\n  pname = \"demo\";\n  version = \"1.0\";\n  units = [ demo-service demo-socket ];\n}\n</code></pre> <p>The above example will build an squashfs archive image in <code>result/$pname_$version.raw</code>. The image will contain the file system structure as required by the portable service specification, and a subset of the Nix store with all the dependencies of the two derivations in the <code>units</code> list. <code>units</code> must be a list of derivations, and their names must be prefixed with the service name (<code>\"demo\"</code> in this case). Otherwise <code>systemd-portabled</code> will ignore them.</p> <p>::: {.note} The <code>.raw</code> file extension of the image is required by the portable services specification. :::</p> <p>Some other options available are: - <code>description</code>, <code>homepage</code></p> <p>Are added to the <code>/etc/os-release</code> in the image and are shown by the portable services tooling.   Default to empty values, not added to os-release. - <code>symlinks</code></p> <p>A list of attribute sets {object, symlink}. Symlinks will be created  in the root filesystem of the image to   objects in the Nix store. Defaults to an empty list. - <code>contents</code></p> <p>A list of additional derivations to be included in the image Nix store, as-is. Defaults to an empty list. - <code>squashfsTools</code></p> <p>Defaults to <code>pkgs.squashfsTools</code>, allows you to override the package that provides <code>mksquashfs</code>. - <code>squash-compression</code>, <code>squash-block-size</code></p> <p>Options to <code>mksquashfs</code>. Default to <code>\"xz -Xdict-size 100%\"</code> and <code>\"1M\"</code> respectively.</p> <p>A typical usage of <code>symlinks</code> would be: <pre><code>  symlinks = [\n    { object = \"${pkgs.cacert}/etc/ssl\"; symlink = \"/etc/ssl\"; }\n    { object = \"${pkgs.bash}/bin/bash\"; symlink = \"/bin/sh\"; }\n    { object = \"${pkgs.php}/bin/php\"; symlink = \"/usr/bin/php\"; }\n  ];\n</code></pre> to create these symlinks for legacy applications that assume them existing globally.</p> <p>Once the image is created, and deployed on a host in <code>/var/lib/portables/</code>, you can attach the image and run the service. As root run: <pre><code>portablectl attach demo_1.0.raw\nsystemctl enable --now demo.socket\nsystemctl enable --now demo.service\n</code></pre> ::: {.note} See the man page of <code>portablectl</code> for more info on its usage. :::</p>"},{"location":"build-helpers/images/snaptools.section.html","title":"pkgs.snapTools","text":"<p><code>pkgs.snapTools</code> is a set of functions for creating Snapcraft images. Snap and Snapcraft is not used to perform these operations.</p>"},{"location":"build-helpers/images/snaptools.section.html#ssec-pkgs-snapTools-makeSnap-signature","title":"The makeSnap Function","text":"<p><code>makeSnap</code> takes a single named argument, <code>meta</code>. This argument mirrors the upstream <code>snap.yaml</code> format exactly.</p> <p>The <code>base</code> should not be specified, as <code>makeSnap</code> will force set it.</p> <p>Currently, <code>makeSnap</code> does not support creating GUI stubs.</p>"},{"location":"build-helpers/images/snaptools.section.html#ssec-pkgs-snapTools-build-a-snap-hello","title":"Build a Hello World Snap","text":"<p>The following expression packages GNU Hello as a Snapcraft snap.</p> <pre><code>let\n  inherit (import &lt;nixpkgs&gt; { }) snapTools hello;\nin snapTools.makeSnap {\n  meta = {\n    name = \"hello\";\n    summary = hello.meta.description;\n    description = hello.meta.longDescription;\n    architectures = [ \"amd64\" ];\n    confinement = \"strict\";\n    apps.hello.command = \"${hello}/bin/hello\";\n  };\n}\n</code></pre> <p><code>nix-build</code> this expression and install it with <code>snap install ./result --dangerous</code>. <code>hello</code> will now be the Snapcraft version of the package.</p>"},{"location":"build-helpers/images/snaptools.section.html#ssec-pkgs-snapTools-build-a-snap-firefox","title":"Build a Graphical Snap","text":"<p>Graphical programs require many more integrations with the host. This example uses Firefox as an example because it is one of the most complicated programs we could package.</p> <pre><code>let\n  inherit (import &lt;nixpkgs&gt; { }) snapTools firefox;\nin snapTools.makeSnap {\n  meta = {\n    name = \"nix-example-firefox\";\n    summary = firefox.meta.description;\n    architectures = [ \"amd64\" ];\n    apps.nix-example-firefox = {\n      command = \"${firefox}/bin/firefox\";\n      plugs = [\n        \"pulseaudio\"\n        \"camera\"\n        \"browser-support\"\n        \"avahi-observe\"\n        \"cups-control\"\n        \"desktop\"\n        \"desktop-legacy\"\n        \"gsettings\"\n        \"home\"\n        \"network\"\n        \"mount-observe\"\n        \"removable-media\"\n        \"x11\"\n      ];\n    };\n    confinement = \"strict\";\n  };\n}\n</code></pre> <p><code>nix-build</code> this expression and install it with <code>snap install ./result --dangerous</code>. <code>nix-example-firefox</code> will now be the Snapcraft version of the Firefox package.</p> <p>The specific meaning behind plugs can be looked up in the Snapcraft interface documentation.</p>"},{"location":"build-helpers/special/checkpoint-build.section.html","title":"pkgs.checkpointBuildTools","text":"<p><code>pkgs.checkpointBuildTools</code> provides a way to build derivations incrementally. It consists of two functions to make checkpoint builds using Nix possible.</p> <p>For hermeticity, Nix derivations do not allow any state to be carried over between builds, making a transparent incremental build within a derivation impossible.</p> <p>However, we can tell Nix explicitly what the previous build state was, by representing that previous state as a derivation output. This allows the passed build state to be used for an incremental build.</p> <p>To change a normal derivation to a checkpoint based build, these steps must be taken:   - apply <code>prepareCheckpointBuild</code> on the desired derivation, e.g. <pre><code>checkpointArtifacts = (pkgs.checkpointBuildTools.prepareCheckpointBuild pkgs.virtualbox);\n</code></pre>   - change something you want in the sources of the package, e.g. use a source override: <pre><code>changedVBox = pkgs.virtualbox.overrideAttrs (old: {\n  src = path/to/vbox/sources;\n});\n</code></pre>   - use <code>mkCheckpointBuild changedVBox checkpointArtifacts</code>   - enjoy shorter build times</p>"},{"location":"build-helpers/special/checkpoint-build.section.html#sec-checkpoint-build-example","title":"Example","text":"<pre><code>{ pkgs ? import &lt;nixpkgs&gt; {} }:\nlet\n  inherit (pkgs.checkpointBuildTools)\n    prepareCheckpointBuild\n    mkCheckpointBuild\n    ;\n  helloCheckpoint = prepareCheckpointBuild pkgs.hello;\n  changedHello = pkgs.hello.overrideAttrs (_: {\n    doCheck = false;\n    patchPhase = ''\n      sed -i 's/Hello, world!/Hello, Nix!/g' src/hello.c\n    '';\n  });\nin mkCheckpointBuild changedHello helloCheckpoint\n</code></pre>"},{"location":"build-helpers/special/fakenss.section.html","title":"fakeNss","text":"<p>Provides <code>/etc/passwd</code> and <code>/etc/group</code> files that contain <code>root</code> and <code>nobody</code>, allowing user/group lookups to work in binaries that insist on doing those. This might be a better choice than a custom script running <code>useradd</code> and related utilities if you only need those files to exist with some entries.</p> <p><code>fakeNss</code> also provides <code>/etc/nsswitch.conf</code>, configuring NSS host resolution to first check <code>/etc/hosts</code> before checking DNS, since the default in the absence of a config file (<code>dns [!UNAVAIL=return] files</code>) is quite unexpected.</p> <p>It also creates an empty directory at <code>/var/empty</code> because it uses that as the home directory for the <code>root</code> and <code>nobody</code> users. The <code>/var/empty</code> directory can also be used as a <code>chroot</code> target to prevent file access in processes that do not need to access files, if your container runs such processes.</p> <p>The user entries created by <code>fakeNss</code> use the <code>/bin/sh</code> shell, which is not provided by <code>fakeNss</code> because in most cases it won't be used. If you need that to be available, see <code>dockerTools.binSh</code> or provide your own.</p>"},{"location":"build-helpers/special/fakenss.section.html#sec-fakeNss-inputs","title":"Inputs","text":"<p><code>fakeNss</code> is made available in Nixpkgs as a package rather than a function, but it has two attributes that can be overridden and might be useful in particular cases. For more details on how overriding works, see  and .</p> <p><code>extraPasswdLines</code> (List of Strings; optional)</p> <p>: A list of lines that will be added to <code>/etc/passwd</code>.   Useful if extra users need to exist in the output of <code>fakeNss</code>.   If <code>extraPasswdLines</code> is specified, it will not override the <code>root</code> and <code>nobody</code> entries created by <code>fakeNss</code>.   Those entries will always exist.</p> <p>Lines specified here must follow the format in {manpage}<code>passwd(5)</code>.</p> <p>Default value: <code>[]</code>.</p> <p><code>extraGroupLines</code> (List of Strings; optional)</p> <p>: A list of lines that will be added to <code>/etc/group</code>.   Useful if extra groups need to exist in the output of <code>fakeNss</code>.   If <code>extraGroupLines</code> is specified, it will not override the <code>root</code> and <code>nobody</code> entries created by <code>fakeNss</code>.   Those entries will always exist.</p> <p>Lines specified here must follow the format in {manpage}<code>group(5)</code>.</p> <p>Default value: <code>[]</code>.</p>"},{"location":"build-helpers/special/fakenss.section.html#sec-fakeNss-examples","title":"Examples","text":"<p>:::{.example #ex-fakeNss-dockerTools-buildImage}</p>"},{"location":"build-helpers/special/fakenss.section.html#using-fakenss-with-dockertoolsbuildimage","title":"Using <code>fakeNss</code> with <code>dockerTools.buildImage</code>","text":"<p>This example shows how to use <code>fakeNss</code> as-is. It is useful with functions in <code>dockerTools</code> to allow building Docker images that have the <code>/etc/passwd</code> and <code>/etc/group</code> files. This example includes the <code>hello</code> binary in the image so it can do something besides just have the extra files.</p> <p><pre><code>{ dockerTools, fakeNss, hello }:\ndockerTools.buildImage {\n  name = \"image-with-passwd\";\n  tag = \"latest\";\n\n  copyToRoot = [ fakeNss hello ];\n\n  config = {\n    Cmd = [ \"/bin/hello\" ];\n  };\n}\n</code></pre> :::</p> <p>:::{.example #ex-fakeNss-overriding}</p>"},{"location":"build-helpers/special/fakenss.section.html#using-fakenss-with-an-override-to-add-extra-lines","title":"Using <code>fakeNss</code> with an override to add extra lines","text":"<p>The following code uses <code>override</code> to add extra lines to <code>/etc/passwd</code> and <code>/etc/group</code> to create another user and group entry.</p> <p><pre><code>{ fakeNss }:\nfakeNss.override {\n  extraPasswdLines = [\"newuser:x:9001:9001:new user:/var/empty:/bin/sh\"];\n  extraGroupLines = [\"newuser:x:9001:\"];\n}\n</code></pre> :::</p>"},{"location":"build-helpers/special/fhs-environments.section.html","title":"buildFHSEnv","text":"<p><code>buildFHSEnv</code> provides a way to build and run FHS-compatible lightweight sandboxes. It creates an isolated root filesystem with the host's <code>/nix/store</code>, so its footprint in terms of disk space is quite small. This allows you to run software which is hard or unfeasible to patch for NixOS; 3rd-party source trees with FHS assumptions, games distributed as tarballs, software with integrity checking and/or external self-updated binaries for instance. It uses Linux' namespaces feature to create temporary lightweight environments which are destroyed after all child processes exit, without requiring elevated privileges. It works similar to containerisation technology such as Docker or FlatPak but provides no security-relevant separation from the host system.</p> <p>Accepted arguments are:</p> <ul> <li><code>name</code>         The name of the environment and the wrapper executable.</li> <li><code>targetPkgs</code>         Packages to be installed for the main host's architecture (i.e. x86_64 on x86_64 installations). Along with libraries binaries are also installed.</li> <li><code>multiPkgs</code>         Packages to be installed for all architectures supported by a host (i.e. i686 and x86_64 on x86_64 installations). Only libraries are installed by default.</li> <li><code>multiArch</code>         Whether to install 32bit multiPkgs into the FHSEnv in 64bit environments</li> <li><code>extraBuildCommands</code>         Additional commands to be executed for finalizing the directory structure.</li> <li><code>extraBuildCommandsMulti</code>         Like <code>extraBuildCommands</code>, but executed only on multilib architectures.</li> <li><code>extraOutputsToInstall</code>         Additional derivation outputs to be linked for both target and multi-architecture packages.</li> <li><code>extraInstallCommands</code>         Additional commands to be executed for finalizing the derivation with runner script.</li> <li><code>runScript</code>         A shell command to be executed inside the sandbox. It defaults to <code>bash</code>. Command line arguments passed to the resulting wrapper are appended to this command by default.         This command must be escaped; i.e. <code>\"foo app\" --do-stuff --with \"some file\"</code>. See <code>lib.escapeShellArgs</code>.</li> <li><code>profile</code>         Optional script for <code>/etc/profile</code> within the sandbox.</li> </ul> <p>You can create a simple environment using a <code>shell.nix</code> like this:</p> <pre><code>{ pkgs ? import &lt;nixpkgs&gt; {} }:\n\n(pkgs.buildFHSEnv {\n  name = \"simple-x11-env\";\n  targetPkgs = pkgs: (with pkgs; [\n    udev\n    alsa-lib\n  ]) ++ (with pkgs.xorg; [\n    libX11\n    libXcursor\n    libXrandr\n  ]);\n  multiPkgs = pkgs: (with pkgs; [\n    udev\n    alsa-lib\n  ]);\n  runScript = \"bash\";\n}).env\n</code></pre> <p>Running <code>nix-shell</code> on it would drop you into a shell inside an FHS env where those libraries and binaries are available in FHS-compliant paths. Applications that expect an FHS structure (i.e. proprietary binaries) can run inside this environment without modification. You can build a wrapper by running your binary in <code>runScript</code>, e.g. <code>./bin/start.sh</code>. Relative paths work as expected.</p> <p>Additionally, the FHS builder links all relocated gsettings-schemas (the glib setup-hook moves them to <code>share/gsettings-schemas/${name}/glib-2.0/schemas</code>) to their standard FHS location. This means you don't need to wrap binaries with <code>wrapGAppsHook</code>.</p>"},{"location":"build-helpers/special/makesetuphook.section.html","title":"pkgs.makeSetupHook","text":"<p><code>pkgs.makeSetupHook</code> is a build helper that produces hooks that go in to <code>nativeBuildInputs</code></p>"},{"location":"build-helpers/special/makesetuphook.section.html#sec-pkgs.makeSetupHook-usage","title":"Usage","text":"<pre><code>pkgs.makeSetupHook {\n  name = \"something-hook\";\n  propagatedBuildInputs = [ pkgs.commandsomething ];\n  depsTargetTargetPropagated = [ pkgs.libsomething ];\n} ./script.sh\n</code></pre>"},{"location":"build-helpers/special/makesetuphook.section.html#sec-pkgs.makeSetupHook-usage-example","title":"setup hook that depends on the hello package and runs hello and @shell@ is substituted with path to bash","text":"<pre><code>pkgs.makeSetupHook {\n    name = \"run-hello-hook\";\n    propagatedBuildInputs = [ pkgs.hello ];\n    substitutions = { shell = \"${pkgs.bash}/bin/bash\"; };\n    passthru.tests.greeting = callPackage ./test { };\n    meta.platforms = lib.platforms.linux;\n} (writeScript \"run-hello-hook.sh\" ''\n    #!@shell@\n    hello\n'')\n</code></pre>"},{"location":"build-helpers/special/makesetuphook.section.html#sec-pkgs.makeSetupHook-attributes","title":"Attributes","text":"<ul> <li><code>name</code> Set the name of the hook.</li> <li><code>propagatedBuildInputs</code> Runtime dependencies (such as binaries) of the hook.</li> <li><code>depsTargetTargetPropagated</code> Non-binary dependencies.</li> <li><code>meta</code></li> <li><code>passthru</code></li> <li><code>substitutions</code> Variables for <code>substituteAll</code></li> </ul>"},{"location":"build-helpers/special/mkshell.section.html","title":"pkgs.mkShell","text":"<p><code>pkgs.mkShell</code> is a specialized <code>stdenv.mkDerivation</code> that removes some repetition when using it with <code>nix-shell</code> (or <code>nix develop</code>).</p>"},{"location":"build-helpers/special/mkshell.section.html#sec-pkgs-mkShell-usage","title":"Usage","text":"<p>Here is a common usage example:</p> <pre><code>{ pkgs ? import &lt;nixpkgs&gt; {} }:\npkgs.mkShell {\n  packages = [ pkgs.gnumake ];\n\n  inputsFrom = [ pkgs.hello pkgs.gnutar ];\n\n  shellHook = ''\n    export DEBUG=1\n  '';\n}\n</code></pre>"},{"location":"build-helpers/special/mkshell.section.html#sec-pkgs-mkShell-attributes","title":"Attributes","text":"<ul> <li><code>name</code> (default: <code>nix-shell</code>). Set the name of the derivation.</li> <li><code>packages</code> (default: <code>[]</code>). Add executable packages to the <code>nix-shell</code> environment.</li> <li><code>inputsFrom</code> (default: <code>[]</code>). Add build dependencies of the listed derivations to the <code>nix-shell</code> environment.</li> <li><code>shellHook</code> (default: <code>\"\"</code>). Bash statements that are executed by <code>nix-shell</code>.</li> </ul> <p>... all the attributes of <code>stdenv.mkDerivation</code>.</p>"},{"location":"build-helpers/special/mkshell.section.html#sec-pkgs-mkShell-variants","title":"Variants","text":"<p><code>pkgs.mkShellNoCC</code> is a variant that uses <code>stdenvNoCC</code> instead of <code>stdenv</code> as base environment. This is useful if no C compiler is needed in the shell environment.</p>"},{"location":"build-helpers/special/mkshell.section.html#sec-pkgs-mkShell-building","title":"Building the shell","text":"<p>This derivation output will contain a text file that contains a reference to all the build inputs. This is useful in CI where we want to make sure that every derivation, and its dependencies, build properly. Or when creating a GC root so that the build dependencies don't get garbage-collected.</p>"},{"location":"build-helpers/special/vm-tools.section.html","title":"vmTools","text":"<p>A set of VM related utilities, that help in building some packages in more advanced scenarios.</p>"},{"location":"build-helpers/special/vm-tools.section.html#vm-tools-createEmptyImage","title":"<code>vmTools.createEmptyImage</code>","text":"<p>A bash script fragment that produces a disk image at <code>destination</code>.</p>"},{"location":"build-helpers/special/vm-tools.section.html#vm-tools-createEmptyImage-attributes","title":"Attributes","text":"<ul> <li><code>size</code>. The disk size, in MiB.</li> <li><code>fullName</code>. Name that will be written to <code>${destination}/nix-support/full-name</code>.</li> <li><code>destination</code> (optional, default <code>$out</code>). Where to write the image files.</li> </ul>"},{"location":"build-helpers/special/vm-tools.section.html#vm-tools-runInLinuxVM","title":"<code>vmTools.runInLinuxVM</code>","text":"<p>Run a derivation in a Linux virtual machine (using Qemu/KVM). By default, there is no disk image; the root filesystem is a <code>tmpfs</code>, and the Nix store is shared with the host (via the 9P protocol). Thus, any pure Nix derivation should run unmodified.</p> <p>If the build fails and Nix is run with the <code>-K/--keep-failed</code> option, a script <code>run-vm</code> will be left behind in the temporary build directory that allows you to boot into the VM and debug it interactively.</p>"},{"location":"build-helpers/special/vm-tools.section.html#vm-tools-runInLinuxVM-attributes","title":"Attributes","text":"<ul> <li><code>preVM</code> (optional). Shell command to be evaluated before the VM is started (i.e., on the host).</li> <li><code>memSize</code> (optional, default <code>512</code>). The memory size of the VM in MiB.</li> <li><code>diskImage</code> (optional). A file system image to be attached to <code>/dev/sda</code>.   Note that currently we expect the image to contain a filesystem, not a full disk image with a partition table etc.</li> </ul>"},{"location":"build-helpers/special/vm-tools.section.html#vm-tools-runInLinuxVM-examples","title":"Examples","text":"<p>Build the derivation hello inside a VM: <pre><code>{ pkgs }: with pkgs; with vmTools;\nrunInLinuxVM hello\n</code></pre></p> <p>Build inside a VM with extra memory: <pre><code>{ pkgs }: with pkgs; with vmTools;\nrunInLinuxVM (hello.overrideAttrs (_: { memSize = 1024; }))\n</code></pre></p> <p>Use VM with a disk image (implicitly sets <code>diskImage</code>, see <code>vmTools.createEmptyImage</code>): <pre><code>{ pkgs }: with pkgs; with vmTools;\nrunInLinuxVM (hello.overrideAttrs (_: {\n  preVM = createEmptyImage {\n    size = 1024;\n    fullName = \"vm-image\";\n  };\n}))\n</code></pre></p>"},{"location":"build-helpers/special/vm-tools.section.html#vm-tools-extractFs","title":"<code>vmTools.extractFs</code>","text":"<p>Takes a file, such as an ISO, and extracts its contents into the store.</p>"},{"location":"build-helpers/special/vm-tools.section.html#vm-tools-extractFs-attributes","title":"Attributes","text":"<ul> <li><code>file</code>. Path to the file to be extracted.   Note that currently we expect the image to contain a filesystem, not a full disk image with a partition table etc.</li> <li><code>fs</code> (optional). Filesystem of the contents of the file.</li> </ul>"},{"location":"build-helpers/special/vm-tools.section.html#vm-tools-extractFs-examples","title":"Examples","text":"<p>Extract the contents of an ISO file: <pre><code>{ pkgs }: with pkgs; with vmTools;\nextractFs { file = ./image.iso; }\n</code></pre></p>"},{"location":"build-helpers/special/vm-tools.section.html#vm-tools-extractMTDfs","title":"<code>vmTools.extractMTDfs</code>","text":"<p>Like , but it makes use of a Memory Technology Device (MTD).</p>"},{"location":"build-helpers/special/vm-tools.section.html#vm-tools-runInLinuxImage","title":"<code>vmTools.runInLinuxImage</code>","text":"<p>Like , but instead of using <code>stdenv</code> from the Nix store, run the build using the tools provided by <code>/bin</code>, <code>/usr/bin</code>, etc. from the specified filesystem image, which typically is a filesystem containing a FHS-based Linux distribution.</p>"},{"location":"build-helpers/special/vm-tools.section.html#vm-tools-makeImageTestScript","title":"<code>vmTools.makeImageTestScript</code>","text":"<p>Generate a script that can be used to run an interactive session in the given image.</p>"},{"location":"build-helpers/special/vm-tools.section.html#vm-tools-makeImageTestScript-examples","title":"Examples","text":"<p>Create a script for running a Fedora 27 VM: <pre><code>{ pkgs }: with pkgs; with vmTools;\nmakeImageTestScript diskImages.fedora27x86_64\n</code></pre></p> <p>Create a script for running an Ubuntu 20.04 VM: <pre><code>{ pkgs }: with pkgs; with vmTools;\nmakeImageTestScript diskImages.ubuntu2004x86_64\n</code></pre></p>"},{"location":"build-helpers/special/vm-tools.section.html#vm-tools-diskImageFuns","title":"<code>vmTools.diskImageFuns</code>","text":"<p>A set of functions that build a predefined set of minimal Linux distributions images.</p>"},{"location":"build-helpers/special/vm-tools.section.html#vm-tools-diskImageFuns-images","title":"Images","text":"<ul> <li>Fedora</li> <li><code>fedora26x86_64</code></li> <li><code>fedora27x86_64</code></li> <li>CentOS</li> <li><code>centos6i386</code></li> <li><code>centos6x86_64</code></li> <li><code>centos7x86_64</code></li> <li>Ubuntu</li> <li><code>ubuntu1404i386</code></li> <li><code>ubuntu1404x86_64</code></li> <li><code>ubuntu1604i386</code></li> <li><code>ubuntu1604x86_64</code></li> <li><code>ubuntu1804i386</code></li> <li><code>ubuntu1804x86_64</code></li> <li><code>ubuntu2004i386</code></li> <li><code>ubuntu2004x86_64</code></li> <li><code>ubuntu2204i386</code></li> <li><code>ubuntu2204x86_64</code></li> <li>Debian</li> <li><code>debian10i386</code></li> <li><code>debian10x86_64</code></li> <li><code>debian11i386</code></li> <li><code>debian11x86_64</code></li> </ul>"},{"location":"build-helpers/special/vm-tools.section.html#vm-tools-diskImageFuns-attributes","title":"Attributes","text":"<ul> <li><code>size</code> (optional, defaults to <code>4096</code>). The size of the image, in MiB.</li> <li><code>extraPackages</code> (optional). A list names of additional packages from the distribution that should be included in the image.</li> </ul>"},{"location":"build-helpers/special/vm-tools.section.html#vm-tools-diskImageFuns-examples","title":"Examples","text":"<p>8GiB image containing Firefox in addition to the default packages: <pre><code>{ pkgs }: with pkgs; with vmTools;\ndiskImageFuns.ubuntu2004x86_64 { extraPackages = [ \"firefox\" ]; size = 8192; }\n</code></pre></p>"},{"location":"build-helpers/special/vm-tools.section.html#vm-tools-diskImageExtraFuns","title":"<code>vmTools.diskImageExtraFuns</code>","text":"<p>Shorthand for <code>vmTools.diskImageFuns.&lt;attr&gt; { extraPackages = ... }</code>.</p>"},{"location":"build-helpers/special/vm-tools.section.html#vm-tools-diskImages","title":"<code>vmTools.diskImages</code>","text":"<p>Shorthand for <code>vmTools.diskImageFuns.&lt;attr&gt; { }</code>.</p>"},{"location":"contributing/coding-conventions.chapter.html","title":"Coding conventions","text":"<p>This section has been moved to CONTRIBUTING.md.</p>"},{"location":"contributing/coding-conventions.chapter.html#sec-syntax","title":"Syntax","text":"<p>This section has been moved to CONTRIBUTING.md.</p>"},{"location":"contributing/coding-conventions.chapter.html#sec-package-naming","title":"Package naming","text":"<p>This section has been moved to pkgs/README.md.</p>"},{"location":"contributing/coding-conventions.chapter.html#sec-organisation","title":"File naming and organisation","text":"<p>This section has been moved to CONTRIBUTING.md.</p>"},{"location":"contributing/coding-conventions.chapter.html#sec-versioning","title":"Versioning","text":"<p>This section has been moved to pkgs/README.md.</p>"},{"location":"contributing/coding-conventions.chapter.html#sec-sources","title":"Fetching Sources","text":"<p>This section has been moved to pkgs/README.md.</p>"},{"location":"contributing/coding-conventions.chapter.html#sec-source-hashes","title":"Obtaining source hash","text":"<p>This section has been moved to pkgs/README.md.</p>"},{"location":"contributing/coding-conventions.chapter.html#sec-source-hashes-security","title":"Obtaining hashes securely","text":"<p>This section has been moved to pkgs/README.md.</p>"},{"location":"contributing/coding-conventions.chapter.html#sec-patches","title":"Patches","text":"<p>This section has been moved to pkgs/README.md.</p>"},{"location":"contributing/coding-conventions.chapter.html#sec-package-tests","title":"Package tests","text":"<p>This section has been moved to pkgs/README.md.</p>"},{"location":"contributing/coding-conventions.chapter.html#ssec-inline-package-tests-writing","title":"Writing inline package tests","text":"<p>This section has been moved to pkgs/README.md.</p>"},{"location":"contributing/coding-conventions.chapter.html#ssec-package-tests-writing","title":"Writing larger package tests","text":"<p>This section has been moved to pkgs/README.md.</p>"},{"location":"contributing/coding-conventions.chapter.html#ssec-package-tests-running","title":"Running package tests","text":"<p>This section has been moved to pkgs/README.md.</p>"},{"location":"contributing/coding-conventions.chapter.html#ssec-package-tests-examples","title":"Examples of package tests","text":"<p>This section has been moved to pkgs/README.md.</p>"},{"location":"contributing/coding-conventions.chapter.html#ssec-nixos-tests-linking","title":"Linking NixOS module tests to a package","text":"<p>This section has been moved to pkgs/README.md.</p>"},{"location":"contributing/coding-conventions.chapter.html#ssec-import-from-derivation","title":"Import From Derivation","text":"<p>This section has been moved to pkgs/README.md.</p>"},{"location":"contributing/contributing-to-documentation.chapter.html","title":"Contributing to Nixpkgs documentation","text":"<p>This section has been moved to doc/README.md.</p>"},{"location":"contributing/contributing-to-documentation.chapter.html#sec-contributing-devmode","title":"devmode","text":"<p>This section has been moved to doc/README.md.</p>"},{"location":"contributing/contributing-to-documentation.chapter.html#sec-contributing-markup","title":"Syntax","text":"<p>This section has been moved to doc/README.md.</p>"},{"location":"contributing/quick-start.chapter.html","title":"Quick Start to Adding a Package","text":"<p>This section has been moved to pkgs/README.md.</p>"},{"location":"contributing/reviewing-contributions.chapter.html","title":"Reviewing contributions","text":"<p>This section has been moved to CONTRIBUTING.md.</p>"},{"location":"contributing/reviewing-contributions.chapter.html#reviewing-contributions-package-updates","title":"Package updates","text":"<p>This section has been moved to pkgs/README.md.</p>"},{"location":"contributing/reviewing-contributions.chapter.html#reviewing-contributions-new-packages","title":"New packages","text":"<p>This section has been moved to pkgs/README.md.</p>"},{"location":"contributing/reviewing-contributions.chapter.html#reviewing-contributions-module-updates","title":"Module updates","text":"<p>This section has been moved to nixos/README.md.</p>"},{"location":"contributing/reviewing-contributions.chapter.html#reviewing-contributions-new-modules","title":"New modules","text":"<p>This section has been moved to nixos/README.md.</p>"},{"location":"contributing/reviewing-contributions.chapter.html#reviewing-contributions-individual-maintainer-list","title":"Individual maintainer list","text":"<p>This section has been moved to maintainers/README.md.</p>"},{"location":"contributing/reviewing-contributions.chapter.html#reviewing-contributions-maintainer-teams","title":"Maintainer teams","text":"<p>This section has been moved to maintainers/README.md.</p>"},{"location":"contributing/reviewing-contributions.chapter.html#reviewing-contributions-other-submissions","title":"Other submissions","text":"<p>This section has been moved to CONTRIBUTING.md.</p>"},{"location":"contributing/reviewing-contributions.chapter.html#reviewing-contributions--merging-pull-requests","title":"Merging pull requests","text":"<p>This section has been moved to CONTRIBUTING.md.</p>"},{"location":"contributing/submitting-changes.chapter.html","title":"Submitting changes","text":"<p>This section has been moved to CONTRIBUTING.md.</p>"},{"location":"contributing/submitting-changes.chapter.html#submitting-changes-submitting-changes","title":"Submitting changes","text":"<p>This section has been moved to CONTRIBUTING.md.</p>"},{"location":"contributing/submitting-changes.chapter.html#submitting-changes-submitting-security-fixes","title":"Submitting security fixes","text":"<p>This section has been moved to pkgs/README.md.</p>"},{"location":"contributing/submitting-changes.chapter.html#submitting-changes-deprecating-packages","title":"Deprecating/removing packages","text":"<p>This section has been moved to pkgs/README.md.</p>"},{"location":"contributing/submitting-changes.chapter.html#steps-to-remove-a-package-from-nixpkgs","title":"Steps to remove a package from Nixpkgs","text":"<p>This section has been moved to pkgs/README.md.</p>"},{"location":"contributing/submitting-changes.chapter.html#submitting-changes-pull-request-template","title":"Pull Request Template","text":"<p>This section has been moved to CONTRIBUTING.md.</p>"},{"location":"contributing/submitting-changes.chapter.html#submitting-changes-tested-with-sandbox","title":"Tested using sandboxing","text":"<p>This section has been moved to CONTRIBUTING.md.</p>"},{"location":"contributing/submitting-changes.chapter.html#submitting-changes-platform-diversity","title":"Built on platform(s)","text":"<p>This section has been moved to CONTRIBUTING.md.</p>"},{"location":"contributing/submitting-changes.chapter.html#submitting-changes-nixos-tests","title":"Tested via one or more NixOS test(s) if existing and applicable for the change (look inside nixos/tests)","text":"<p>This section has been moved to CONTRIBUTING.md.</p>"},{"location":"contributing/submitting-changes.chapter.html#submitting-changes-tested-compilation","title":"Tested compilation of all pkgs that depend on this change using <code>nixpkgs-review</code>","text":"<p>This section has been moved to CONTRIBUTING.md.</p>"},{"location":"contributing/submitting-changes.chapter.html#submitting-changes-tested-execution","title":"Tested execution of all binary files (usually in <code>./result/bin/</code>)","text":"<p>This section has been moved to CONTRIBUTING.md.</p>"},{"location":"contributing/submitting-changes.chapter.html#submitting-changes-contribution-standards","title":"Meets Nixpkgs contribution standards","text":"<p>This section has been moved to CONTRIBUTING.md.</p>"},{"location":"contributing/submitting-changes.chapter.html#submitting-changes-hotfixing-pull-requests","title":"Hotfixing pull requests","text":"<p>This section has been moved to CONTRIBUTING.md.</p>"},{"location":"contributing/submitting-changes.chapter.html#submitting-changes-commit-policy","title":"Commit policy","text":"<p>This section has been moved to CONTRIBUTING.md.</p>"},{"location":"contributing/submitting-changes.chapter.html#submitting-changes-branches","title":"Branches","text":"<p>This section has been moved to CONTRIBUTING.md.</p>"},{"location":"contributing/submitting-changes.chapter.html#submitting-changes-master-branch","title":"Master branch","text":"<p>This section has been moved to CONTRIBUTING.md.</p>"},{"location":"contributing/submitting-changes.chapter.html#submitting-changes-staging-branch","title":"Staging branch","text":"<p>This section has been moved to CONTRIBUTING.md.</p>"},{"location":"contributing/submitting-changes.chapter.html#submitting-changes-staging-next-branch","title":"Staging-next branch","text":"<p>This section has been moved to CONTRIBUTING.md.</p>"},{"location":"contributing/submitting-changes.chapter.html#submitting-changes-stable-release-branches","title":"Stable release branches","text":"<p>This section has been moved to CONTRIBUTING.md.</p>"},{"location":"contributing/submitting-changes.chapter.html#submitting-changes-stable-release-branches-automatic-backports","title":"Automatically backporting a Pull Request","text":"<p>This section has been moved to CONTRIBUTING.md.</p>"},{"location":"contributing/submitting-changes.chapter.html#submitting-changes-stable-release-branches-manual-backports","title":"Manually backporting changes","text":"<p>This section has been moved to CONTRIBUTING.md.</p>"},{"location":"contributing/submitting-changes.chapter.html#acceptable-backport-criteria","title":"Acceptable backport criteria","text":"<p>This section has been moved to CONTRIBUTING.md.</p>"},{"location":"contributing/vulnerability-roundup.chapter.html","title":"Vulnerability Roundup","text":"<p>This section has been moved to pkgs/README.md.</p>"},{"location":"contributing/vulnerability-roundup.chapter.html#vulnerability-roundup-issues","title":"Issues","text":"<p>This section has been moved to pkgs/README.md.</p>"},{"location":"contributing/vulnerability-roundup.chapter.html#vulnerability-roundup-triaging-and-fixing","title":"Triaging and Fixing","text":"<p>This section has been moved to pkgs/README.md.</p>"},{"location":"development/opening-issues.chapter.html","title":"Opening issues","text":"<ul> <li>Make sure you have a GitHub account</li> <li>Make sure there is no open issue on the topic</li> <li>Submit a new issue by choosing the kind of topic and fill out the template</li> </ul>"},{"location":"functions/debug.section.html","title":"Debugging Nix Expressions","text":"<p>Nix is a unityped, dynamic language, this means every value can potentially appear anywhere. Since it is also non-strict, evaluation order and what ultimately is evaluated might surprise you. Therefore it is important to be able to debug nix expressions.</p> <p>In the <code>lib/debug.nix</code> file you will find a number of functions that help (pretty-)printing values while evaluation is running. You can even specify how deep these values should be printed recursively, and transform them on the fly. Please consult the docstrings in <code>lib/debug.nix</code> for usage information.</p>"},{"location":"functions/generators.section.html","title":"Generators","text":"<p>Generators are functions that create file formats from nix data structures, e.\u202fg. for configuration files. There are generators available for: <code>INI</code>, <code>JSON</code> and <code>YAML</code></p> <p>All generators follow a similar call interface: <code>generatorName configFunctions data</code>, where <code>configFunctions</code> is an attrset of user-defined functions that format nested parts of the content. They each have common defaults, so often they do not need to be set manually. An example is <code>mkSectionName ? (name: libStr.escape [ \"[\" \"]\" ] name)</code> from the <code>INI</code> generator. It receives the name of a section and sanitizes it. The default <code>mkSectionName</code> escapes <code>[</code> and <code>]</code> with a backslash.</p> <p>Generators can be fine-tuned to produce exactly the file format required by your application/service. One example is an INI-file format which uses <code>:</code> as separator, the strings <code>\"yes\"</code>/<code>\"no\"</code> as boolean values and requires all string values to be quoted:</p> <pre><code>with lib;\nlet\n  customToINI = generators.toINI {\n    # specifies how to format a key/value pair\n    mkKeyValue = generators.mkKeyValueDefault {\n      # specifies the generated string for a subset of nix values\n      mkValueString = v:\n             if v == true then ''\"yes\"''\n        else if v == false then ''\"no\"''\n        else if isString v then ''\"${v}\"''\n        # and delegates all other values to the default generator\n        else generators.mkValueStringDefault {} v;\n    } \":\";\n  };\n\n# the INI file can now be given as plain old nix values\nin customToINI {\n  main = {\n    pushinfo = true;\n    autopush = false;\n    host = \"localhost\";\n    port = 42;\n  };\n  mergetool = {\n    merge = \"diff3\";\n  };\n}\n</code></pre> <p>This will produce the following INI file as nix string:</p> <pre><code>[main]\nautopush:\"no\"\nhost:\"localhost\"\nport:42\npushinfo:\"yes\"\nstr\\:ange:\"very::strange\"\n\n[mergetool]\nmerge:\"diff3\"\n</code></pre> <p>::: {.note} Nix store paths can be converted to strings by enclosing a derivation attribute like so: <code>\"${drv}\"</code>. :::</p> <p>Detailed documentation for each generator can be found in <code>lib/generators.nix</code>.</p>"},{"location":"functions/nix-gitignore.section.html","title":"pkgs.nix-gitignore","text":"<p><code>pkgs.nix-gitignore</code> is a function that acts similarly to <code>builtins.filterSource</code> but also allows filtering with the help of the gitignore format.</p>"},{"location":"functions/nix-gitignore.section.html#sec-pkgs-nix-gitignore-usage","title":"Usage","text":"<p><code>pkgs.nix-gitignore</code> exports a number of functions, but you'll most likely need either <code>gitignoreSource</code> or <code>gitignoreSourcePure</code>. As their first argument, they both accept either 1. a file with gitignore lines or 2. a string with gitignore lines, or 3. a list of either of the two. They will be concatenated into a single big string.</p> <pre><code>{ pkgs ? import &lt;nixpkgs&gt; {} }:\n\n nix-gitignore.gitignoreSource [] ./source\n     # Simplest version\n\n nix-gitignore.gitignoreSource \"supplemental-ignores\\n\" ./source\n     # This one reads the ./source/.gitignore and concats the auxiliary ignores\n\n nix-gitignore.gitignoreSourcePure \"ignore-this\\nignore-that\\n\" ./source\n     # Use this string as gitignore, don't read ./source/.gitignore.\n\n nix-gitignore.gitignoreSourcePure [\"ignore-this\\nignore-that\\n\", ~/.gitignore] ./source\n     # It also accepts a list (of strings and paths) that will be concatenated\n     # once the paths are turned to strings via readFile.\n</code></pre> <p>These functions are derived from the <code>Filter</code> functions by setting the first filter argument to <code>(_: _: true)</code>:</p> <pre><code>gitignoreSourcePure = gitignoreFilterSourcePure (_: _: true);\ngitignoreSource = gitignoreFilterSource (_: _: true);\n</code></pre> <p>Those filter functions accept the same arguments the <code>builtins.filterSource</code> function would pass to its filters, thus <code>fn: gitignoreFilterSourcePure fn \"\"</code> should be extensionally equivalent to <code>filterSource</code>. The file is blacklisted if it's blacklisted by either your filter or the gitignoreFilter.</p> <p>If you want to make your own filter from scratch, you may use</p> <pre><code>gitignoreFilter = ign: root: filterPattern (gitignoreToPatterns ign) root;\n</code></pre>"},{"location":"functions/nix-gitignore.section.html#sec-pkgs-nix-gitignore-usage-recursive","title":"gitignore files in subdirectories","text":"<p>If you wish to use a filter that would search for .gitignore files in subdirectories, just like git does by default, use this function:</p> <pre><code>gitignoreFilterRecursiveSource = filter: patterns: root:\n# OR\ngitignoreRecursiveSource = gitignoreFilterSourcePure (_: _: true);\n</code></pre>"},{"location":"functions/prefer-remote-fetch.section.html","title":"prefer-remote-fetch overlay","text":"<p><code>prefer-remote-fetch</code> is an overlay that download sources on remote builder. This is useful when the evaluating machine has a slow upload while the builder can fetch faster directly from the source. To use it, put the following snippet as a new overlay:</p> <pre><code>self: super:\n  (super.prefer-remote-fetch self super)\n</code></pre> <p>A full configuration example for that sets the overlay up for your own account, could look like this</p> <pre><code>$ mkdir ~/.config/nixpkgs/overlays/\n$ cat &gt; ~/.config/nixpkgs/overlays/prefer-remote-fetch.nix &lt;&lt;EOF\n  self: super: super.prefer-remote-fetch self super\nEOF\n</code></pre>"},{"location":"hooks/index.html","title":"Hooks reference","text":"<p>Nixpkgs has several hook packages that augment the stdenv phases.</p> <p>The stdenv built-in hooks are documented in .</p> <p><code>{=include=} sections autoconf.section.md automake.section.md autopatchelf.section.md bmake.section.md breakpoint.section.md cmake.section.md gdk-pixbuf.section.md ghc.section.md gnome.section.md installShellFiles.section.md libiconv.section.md libxml2.section.md meson.section.md mpi-check-hook.section.md ninja.section.md patch-rc-path-hooks.section.md perl.section.md pkg-config.section.md postgresql-test-hook.section.md python.section.md scons.section.md tetex-tex-live.section.md unzip.section.md validatePkgConfig.section.md waf.section.md zig.section.md xcbuild.section.md</code></p>"},{"location":"hooks/autoconf.section.html","title":"Autoconf","text":"<p>The <code>autoreconfHook</code> derivation adds <code>autoreconfPhase</code>, which runs autoreconf, libtoolize and automake, essentially preparing the configure script in autotools-based builds. Most autotools-based packages come with the configure script pre-generated, but this hook is necessary for a few packages and when you need to patch the package\u2019s configure scripts.</p>"},{"location":"hooks/automake.section.html","title":"Automake","text":"<p>Adds the <code>share/aclocal</code> subdirectory of each build input to the <code>ACLOCAL_PATH</code> environment variable.</p>"},{"location":"hooks/autopatchelf.section.html","title":"autoPatchelfHook","text":"<p>This is a special setup hook which helps in packaging proprietary software in that it automatically tries to find missing shared library dependencies of ELF files based on the given <code>buildInputs</code> and <code>nativeBuildInputs</code>.</p> <p>You can also specify a <code>runtimeDependencies</code> variable which lists dependencies to be unconditionally added to rpath of all executables. This is useful for programs that use dlopen 3 to load libraries at runtime.</p> <p>In certain situations you may want to run the main command (<code>autoPatchelf</code>) of the setup hook on a file or a set of directories instead of unconditionally patching all outputs. This can be done by setting the <code>dontAutoPatchelf</code> environment variable to a non-empty value.</p> <p>By default <code>autoPatchelf</code> will fail as soon as any ELF file requires a dependency which cannot be resolved via the given build inputs. In some situations you might prefer to just leave missing dependencies unpatched and continue to patch the rest. This can be achieved by setting the <code>autoPatchelfIgnoreMissingDeps</code> environment variable to a non-empty value. <code>autoPatchelfIgnoreMissingDeps</code> can be set to a list like <code>autoPatchelfIgnoreMissingDeps = [ \"libcuda.so.1\" \"libcudart.so.1\" ];</code> or to <code>[ \"*\" ]</code> to ignore all missing dependencies.</p> <p>The <code>autoPatchelf</code> command also recognizes a <code>--no-recurse</code> command line flag, which prevents it from recursing into subdirectories.</p>"},{"location":"hooks/bmake.section.html","title":"bmake","text":"<p>bmake is the portable variant of NetBSD make utility.</p> <p>In Nixpkgs, <code>bmake</code> comes with a hook that overrides the default build, check, install and dist phases.</p>"},{"location":"hooks/breakpoint.section.html","title":"breakpointHook","text":"<p>This hook will make a build pause instead of stopping when a failure happens. It prevents nix from cleaning up the build environment immediately and allows the user to attach to a build environment using the <code>cntr</code> command. Upon build error it will print instructions on how to use <code>cntr</code>, which can be used to enter the environment for debugging. Installing cntr and running the command will provide shell access to the build sandbox of failed build. At <code>/var/lib/cntr</code> the sandboxed filesystem is mounted. All commands and files of the system are still accessible within the shell. To execute commands from the sandbox use the cntr exec subcommand. <code>cntr</code> is only supported on Linux-based platforms. To use it first add <code>cntr</code> to your <code>environment.systemPackages</code> on NixOS or alternatively to the root user on non-NixOS systems. Then in the package that is supposed to be inspected, add <code>breakpointHook</code> to <code>nativeBuildInputs</code>.</p> <pre><code>nativeBuildInputs = [ breakpointHook ];\n</code></pre> <p>When a build failure happens there will be an instruction printed that shows how to attach with <code>cntr</code> to the build sandbox.</p> <p>::: {.note} Caution with remote builds</p> <p>This won\u2019t work with remote builds as the build environment is on a different machine and can\u2019t be accessed by <code>cntr</code>. Remote builds can be turned off by setting <code>--option builders ''</code> for <code>nix-build</code> or <code>--builders ''</code> for <code>nix build</code>. :::</p>"},{"location":"hooks/cmake.section.html","title":"cmake","text":"<p>Overrides the default configure phase to run the CMake command. By default, we use the Make generator of CMake. In addition, dependencies are added automatically to <code>CMAKE_PREFIX_PATH</code> so that packages are correctly detected by CMake. Some additional flags are passed in to give similar behavior to configure-based packages. You can disable this hook\u2019s behavior by setting <code>configurePhase</code> to a custom value, or by setting <code>dontUseCmakeConfigure</code>. <code>cmakeFlags</code> controls flags passed only to CMake. By default, parallel building is enabled as CMake supports parallel building almost everywhere. When Ninja is also in use, CMake will detect that and use the ninja generator.</p>"},{"location":"hooks/gdk-pixbuf.section.html","title":"gdk-pixbuf","text":"<p>Exports <code>GDK_PIXBUF_MODULE_FILE</code> environment variable to the builder. Add librsvg package to <code>buildInputs</code> to get svg support. See also the setup hook description in GNOME platform docs.</p>"},{"location":"hooks/ghc.section.html","title":"GHC","text":"<p>Creates a temporary package database and registers every Haskell build input in it (TODO: how?).</p>"},{"location":"hooks/gnome.section.html","title":"GNOME platform","text":"<p>Hooks related to GNOME platform and related libraries like GLib, GTK and GStreamer are described in .</p>"},{"location":"hooks/installShellFiles.section.html","title":"<code>installShellFiles</code>","text":"<p>This hook helps with installing manpages and shell completion files. It exposes 2 shell functions <code>installManPage</code> and <code>installShellCompletion</code> that can be used from your <code>postInstall</code> hook.</p> <p>The <code>installManPage</code> function takes one or more paths to manpages to install. The manpages must have a section suffix, and may optionally be compressed (with <code>.gz</code> suffix). This function will place them into the correct <code>share/man/man&lt;section&gt;/</code> directory, in <code>outputMan</code>.</p> <p>The <code>installShellCompletion</code> function takes one or more paths to shell completion files. By default it will autodetect the shell type from the completion file extension, but you may also specify it by passing one of <code>--bash</code>, <code>--fish</code>, or <code>--zsh</code>. These flags apply to all paths listed after them (up until another shell flag is given). Each path may also have a custom installation name provided by providing a flag <code>--name NAME</code> before the path. If this flag is not provided, zsh completions will be renamed automatically such that <code>foobar.zsh</code> becomes <code>_foobar</code>. A root name may be provided for all paths using the flag <code>--cmd NAME</code>; this synthesizes the appropriate name depending on the shell (e.g. <code>--cmd foo</code> will synthesize the name <code>foo.bash</code> for bash and <code>_foo</code> for zsh). The path may also be a fifo or named fd (such as produced by <code>&lt;(cmd)</code>), in which case the shell and name must be provided.</p> <pre><code>nativeBuildInputs = [ installShellFiles ];\npostInstall = ''\n  installManPage doc/foobar.1 doc/barfoo.3\n  # explicit behavior\n  installShellCompletion --bash --name foobar.bash share/completions.bash\n  installShellCompletion --fish --name foobar.fish share/completions.fish\n  installShellCompletion --zsh --name _foobar share/completions.zsh\n  # implicit behavior\n  installShellCompletion share/completions/foobar.{bash,fish,zsh}\n  # using named fd\n  installShellCompletion --cmd foobar \\\n    --bash &lt;($out/bin/foobar --bash-completion) \\\n    --fish &lt;($out/bin/foobar --fish-completion) \\\n    --zsh &lt;($out/bin/foobar --zsh-completion)\n'';\n</code></pre>"},{"location":"hooks/libiconv.section.html","title":"libiconv, libintl","text":"<p>A few libraries automatically add to <code>NIX_LDFLAGS</code> their library, making their symbols automatically available to the linker. This includes libiconv and libintl (gettext). This is done to provide compatibility between GNU Linux, where libiconv and libintl are bundled in, and other systems where that might not be the case. Sometimes, this behavior is not desired. To disable this behavior, set <code>dontAddExtraLibs</code>.</p>"},{"location":"hooks/libxml2.section.html","title":"libxml2","text":"<p>Adds every file named <code>catalog.xml</code> found under the <code>xml/dtd</code> and <code>xml/xsl</code> subdirectories of each build input to the <code>XML_CATALOG_FILES</code> environment variable.</p>"},{"location":"hooks/meson.section.html","title":"Meson","text":"<p>Meson is an open source meta build system meant to be fast and user-friendly.</p> <p>In Nixpkgs, meson comes with a setup hook that overrides the configure, check, and install phases.</p> <p>Being a meta build system, meson needs an accompanying backend. In the context of Nixpkgs, the typical companion backend is Ninja, that provides a setup hook registering ninja-based build and install phases.</p>"},{"location":"hooks/meson.section.html#meson-variables-controlling","title":"Variables controlling Meson","text":""},{"location":"hooks/meson.section.html#meson-exclusive-variables","title":"Meson Exclusive Variables","text":""},{"location":"hooks/meson.section.html#meson-flags","title":"<code>mesonFlags</code>","text":"<p>Controls the flags passed to <code>meson setup</code> during configure phase.</p>"},{"location":"hooks/meson.section.html#meson-wrap-mode","title":"<code>mesonWrapMode</code>","text":"<p>Which value is passed as <code>-Dwrap_mode=</code> to. In Nixpkgs the default value is <code>nodownload</code>, so that no subproject will be downloaded (since network access is already disabled during deployment in Nixpkgs).</p> <p>Note: Meson allows pre-population of subprojects that would otherwise be downloaded.</p>"},{"location":"hooks/meson.section.html#meson-build-type","title":"<code>mesonBuildType</code>","text":"<p>Which value is passed as <code>--buildtype</code> to <code>meson setup</code> during configure phase. In Nixpkgs the default value is <code>plain</code>.</p>"},{"location":"hooks/meson.section.html#meson-auto-features","title":"<code>mesonAutoFeatures</code>","text":"<p>Which value is passed as <code>-Dauto_features=</code> to <code>meson setup</code> during configure phase. In Nixpkgs the default value is <code>enabled</code>, meaning that every feature declared as \"auto\" by the meson scripts will be enabled.</p>"},{"location":"hooks/meson.section.html#meson-check-flags","title":"<code>mesonCheckFlags</code>","text":"<p>Controls the flags passed to <code>meson test</code> during check phase.</p>"},{"location":"hooks/meson.section.html#meson-install-flags","title":"<code>mesonInstallFlags</code>","text":"<p>Controls the flags passed to <code>meson install</code> during install phase.</p>"},{"location":"hooks/meson.section.html#meson-install-tags","title":"<code>mesonInstallTags</code>","text":"<p>A list of installation tags passed to Meson's commandline option <code>--tags</code> during install phase.</p> <p>Note: <code>mesonInstallTags</code> should be a list of strings, that will be converted to a comma-separated string that is recognized to <code>--tags</code>. Example: <code>mesonInstallTags = [ \"emulator\" \"assembler\" ];</code> will be converted to <code>--tags emulator,assembler</code>.</p>"},{"location":"hooks/meson.section.html#dont-use-meson-configure","title":"<code>dontUseMesonConfigure</code>","text":"<p>When set to true, don't use the predefined <code>mesonConfigurePhase</code>.</p>"},{"location":"hooks/meson.section.html#dont-use-meson-check","title":"<code>dontUseMesonCheck</code>","text":"<p>When set to true, don't use the predefined <code>mesonCheckPhase</code>.</p>"},{"location":"hooks/meson.section.html#dont-use-meson-install","title":"<code>dontUseMesonInstall</code>","text":"<p>When set to true, don't use the predefined <code>mesonInstallPhase</code>.</p>"},{"location":"hooks/meson.section.html#meson-honored-variables","title":"Honored variables","text":"<p>The following variables commonly used by <code>stdenv.mkDerivation</code> are honored by Meson setup hook.</p> <ul> <li><code>prefixKey</code></li> <li><code>enableParallelBuilding</code></li> </ul>"},{"location":"hooks/mpi-check-hook.section.html","title":"mpiCheckPhaseHook","text":"<p>This hook can be used to setup a check phase that requires running a MPI application. It detects the used present MPI implementation type and exports the neceesary environment variables to use <code>mpirun</code> and <code>mpiexec</code> in a Nix sandbox.</p> <p>Example:</p> <pre><code>  { mpiCheckPhaseHook, mpi, ... }:\n\n  ...\n\n  nativeCheckInputs = [\n    openssh\n    mpiCheckPhaseHook\n  ];\n</code></pre>"},{"location":"hooks/ninja.section.html","title":"ninja","text":"<p>Overrides the build, install, and check phase to run ninja instead of make. You can disable this behavior with the <code>dontUseNinjaBuild</code>, <code>dontUseNinjaInstall</code>, and <code>dontUseNinjaCheck</code>, respectively. Parallel building is enabled by default in Ninja.</p> <p>Note that if the Meson setup hook is also active, Ninja's install and check phases will be disabled in favor of Meson's.</p>"},{"location":"hooks/patch-rc-path-hooks.section.html","title":"<code>patchRcPath</code> hooks","text":"<p>These hooks provide shell-specific utilities (with the same name as the hook) to patch shell scripts meant to be sourced by software users.</p> <p>The typical usage is to patch initialisation or rc scripts inside <code>$out/bin</code> or <code>$out/etc</code>. Such scripts, when being sourced, would insert the binary locations of certain commands into <code>PATH</code>, modify other environment variables or run a series of start-up commands. When shipped from the upstream, they sometimes use commands that might not be available in the environment they are getting sourced in.</p> <p>The compatible shells for each hook are:</p> <ul> <li><code>patchRcPathBash</code>: Bash, ksh, zsh and other shells supporting the Bash-like parameter expansions.</li> <li><code>patchRcPathCsh</code>: Csh scripts, such as those targeting tcsh.</li> <li><code>patchRcPathFish</code>: Fish scripts.</li> <li><code>patchRcPathPosix</code>: POSIX-conformant shells supporting the limited parameter expansions specified by the POSIX standard. Current implementation uses the parameter expansion <code>${foo-}</code> only.</li> </ul> <p>For each supported shell, it modifies the script with a <code>PATH</code> prefix that is later removed when the script ends. It allows nested patching, which guarantees that a patched script may source another patched script.</p> <p>Syntax to apply the utility to a script:</p> <pre><code>patchRcPath&lt;shell&gt; &lt;file&gt; &lt;PATH-prefix&gt;\n</code></pre> <p>Example usage:</p> <p>Given a package <code>foo</code> containing an init script <code>this-foo.fish</code> that depends on <code>coreutils</code>, <code>man</code> and <code>which</code>, patch the init script for users to source without having the above dependencies in their <code>PATH</code>:</p> <pre><code>{ lib, stdenv, patchRcPathFish}:\nstdenv.mkDerivation {\n\n  # ...\n\n  nativeBuildInputs = [\n    patchRcPathFish\n  ];\n\n  postFixup = ''\n    patchRcPathFish $out/bin/this-foo.fish ${lib.makeBinPath [ coreutils man which ]}\n  '';\n}\n</code></pre> <p>::: {.note} <code>patchRcPathCsh</code> and <code>patchRcPathPosix</code> implementation depends on <code>sed</code> to do the string processing. The others are in vanilla shell and have no third-party dependencies. :::</p>"},{"location":"hooks/perl.section.html","title":"Perl","text":"<p>Adds the <code>lib/site_perl</code> subdirectory of each build input to the <code>PERL5LIB</code> environment variable. For instance, if <code>buildInputs</code> contains Perl, then the <code>lib/site_perl</code> subdirectory of each input is added to the <code>PERL5LIB</code> environment variable.</p>"},{"location":"hooks/pkg-config.section.html","title":"pkg-config","text":"<p>Adds the <code>lib/pkgconfig</code> and <code>share/pkgconfig</code> subdirectories of each build input to the <code>PKG_CONFIG_PATH</code> environment variable.</p>"},{"location":"hooks/postgresql-test-hook.section.html","title":"<code>postgresqlTestHook</code>","text":"<p>This hook starts a PostgreSQL server during the <code>checkPhase</code>. Example:</p> <pre><code>{ stdenv, postgresql, postgresqlTestHook }:\nstdenv.mkDerivation {\n\n  # ...\n\n  nativeCheckInputs = [\n    postgresql\n    postgresqlTestHook\n  ];\n}\n</code></pre> <p>If you use a custom <code>checkPhase</code>, remember to add the <code>runHook</code> calls: <pre><code>  checkPhase ''\n    runHook preCheck\n\n    # ... your tests\n\n    runHook postCheck\n  ''\n</code></pre></p>"},{"location":"hooks/postgresql-test-hook.section.html#sec-postgresqlTestHook-variables","title":"Variables","text":"<p>The hook logic will read a number of variables and set them to a default value if unset or empty.</p> <p>Exported variables:</p> <ul> <li><code>PGDATA</code>: location of server files.</li> <li><code>PGHOST</code>: location of UNIX domain socket directory; the default <code>host</code> in a connection string.</li> <li><code>PGUSER</code>: user to create / log in with, default: <code>test_user</code>.</li> <li><code>PGDATABASE</code>: database name, default: <code>test_db</code>.</li> </ul> <p>Bash-only variables:</p> <ul> <li><code>postgresqlTestUserOptions</code>: SQL options to use when creating the <code>$PGUSER</code> role, default: <code>\"LOGIN\"</code>. Example: <code>\"LOGIN SUPERUSER\"</code></li> <li><code>postgresqlTestSetupSQL</code>: SQL commands to run as database administrator after startup, default: statements that create <code>$PGUSER</code> and <code>$PGDATABASE</code>.</li> <li><code>postgresqlTestSetupCommands</code>: bash commands to run after database start, defaults to running <code>$postgresqlTestSetupSQL</code> as database administrator.</li> <li><code>postgresqlEnableTCP</code>: set to <code>1</code> to enable TCP listening. Flaky; not recommended.</li> <li><code>postgresqlStartCommands</code>: defaults to <code>pg_ctl start</code>.</li> <li><code>postgresqlExtraSettings</code>: Additional configuration to add to <code>postgresql.conf</code></li> </ul>"},{"location":"hooks/postgresql-test-hook.section.html#sec-postgresqlTestHook-hooks","title":"Hooks","text":"<p>A number of additional hooks are ran in postgresqlTestHook</p> <ul> <li><code>postgresqlTestSetupPost</code>: ran after postgresql has been set up.</li> </ul>"},{"location":"hooks/postgresql-test-hook.section.html#sec-postgresqlTestHook-tcp","title":"TCP and the Nix sandbox","text":"<p><code>postgresqlEnableTCP</code> relies on network sandboxing, which is not available on macOS and some custom Nix installations, resulting in flaky tests. For this reason, it is disabled by default.</p> <p>The preferred solution is to make the test suite use a UNIX domain socket connection. This is the default behavior when no <code>host</code> connection parameter is provided. Some test suites hardcode a value for <code>host</code> though, so a patch may be required. If you can upstream the patch, you can make <code>host</code> default to the <code>PGHOST</code> environment variable when set. Otherwise, you can patch it locally to omit the <code>host</code> connection string parameter altogether.</p> <p>::: {.note} The error <code>libpq: failed (could not receive data from server: Connection refused</code> is generally an indication that the test suite is trying to connect through TCP. :::</p>"},{"location":"hooks/python.section.html","title":"Python","text":"<p>Adds the <code>python.sitePackages</code> subdirectory (i.e. <code>lib/pythonX.Y/site-packages</code>) of each build input to the <code>PYTHONPATH</code> environment variable.</p>"},{"location":"hooks/scons.section.html","title":"scons","text":"<p>Overrides the build, install, and check phases. This uses the scons build system as a replacement for make. scons does not provide a configure phase, so everything is managed at build and install time.</p>"},{"location":"hooks/tetex-tex-live.section.html","title":"teTeX / TeX Live","text":"<p>Adds the <code>share/texmf-nix</code> subdirectory of each build input to the <code>TEXINPUTS</code> environment variable.</p>"},{"location":"hooks/unzip.section.html","title":"unzip","text":"<p>This setup hook will allow you to unzip .zip files specified in <code>$src</code>. There are many similar packages like <code>unrar</code>, <code>undmg</code>, etc.</p>"},{"location":"hooks/validatePkgConfig.section.html","title":"validatePkgConfig","text":"<p>The <code>validatePkgConfig</code> hook validates all pkg-config (<code>.pc</code>) files in a package. This helps catching some common errors in pkg-config files, such as undefined variables.</p>"},{"location":"hooks/waf.section.html","title":"wafHook","text":"<p>Waf is a Python-based software building system.</p> <p>In Nixpkgs, <code>wafHook</code> overrides the default configure, build, and install phases.</p>"},{"location":"hooks/waf.section.html#waf-hook-variables-controlling","title":"Variables controlling wafHook","text":""},{"location":"hooks/waf.section.html#waf-hook-exclusive-variables","title":"<code>wafHook</code> Exclusive Variables","text":"<p>The variables below are exclusive of <code>wafHook</code>.</p>"},{"location":"hooks/waf.section.html#waf-path","title":"<code>wafPath</code>","text":"<p>Location of the <code>waf</code> tool. It defaults to <code>./waf</code>, to honor software projects that include it directly inside their source trees.</p> <p>If <code>wafPath</code> doesn't exist, then <code>wafHook</code> will copy the <code>waf</code> provided from Nixpkgs to it.</p>"},{"location":"hooks/waf.section.html#waf-flags","title":"<code>wafFlags</code>","text":"<p>Controls the flags passed to waf tool during build and install phases. For settings specific to build or install phases, use <code>wafBuildFlags</code> or <code>wafInstallFlags</code> respectively.</p>"},{"location":"hooks/waf.section.html#dont-add-waf-cross-flags","title":"<code>dontAddWafCrossFlags</code>","text":"<p>When set to <code>true</code>, don't add cross compilation flags during configure phase.</p>"},{"location":"hooks/waf.section.html#dont-use-waf-configure","title":"<code>dontUseWafConfigure</code>","text":"<p>When set to true, don't use the predefined <code>wafConfigurePhase</code>.</p>"},{"location":"hooks/waf.section.html#dont-use-waf-build","title":"<code>dontUseWafBuild</code>","text":"<p>When set to true, don't use the predefined <code>wafBuildPhase</code>.</p>"},{"location":"hooks/waf.section.html#dont-use-waf-install","title":"<code>dontUseWafInstall</code>","text":"<p>When set to true, don't use the predefined <code>wafInstallPhase</code>.</p>"},{"location":"hooks/waf.section.html#waf-hook-similar-variables","title":"Similar variables","text":"<p>The following variables are similar to their <code>stdenv.mkDerivation</code> counterparts.</p> <code>wafHook</code> Variable <code>stdenv.mkDerivation</code> Counterpart <code>wafConfigureFlags</code> <code>configureFlags</code> <code>wafConfigureTargets</code> <code>configureTargets</code> <code>wafBuildFlags</code> <code>buildFlags</code> <code>wafBuildTargets</code> <code>buildTargets</code> <code>wafInstallFlags</code> <code>installFlags</code> <code>wafInstallTargets</code> <code>installTargets</code>"},{"location":"hooks/waf.section.html#waf-hook-honored-variables","title":"Honored variables","text":"<p>The following variables commonly used by <code>stdenv.mkDerivation</code> are honored by <code>wafHook</code>.</p> <ul> <li><code>prefixKey</code></li> <li><code>enableParallelBuilding</code></li> <li><code>enableParallelInstalling</code></li> </ul>"},{"location":"hooks/xcbuild.section.html","title":"xcbuildHook","text":"<p>Overrides the build and install phases to run the \"xcbuild\" command. This hook is needed when a project only comes with build files for the XCode build system. You can disable this behavior by setting buildPhase and configurePhase to a custom value. xcbuildFlags controls flags passed only to xcbuild.</p>"},{"location":"hooks/zig.section.html","title":"zig.hook","text":"<p>Zig is a general-purpose programming language and toolchain for maintaining robust, optimal and reusable software.</p> <p>In Nixpkgs, <code>zig.hook</code> overrides the default build, check and install phases.</p>"},{"location":"hooks/zig.section.html#zig-hook-example-code-snippet","title":"Example code snippet","text":"<pre><code>{ lib\n, stdenv\n, zig_0_11\n}:\n\nstdenv.mkDerivation {\n  # . . .\n\n  nativeBuildInputs = [\n    zig_0_11.hook\n  ];\n\n  zigBuildFlags = [ \"-Dman-pages=true\" ];\n\n  dontUseZigCheck = true;\n\n  # . . .\n}\n</code></pre>"},{"location":"hooks/zig.section.html#zig-hook-variables-controlling","title":"Variables controlling zig.hook","text":""},{"location":"hooks/zig.section.html#zig-hook-exclusive-variables","title":"<code>zig.hook</code> Exclusive Variables","text":"<p>The variables below are exclusive to <code>zig.hook</code>.</p>"},{"location":"hooks/zig.section.html#dont-use-zig-build","title":"<code>dontUseZigBuild</code>","text":"<p>Disables using <code>zigBuildPhase</code>.</p>"},{"location":"hooks/zig.section.html#dont-use-zig-check","title":"<code>dontUseZigCheck</code>","text":"<p>Disables using <code>zigCheckPhase</code>.</p>"},{"location":"hooks/zig.section.html#dont-use-zig-install","title":"<code>dontUseZigInstall</code>","text":"<p>Disables using <code>zigInstallPhase</code>.</p>"},{"location":"hooks/zig.section.html#zig-hook-similar-variables","title":"Similar variables","text":"<p>The following variables are similar to their <code>stdenv.mkDerivation</code> counterparts.</p> <code>zig.hook</code> Variable <code>stdenv.mkDerivation</code> Counterpart <code>zigBuildFlags</code> <code>buildFlags</code> <code>zigCheckFlags</code> <code>checkFlags</code> <code>zigInstallFlags</code> <code>installFlags</code>"},{"location":"hooks/zig.section.html#zig-hook-variables-honored","title":"Variables honored by zig.hook","text":"<p>The following variables commonly used by <code>stdenv.mkDerivation</code> are honored by <code>zig.hook</code>.</p> <ul> <li><code>prefixKey</code></li> <li><code>dontAddPrefix</code></li> </ul>"},{"location":"languages-frameworks/index.html","title":"Languages and frameworks","text":"<p>The standard build environment makes it easy to build typical Autotools-based packages with very little code. Any other kind of package can be accommodated by overriding the appropriate phases of <code>stdenv</code>. However, there are specialised functions in Nixpkgs to easily build packages for other programming languages, such as Perl or Haskell. These are described in this chapter.</p> <p><code>{=include=} sections agda.section.md android.section.md beam.section.md bower.section.md chicken.section.md coq.section.md crystal.section.md cuda.section.md cuelang.section.md dart.section.md dhall.section.md dotnet.section.md emscripten.section.md gnome.section.md go.section.md haskell.section.md hy.section.md idris.section.md idris2.section.md ios.section.md java.section.md javascript.section.md julia.section.md lisp.section.md lua.section.md maven.section.md nim.section.md ocaml.section.md octave.section.md perl.section.md php.section.md pkg-config.section.md python.section.md qt.section.md r.section.md ruby.section.md rust.section.md swift.section.md texlive.section.md titanium.section.md vim.section.md</code></p>"},{"location":"languages-frameworks/agda.section.html","title":"Agda","text":""},{"location":"languages-frameworks/agda.section.html#how-to-use-agda","title":"How to use Agda","text":"<p>Agda is available as the agda package.</p> <p>The <code>agda</code> package installs an Agda-wrapper, which calls <code>agda</code> with <code>--library-file</code> set to a generated library-file within the nix store, this means your library-file in <code>$HOME/.agda/libraries</code> will be ignored. By default the agda package installs Agda with no libraries, i.e. the generated library-file is empty. To use Agda with libraries, the <code>agda.withPackages</code> function can be used. This function either takes:</p> <ul> <li>A list of packages,</li> <li>or a function which returns a list of packages when given the <code>agdaPackages</code> attribute set,</li> <li>or an attribute set containing a list of packages and a GHC derivation for compilation (see below).</li> <li>or an attribute set containing a function which returns a list of packages when given the <code>agdaPackages</code> attribute set and a GHC derivation for compilation (see below).</li> </ul> <p>For example, suppose we wanted a version of Agda which has access to the standard library. This can be obtained with the expressions:</p> <pre><code>agda.withPackages [ agdaPackages.standard-library ]\n</code></pre> <p>or</p> <pre><code>agda.withPackages (p: [ p.standard-library ])\n</code></pre> <p>or can be called as in the Compiling Agda section.</p> <p>If you want to use a different version of a library (for instance a development version) override the <code>src</code> attribute of the package to point to your local repository</p> <pre><code>agda.withPackages (p: [\n  (p.standard-library.overrideAttrs (oldAttrs: {\n    version = \"local version\";\n    src = /path/to/local/repo/agda-stdlib;\n  }))\n])\n</code></pre> <p>You can also reference a GitHub repository</p> <pre><code>agda.withPackages (p: [\n  (p.standard-library.overrideAttrs (oldAttrs: {\n    version = \"1.5\";\n    src =  fetchFromGitHub {\n      repo = \"agda-stdlib\";\n      owner = \"agda\";\n      rev = \"v1.5\";\n      hash = \"sha256-nEyxYGSWIDNJqBfGpRDLiOAnlHJKEKAOMnIaqfVZzJk=\";\n    };\n  }))\n])\n</code></pre> <p>If you want to use a library not added to Nixpkgs, you can add a dependency to a local library by calling <code>agdaPackages.mkDerivation</code>.</p> <pre><code>agda.withPackages (p: [\n  (p.mkDerivation {\n    pname = \"your-agda-lib\";\n    version = \"1.0.0\";\n    src = /path/to/your-agda-lib;\n  })\n])\n</code></pre> <p>Again you can reference GitHub</p> <pre><code>agda.withPackages (p: [\n  (p.mkDerivation {\n    pname = \"your-agda-lib\";\n    version = \"1.0.0\";\n    src = fetchFromGitHub {\n      repo = \"repo\";\n      owner = \"owner\";\n      version = \"...\";\n      rev = \"...\";\n      hash = \"...\";\n    };\n  })\n])\n</code></pre> <p>See Building Agda Packages for more information on <code>mkDerivation</code>.</p> <p>Agda will not by default use these libraries. To tell Agda to use a library we have some options:</p> <ul> <li>Call <code>agda</code> with the library flag:   <pre><code>$ agda -l standard-library -i . MyFile.agda\n</code></pre></li> <li>Write a <code>my-library.agda-lib</code> file for the project you are working on which may look like:   <pre><code>name: my-library\ninclude: .\ndepend: standard-library\n</code></pre></li> <li>Create the file <code>~/.agda/defaults</code> and add any libraries you want to use by default.</li> </ul> <p>More information can be found in the official Agda documentation on library management.</p>"},{"location":"languages-frameworks/agda.section.html#compiling-agda","title":"Compiling Agda","text":"<p>Agda modules can be compiled using the GHC backend with the <code>--compile</code> flag. A version of <code>ghc</code> with <code>ieee754</code> is made available to the Agda program via the <code>--with-compiler</code> flag. This can be overridden by a different version of <code>ghc</code> as follows:</p> <pre><code>agda.withPackages {\n  pkgs = [ ... ];\n  ghc = haskell.compiler.ghcHEAD;\n}\n</code></pre>"},{"location":"languages-frameworks/agda.section.html#writing-agda-packages","title":"Writing Agda packages","text":"<p>To write a nix derivation for an Agda library, first check that the library has a <code>*.agda-lib</code> file.</p> <p>A derivation can then be written using <code>agdaPackages.mkDerivation</code>. This has similar arguments to <code>stdenv.mkDerivation</code> with the following additions:</p> <ul> <li><code>everythingFile</code> can be used to specify the location of the <code>Everything.agda</code> file, defaulting to <code>./Everything.agda</code>. If this file does not exist then either it should be patched in or the <code>buildPhase</code> should be overridden (see below).</li> <li><code>libraryName</code> should be the name that appears in the <code>*.agda-lib</code> file, defaulting to <code>pname</code>.</li> <li><code>libraryFile</code> should be the file name of the <code>*.agda-lib</code> file, defaulting to <code>${libraryName}.agda-lib</code>.</li> </ul> <p>Here is an example <code>default.nix</code></p> <pre><code>{ nixpkgs ?  &lt;nixpkgs&gt; }:\nwith (import nixpkgs {});\nagdaPackages.mkDerivation {\n  version = \"1.0\";\n  pname = \"my-agda-lib\";\n  src = ./.;\n  buildInputs = [\n    agdaPackages.standard-library\n  ];\n}\n</code></pre>"},{"location":"languages-frameworks/agda.section.html#building-agda-packages","title":"Building Agda packages","text":"<p>The default build phase for <code>agdaPackages.mkDerivation</code> runs <code>agda</code> on the <code>Everything.agda</code> file. If something else is needed to build the package (e.g. <code>make</code>) then the <code>buildPhase</code> should be overridden. Additionally, a <code>preBuild</code> or <code>configurePhase</code> can be used if there are steps that need to be done prior to checking the <code>Everything.agda</code> file. <code>agda</code> and the Agda libraries contained in <code>buildInputs</code> are made available during the build phase.</p>"},{"location":"languages-frameworks/agda.section.html#installing-agda-packages","title":"Installing Agda packages","text":"<p>The default install phase copies Agda source files, Agda interface files (<code>*.agdai</code>) and <code>*.agda-lib</code> files to the output directory. This can be overridden.</p> <p>By default, Agda sources are files ending on <code>.agda</code>, or literate Agda files ending on <code>.lagda</code>, <code>.lagda.tex</code>, <code>.lagda.org</code>, <code>.lagda.md</code>, <code>.lagda.rst</code>. The list of recognised Agda source extensions can be extended by setting the <code>extraExtensions</code> config variable.</p>"},{"location":"languages-frameworks/agda.section.html#maintaining-the-agda-package-set-on-nixpkgs","title":"Maintaining the Agda package set on Nixpkgs","text":"<p>We are aiming at providing all common Agda libraries as packages on <code>nixpkgs</code>, and keeping them up to date. Contributions and maintenance help is always appreciated, but the maintenance effort is typically low since the Agda ecosystem is quite small.</p> <p>The <code>nixpkgs</code> Agda package set tries to take up a role similar to that of Stackage in the Haskell world. It is a curated set of libraries that:</p> <ol> <li>Always work together.</li> <li>Are as up-to-date as possible.</li> </ol> <p>While the Haskell ecosystem is huge, and Stackage is highly automatised, the Agda package set is small and can (still) be maintained by hand.</p>"},{"location":"languages-frameworks/agda.section.html#adding-agda-packages-to-nixpkgs","title":"Adding Agda packages to Nixpkgs","text":"<p>To add an Agda package to <code>nixpkgs</code>, the derivation should be written to <code>pkgs/development/libraries/agda/${library-name}/</code> and an entry should be added to <code>pkgs/top-level/agda-packages.nix</code>. Here it is called in a scope with access to all other Agda libraries, so the top line of the <code>default.nix</code> can look like:</p> <pre><code>{ mkDerivation, standard-library, fetchFromGitHub }:\n</code></pre> <p>Note that the derivation function is called with <code>mkDerivation</code> set to <code>agdaPackages.mkDerivation</code>, therefore you could use a similar set as in your <code>default.nix</code> from Writing Agda Packages with <code>agdaPackages.mkDerivation</code> replaced with <code>mkDerivation</code>.</p> <p>Here is an example skeleton derivation for iowa-stdlib:</p> <pre><code>mkDerivation {\n  version = \"1.5.0\";\n  pname = \"iowa-stdlib\";\n\n  src = ...\n\n  libraryFile = \"\";\n  libraryName = \"IAL-1.3\";\n\n  buildPhase = ''\n    patchShebangs find-deps.sh\n    make\n  '';\n}\n</code></pre> <p>This library has a file called <code>.agda-lib</code>, and so we give an empty string to <code>libraryFile</code> as nothing precedes <code>.agda-lib</code> in the filename. This file contains <code>name: IAL-1.3</code>, and so we let <code>libraryName =  \"IAL-1.3\"</code>. This library does not use an <code>Everything.agda</code> file and instead has a Makefile, so there is no need to set <code>everythingFile</code> and we set a custom <code>buildPhase</code>.</p> <p>When writing an Agda package it is essential to make sure that no <code>.agda-lib</code> file gets added to the store as a single file (for example by using <code>writeText</code>). This causes Agda to think that the nix store is a Agda library and it will attempt to write to it whenever it typechecks something. See https://github.com/agda/agda/issues/4613.</p> <p>In the pull request adding this library, you can test whether it builds correctly by writing in a comment:</p> <pre><code>@ofborg build agdaPackages.iowa-stdlib\n</code></pre>"},{"location":"languages-frameworks/agda.section.html#agda-maintaining-packages","title":"Maintaining Agda packages","text":"<p>As mentioned before, the aim is to have a compatible, and up-to-date package set. These two conditions sometimes exclude each other: For example, if we update <code>agdaPackages.standard-library</code> because there was an upstream release, this will typically break many reverse dependencies, i.e. downstream Agda libraries that depend on the standard library. In <code>nixpkgs</code> we are typically among the first to notice this, since we have build tests in place to check this.</p> <p>In a pull request updating e.g. the standard library, you should write the following comment:</p> <pre><code>@ofborg build agdaPackages.standard-library.passthru.tests\n</code></pre> <p>This will build all reverse dependencies of the standard library, for example <code>agdaPackages.agda-categories</code>, or <code>agdaPackages.generic</code>.</p> <p>In some cases it is useful to build all Agda packages. This can be done with the following Github comment:</p> <pre><code>@ofborg build agda.passthru.tests.allPackages\n</code></pre> <p>Sometimes, the builds of the reverse dependencies fail because they have not yet been updated and released. You should drop the maintainers a quick issue notifying them of the breakage, citing the build error (which you can get from the ofborg logs). If you are motivated, you might even send a pull request that fixes it. Usually, the maintainers will answer within a week or two with a new release. Bumping the version of that reverse dependency should be a further commit on your PR.</p> <p>In the rare case that a new release is not to be expected within an acceptable time, mark the broken package as broken by setting <code>meta.broken = true;</code>. This will exclude it from the build test. It can be added later when it is fixed, and does not hinder the advancement of the whole package set in the meantime.</p>"},{"location":"languages-frameworks/android.section.html","title":"Android","text":"<p>The Android build environment provides three major features and a number of supporting features.</p>"},{"location":"languages-frameworks/android.section.html#deploying-an-android-sdk-installation-with-plugins","title":"Deploying an Android SDK installation with plugins","text":"<p>The first use case is deploying the SDK with a desired set of plugins or subsets of an SDK.</p> <pre><code>with import &lt;nixpkgs&gt; {};\n\nlet\n  androidComposition = androidenv.composeAndroidPackages {\n    cmdLineToolsVersion = \"8.0\";\n    toolsVersion = \"26.1.1\";\n    platformToolsVersion = \"30.0.5\";\n    buildToolsVersions = [ \"30.0.3\" ];\n    includeEmulator = false;\n    emulatorVersion = \"30.3.4\";\n    platformVersions = [ \"28\" \"29\" \"30\" ];\n    includeSources = false;\n    includeSystemImages = false;\n    systemImageTypes = [ \"google_apis_playstore\" ];\n    abiVersions = [ \"armeabi-v7a\" \"arm64-v8a\" ];\n    cmakeVersions = [ \"3.10.2\" ];\n    includeNDK = true;\n    ndkVersions = [\"22.0.7026061\"];\n    useGoogleAPIs = false;\n    useGoogleTVAddOns = false;\n    includeExtras = [\n      \"extras;google;gcm\"\n    ];\n  };\nin\nandroidComposition.androidsdk\n</code></pre> <p>The above function invocation states that we want an Android SDK with the above specified plugin versions. By default, most plugins are disabled. Notable exceptions are the tools, platform-tools and build-tools sub packages.</p> <p>The following parameters are supported:</p> <ul> <li><code>cmdLineToolsVersion</code>, specifies the version of the <code>cmdline-tools</code> package to use</li> <li><code>toolsVersion</code>, specifies the version of the <code>tools</code> package. Notice <code>tools</code> is   obsolete, and currently only <code>26.1.1</code> is available, so there's not a lot of   options here, however, you can set it as <code>null</code> if you don't want it.</li> <li><code>platformsToolsVersion</code> specifies the version of the <code>platform-tools</code> plugin</li> <li><code>buildToolsVersions</code> specifies the versions of the <code>build-tools</code> plugins to   use.</li> <li><code>includeEmulator</code> specifies whether to deploy the emulator package (<code>false</code>   by default). When enabled, the version of the emulator to deploy can be   specified by setting the <code>emulatorVersion</code> parameter.</li> <li><code>cmakeVersions</code> specifies which CMake versions should be deployed.</li> <li><code>includeNDK</code> specifies that the Android NDK bundle should be included.   Defaults to: <code>false</code>.</li> <li><code>ndkVersions</code> specifies the NDK versions that we want to use. These are linked   under the <code>ndk</code> directory of the SDK root, and the first is linked under the   <code>ndk-bundle</code> directory.</li> <li><code>ndkVersion</code> is equivalent to specifying one entry in <code>ndkVersions</code>, and   <code>ndkVersions</code> overrides this parameter if provided.</li> <li><code>includeExtras</code> is an array of identifier strings referring to arbitrary   add-on packages that should be installed.</li> <li><code>platformVersions</code> specifies which platform SDK versions should be included.</li> </ul> <p>For each platform version that has been specified, we can apply the following options:</p> <ul> <li><code>includeSystemImages</code> specifies whether a system image for each platform SDK   should be included.</li> <li><code>includeSources</code> specifies whether the sources for each SDK version should be   included.</li> <li><code>useGoogleAPIs</code> specifies that for each selected platform version the   Google API should be included.</li> <li><code>useGoogleTVAddOns</code> specifies that for each selected platform version the   Google TV add-on should be included.</li> </ul> <p>For each requested system image we can specify the following options:</p> <ul> <li><code>systemImageTypes</code> specifies what kind of system images should be included.   Defaults to: <code>default</code>.</li> <li><code>abiVersions</code> specifies what kind of ABI version of each system image should   be included. Defaults to: <code>armeabi-v7a</code>.</li> </ul> <p>Most of the function arguments have reasonable default settings.</p> <p>You can specify license names:</p> <ul> <li><code>extraLicenses</code> is a list of license names.   You can get these names from repo.json or <code>querypackages.sh licenses</code>. The SDK   license (<code>android-sdk-license</code>) is accepted for you if you set accept_license   to true. If you are doing something like working with preview SDKs, you will   want to add <code>android-sdk-preview-license</code> or whichever license applies here.</li> </ul> <p>Additionally, you can override the repositories that composeAndroidPackages will pull from:</p> <ul> <li><code>repoJson</code> specifies a path to a generated repo.json file. You can generate this   by running <code>generate.sh</code>, which in turn will call into <code>mkrepo.rb</code>.</li> <li><code>repoXmls</code> is an attribute set containing paths to repo XML files. If specified,   it takes priority over <code>repoJson</code>, and will trigger a local build writing out a   repo.json to the Nix store based on the given repository XMLs.</li> </ul> <pre><code>repoXmls = {\n  packages = [ ./xml/repository2-1.xml ];\n  images = [\n    ./xml/android-sys-img2-1.xml\n    ./xml/android-tv-sys-img2-1.xml\n    ./xml/android-wear-sys-img2-1.xml\n    ./xml/android-wear-cn-sys-img2-1.xml\n    ./xml/google_apis-sys-img2-1.xml\n    ./xml/google_apis_playstore-sys-img2-1.xml\n  ];\n  addons = [ ./xml/addon2-1.xml ];\n};\n</code></pre> <p>When building the above expression with:</p> <pre><code>$ nix-build\n</code></pre> <p>The Android SDK gets deployed with all desired plugin versions.</p> <p>We can also deploy subsets of the Android SDK. For example, to only the <code>platform-tools</code> package, you can evaluate the following expression:</p> <pre><code>with import &lt;nixpkgs&gt; {};\n\nlet\n  androidComposition = androidenv.composeAndroidPackages {\n    # ...\n  };\nin\nandroidComposition.platform-tools\n</code></pre>"},{"location":"languages-frameworks/android.section.html#using-predefined-android-package-compositions","title":"Using predefined Android package compositions","text":"<p>In addition to composing an Android package set manually, it is also possible to use a predefined composition that contains all basic packages for a specific Android version, such as version 9.0 (API-level 28).</p> <p>The following Nix expression can be used to deploy the entire SDK with all basic plugins:</p> <pre><code>with import &lt;nixpkgs&gt; {};\n\nandroidenv.androidPkgs_9_0.androidsdk\n</code></pre> <p>It is also possible to use one plugin only:</p> <pre><code>with import &lt;nixpkgs&gt; {};\n\nandroidenv.androidPkgs_9_0.platform-tools\n</code></pre>"},{"location":"languages-frameworks/android.section.html#building-an-android-application","title":"Building an Android application","text":"<p>In addition to the SDK, it is also possible to build an Ant-based Android project and automatically deploy all the Android plugins that a project requires.</p> <pre><code>with import &lt;nixpkgs&gt; {};\n\nandroidenv.buildApp {\n  name = \"MyAndroidApp\";\n  src = ./myappsources;\n  release = true;\n\n  # If release is set to true, you need to specify the following parameters\n  keyStore = ./keystore;\n  keyAlias = \"myfirstapp\";\n  keyStorePassword = \"mykeystore\";\n  keyAliasPassword = \"myfirstapp\";\n\n  # Any Android SDK parameters that install all the relevant plugins that a\n  # build requires\n  platformVersions = [ \"24\" ];\n\n  # When we include the NDK, then ndk-build is invoked before Ant gets invoked\n  includeNDK = true;\n}\n</code></pre> <p>Aside from the app-specific build parameters (<code>name</code>, <code>src</code>, <code>release</code> and keystore parameters), the <code>buildApp {}</code> function supports all the function parameters that the SDK composition function (the function shown in the previous section) supports.</p> <p>This build function is particularly useful when it is desired to use Hydra: the Nix-based continuous integration solution to build Android apps. An Android APK gets exposed as a build product and can be installed on any Android device with a web browser by navigating to the build result page.</p>"},{"location":"languages-frameworks/android.section.html#spawning-emulator-instances","title":"Spawning emulator instances","text":"<p>For testing purposes, it can also be quite convenient to automatically generate scripts that spawn emulator instances with all desired configuration settings.</p> <p>An emulator spawn script can be configured by invoking the <code>emulateApp {}</code> function:</p> <pre><code>with import &lt;nixpkgs&gt; {};\n\nandroidenv.emulateApp {\n  name = \"emulate-MyAndroidApp\";\n  platformVersion = \"28\";\n  abiVersion = \"x86\"; # armeabi-v7a, mips, x86_64\n  systemImageType = \"google_apis_playstore\";\n}\n</code></pre> <p>Additional flags may be applied to the Android SDK's emulator through the runtime environment variable <code>$NIX_ANDROID_EMULATOR_FLAGS</code>.</p> <p>It is also possible to specify an APK to deploy inside the emulator and the package and activity names to launch it:</p> <pre><code>with import &lt;nixpkgs&gt; {};\n\nandroidenv.emulateApp {\n  name = \"emulate-MyAndroidApp\";\n  platformVersion = \"24\";\n  abiVersion = \"armeabi-v7a\"; # mips, x86, x86_64\n  systemImageType = \"default\";\n  app = ./MyApp.apk;\n  package = \"MyApp\";\n  activity = \"MainActivity\";\n}\n</code></pre> <p>In addition to prebuilt APKs, you can also bind the APK parameter to a <code>buildApp {}</code> function invocation shown in the previous example.</p>"},{"location":"languages-frameworks/android.section.html#notes-on-environment-variables-in-android-projects","title":"Notes on environment variables in Android projects","text":"<ul> <li><code>ANDROID_SDK_ROOT</code> should point to the Android SDK. In your Nix expressions, this should be   <code>${androidComposition.androidsdk}/libexec/android-sdk</code>. Note that <code>ANDROID_HOME</code> is deprecated,   but if you rely on tools that need it, you can export it too.</li> <li><code>ANDROID_NDK_ROOT</code> should point to the Android NDK, if you're doing NDK development.   In your Nix expressions, this should be <code>${ANDROID_SDK_ROOT}/ndk-bundle</code>.</li> </ul> <p>If you are running the Android Gradle plugin, you need to export GRADLE_OPTS to override aapt2 to point to the aapt2 binary in the Nix store as well, or use a FHS environment so the packaged aapt2 can run. If you don't want to use a FHS environment, something like this should work:</p> <pre><code>let\n  buildToolsVersion = \"30.0.3\";\n\n  # Use buildToolsVersion when you define androidComposition\n  androidComposition = &lt;...&gt;;\nin\npkgs.mkShell rec {\n  ANDROID_SDK_ROOT = \"${androidComposition.androidsdk}/libexec/android-sdk\";\n  ANDROID_NDK_ROOT = \"${ANDROID_SDK_ROOT}/ndk-bundle\";\n\n  # Use the same buildToolsVersion here\n  GRADLE_OPTS = \"-Dorg.gradle.project.android.aapt2FromMavenOverride=${ANDROID_SDK_ROOT}/build-tools/${buildToolsVersion}/aapt2\";\n}\n</code></pre> <p>If you are using cmake, you need to add it to PATH in a shell hook or FHS env profile. The path is suffixed with a build number, but properly prefixed with the version. So, something like this should suffice:</p> <pre><code>let\n  cmakeVersion = \"3.10.2\";\n\n  # Use cmakeVersion when you define androidComposition\n  androidComposition = &lt;...&gt;;\nin\npkgs.mkShell rec {\n  ANDROID_SDK_ROOT = \"${androidComposition.androidsdk}/libexec/android-sdk\";\n  ANDROID_NDK_ROOT = \"${ANDROID_SDK_ROOT}/ndk-bundle\";\n\n  # Use the same cmakeVersion here\n  shellHook = ''\n    export PATH=\"$(echo \"$ANDROID_SDK_ROOT/cmake/${cmakeVersion}\".*/bin):$PATH\"\n  '';\n}\n</code></pre> <p>Note that running Android Studio with ANDROID_SDK_ROOT set will automatically write a <code>local.properties</code> file with <code>sdk.dir</code> set to $ANDROID_SDK_ROOT if one does not already exist. If you are using the NDK as well, you may have to add <code>ndk.dir</code> to this file.</p> <p>An example shell.nix that does all this for you is provided in examples/shell.nix. This shell.nix includes a shell hook that overwrites local.properties with the correct sdk.dir and ndk.dir values. This will ensure that the SDK and NDK directories will both be correct when you run Android Studio inside nix-shell.</p>"},{"location":"languages-frameworks/android.section.html#notes-on-improving-build.gradle-compatibility","title":"Notes on improving build.gradle compatibility","text":"<p>Ensure that your buildToolsVersion and ndkVersion match what is declared in androidenv. If you are using cmake, make sure its declared version is correct too.</p> <p>Otherwise, you may get cryptic errors from aapt2 and the Android Gradle plugin warning that it cannot install the build tools because the SDK directory is not writeable.</p> <pre><code>android {\n    buildToolsVersion \"30.0.3\"\n    ndkVersion = \"22.0.7026061\"\n    externalNativeBuild {\n        cmake {\n            version \"3.10.2\"\n        }\n    }\n}\n</code></pre>"},{"location":"languages-frameworks/android.section.html#querying-the-available-versions-of-each-plugin","title":"Querying the available versions of each plugin","text":"<p>repo.json provides all the options in one file now.</p> <p>A shell script in the <code>pkgs/development/mobile/androidenv/</code> subdirectory can be used to retrieve all possible options:</p> <pre><code>./querypackages.sh packages\n</code></pre> <p>The above command-line instruction queries all package versions in repo.json.</p>"},{"location":"languages-frameworks/android.section.html#updating-the-generated-expressions","title":"Updating the generated expressions","text":"<p>repo.json is generated from XML files that the Android Studio package manager uses. To update the expressions run the <code>generate.sh</code> script that is stored in the <code>pkgs/development/mobile/androidenv/</code> subdirectory:</p> <pre><code>./generate.sh\n</code></pre>"},{"location":"languages-frameworks/beam.section.html","title":"BEAM Languages (Erlang, Elixir &amp; LFE)","text":""},{"location":"languages-frameworks/beam.section.html#beam-introduction","title":"Introduction","text":"<p>In this document and related Nix expressions, we use the term, BEAM, to describe the environment. BEAM is the name of the Erlang Virtual Machine and, as far as we're concerned, from a packaging perspective, all languages that run on the BEAM are interchangeable. That which varies, like the build system, is transparent to users of any given BEAM package, so we make no distinction.</p>"},{"location":"languages-frameworks/beam.section.html#available-versions-and-deprecations-schedule","title":"Available versions and deprecations schedule","text":""},{"location":"languages-frameworks/beam.section.html#elixir","title":"Elixir","text":"<p>nixpkgs follows the official elixir deprecation schedule and keeps the last 5 released versions of Elixir available.</p>"},{"location":"languages-frameworks/beam.section.html#beam-structure","title":"Structure","text":"<p>All BEAM-related expressions are available via the top-level <code>beam</code> attribute, which includes:</p> <ul> <li> <p><code>interpreters</code>: a set of compilers running on the BEAM, including multiple Erlang/OTP versions (<code>beam.interpreters.erlang_22</code>, etc), Elixir (<code>beam.interpreters.elixir</code>) and LFE (Lisp Flavoured Erlang) (<code>beam.interpreters.lfe</code>).</p> </li> <li> <p><code>packages</code>: a set of package builders (Mix and rebar3), each compiled with a specific Erlang/OTP version, e.g. <code>beam.packages.erlang22</code>.</p> </li> </ul> <p>The default Erlang compiler, defined by <code>beam.interpreters.erlang</code>, is aliased as <code>erlang</code>. The default BEAM package set is defined by <code>beam.packages.erlang</code> and aliased at the top level as <code>beamPackages</code>.</p> <p>To create a package builder built with a custom Erlang version, use the lambda, <code>beam.packagesWith</code>, which accepts an Erlang/OTP derivation and produces a package builder similar to <code>beam.packages.erlang</code>.</p> <p>Many Erlang/OTP distributions available in <code>beam.interpreters</code> have versions with ODBC and/or Java enabled or without wx (no observer support). For example, there's <code>beam.interpreters.erlang_22_odbc_javac</code>, which corresponds to <code>beam.interpreters.erlang_22</code> and <code>beam.interpreters.erlang_22_nox</code>, which corresponds to <code>beam.interpreters.erlang_22</code>.</p>"},{"location":"languages-frameworks/beam.section.html#build-tools","title":"Build Tools","text":""},{"location":"languages-frameworks/beam.section.html#build-tools-rebar3","title":"Rebar3","text":"<p>We provide a version of Rebar3, under <code>rebar3</code>. We also provide a helper to fetch Rebar3 dependencies from a lockfile under <code>fetchRebar3Deps</code>.</p> <p>We also provide a version on Rebar3 with plugins included, under <code>rebar3WithPlugins</code>. This package is a function which takes two arguments: <code>plugins</code>, a list of nix derivations to include as plugins (loaded only when specified in <code>rebar.config</code>), and <code>globalPlugins</code>, which should always be loaded by rebar3. Example: <code>rebar3WithPlugins { globalPlugins = [beamPackages.pc]; }</code>.</p> <p>When adding a new plugin it is important that the <code>packageName</code> attribute is the same as the atom used by rebar3 to refer to the plugin.</p>"},{"location":"languages-frameworks/beam.section.html#build-tools-other","title":"Mix &amp; Erlang.mk","text":"<p>Erlang.mk works exactly as expected. There is a bootstrap process that needs to be run, which is supported by the <code>buildErlangMk</code> derivation.</p> <p>For Elixir applications use <code>mixRelease</code> to make a release. See examples for more details.</p> <p>There is also a <code>buildMix</code> helper, whose behavior is closer to that of <code>buildErlangMk</code> and <code>buildRebar3</code>. The primary difference is that mixRelease makes a release, while buildMix only builds the package, making it useful for libraries and other dependencies.</p>"},{"location":"languages-frameworks/beam.section.html#how-to-install-beam-packages","title":"How to Install BEAM Packages","text":"<p>BEAM builders are not registered at the top level, because they are not relevant to the vast majority of Nix users. To use any of those builders into your environment, refer to them by their attribute path under <code>beamPackages</code>, e.g. <code>beamPackages.rebar3</code>:</p> <p>::: {.example #ex-beam-ephemeral-shell}</p>"},{"location":"languages-frameworks/beam.section.html#ephemeral-shell","title":"Ephemeral shell","text":"<p><pre><code>$ nix-shell -p beamPackages.rebar3\n</code></pre> :::</p> <p>::: {.example #ex-beam-declarative-shell}</p>"},{"location":"languages-frameworks/beam.section.html#declarative-shell","title":"Declarative shell","text":"<p><pre><code>let\n  pkgs = import &lt;nixpkgs&gt; { config = {}; overlays = []; };\nin\npkgs.mkShell {\n  packages = [ pkgs.beamPackages.rebar3 ];\n}\n</code></pre> :::</p>"},{"location":"languages-frameworks/beam.section.html#packaging-beam-applications","title":"Packaging BEAM Applications","text":""},{"location":"languages-frameworks/beam.section.html#packaging-erlang-applications","title":"Erlang Applications","text":""},{"location":"languages-frameworks/beam.section.html#rebar3-packages","title":"Rebar3 Packages","text":"<p>The Nix function, <code>buildRebar3</code>, defined in <code>beam.packages.erlang.buildRebar3</code> and aliased at the top level, can be used to build a derivation that understands how to build a Rebar3 project.</p> <p>If a package needs to compile native code via Rebar3's port compilation mechanism, add <code>compilePort = true;</code> to the derivation.</p>"},{"location":"languages-frameworks/beam.section.html#erlang-mk-packages","title":"Erlang.mk Packages","text":"<p>Erlang.mk functions similarly to Rebar3, except we use <code>buildErlangMk</code> instead of <code>buildRebar3</code>.</p>"},{"location":"languages-frameworks/beam.section.html#mix-packages","title":"Mix Packages","text":"<p><code>mixRelease</code> is used to make a release in the mix sense. Dependencies will need to be fetched with <code>fetchMixDeps</code> and passed to it.</p>"},{"location":"languages-frameworks/beam.section.html#mix-release-elixir-phoenix-example","title":"mixRelease - Elixir Phoenix example","text":"<p>there are 3 steps, frontend dependencies (javascript), backend dependencies (elixir) and the final derivation that puts both of those together</p>"},{"location":"languages-frameworks/beam.section.html#mix-release-javascript-deps","title":"mixRelease - Frontend dependencies (javascript)","text":"<p>For phoenix projects, inside of nixpkgs you can either use yarn2nix (mkYarnModule) or node2nix. An example with yarn2nix can be found here. An example with node2nix will follow. To package something outside of nixpkgs, you have alternatives like npmlock2nix or nix-npm-buildpackage</p>"},{"location":"languages-frameworks/beam.section.html#mix-release-mix-deps","title":"mixRelease - backend dependencies (mix)","text":"<p>There are 2 ways to package backend dependencies. With mix2nix and with a fixed-output-derivation (FOD).</p>"},{"location":"languages-frameworks/beam.section.html#mix2nix","title":"mix2nix","text":"<p><code>mix2nix</code> is a cli tool available in nixpkgs. it will generate a nix expression from a mix.lock file. It is quite standard in the 2nix tool series.</p> <p>Note that currently mix2nix can't handle git dependencies inside the mix.lock file. If you have git dependencies, you can either add them manually (see example) or use the FOD method.</p> <p>The advantage of using mix2nix is that nix will know your whole dependency graph. On a dependency update, this won't trigger a full rebuild and download of all the dependencies, where FOD will do so.</p> <p>Practical steps:</p> <ul> <li>run <code>mix2nix &gt; mix_deps.nix</code> in the upstream repo.</li> <li>pass <code>mixNixDeps = with pkgs; import ./mix_deps.nix { inherit lib beamPackages; };</code> as an argument to mixRelease.</li> </ul> <p>If there are git dependencies.</p> <ul> <li>You'll need to fix the version artificially in mix.exs and regenerate the mix.lock with fixed version (on upstream). This will enable you to run <code>mix2nix &gt; mix_deps.nix</code>.</li> <li>From the mix_deps.nix file, remove the dependencies that had git versions and pass them as an override to the import function.</li> </ul> <pre><code>  mixNixDeps = import ./mix.nix {\n    inherit beamPackages lib;\n    overrides = (final: prev: {\n      # mix2nix does not support git dependencies yet,\n      # so we need to add them manually\n      prometheus_ex = beamPackages.buildMix rec {\n        name = \"prometheus_ex\";\n        version = \"3.0.5\";\n\n        # Change the argument src with the git src that you actually need\n        src = fetchFromGitLab {\n          domain = \"git.pleroma.social\";\n          group = \"pleroma\";\n          owner = \"elixir-libraries\";\n          repo = \"prometheus.ex\";\n          rev = \"a4e9beb3c1c479d14b352fd9d6dd7b1f6d7deee5\";\n          hash = \"sha256-U17LlN6aGUKUFnT4XyYXppRN+TvUBIBRHEUsfeIiGOw=\";\n        };\n        # you can re-use the same beamDeps argument as generated\n        beamDeps = with final; [ prometheus ];\n      };\n  });\n};\n</code></pre> <p>You will need to run the build process once to fix the hash to correspond to your new git src.</p>"},{"location":"languages-frameworks/beam.section.html#fixed-output-derivation","title":"FOD","text":"<p>A fixed output derivation will download mix dependencies from the internet. To ensure reproducibility, a hash will be supplied. Note that mix is relatively reproducible. An FOD generating a different hash on each run hasn't been observed (as opposed to npm where the chances are relatively high). See elixir-ls for a usage example of FOD.</p> <p>Practical steps</p> <ul> <li>start with the following argument to mixRelease</li> </ul> <pre><code>  mixFodDeps = fetchMixDeps {\n    pname = \"mix-deps-${pname}\";\n    inherit src version;\n    hash = lib.fakeHash;\n  };\n</code></pre> <p>The first build will complain about the hash value, you can replace with the suggested value after that.</p> <p>Note that if after you've replaced the value, nix suggests another hash, then mix is not fetching the dependencies reproducibly. An FOD will not work in that case and you will have to use mix2nix.</p>"},{"location":"languages-frameworks/beam.section.html#mix-release-example","title":"mixRelease - example","text":"<p>Here is how your <code>default.nix</code> file would look for a phoenix project.</p> <pre><code>with import &lt;nixpkgs&gt; { };\n\nlet\n  # beam.interpreters.erlang_26 is available if you need a particular version\n  packages = beam.packagesWith beam.interpreters.erlang;\n\n  pname = \"your_project\";\n  version = \"0.0.1\";\n\n  src = builtins.fetchgit {\n    url = \"ssh://git@github.com/your_id/your_repo\";\n    rev = \"replace_with_your_commit\";\n  };\n\n  # if using mix2nix you can use the mixNixDeps attribute\n  mixFodDeps = packages.fetchMixDeps {\n    pname = \"mix-deps-${pname}\";\n    inherit src version;\n    # nix will complain and tell you the right value to replace this with\n    hash = lib.fakeHash;\n    mixEnv = \"\"; # default is \"prod\", when empty includes all dependencies, such as \"dev\", \"test\".\n    # if you have build time environment variables add them here\n    MY_ENV_VAR=\"my_value\";\n  };\n\n  nodeDependencies = (pkgs.callPackage ./assets/default.nix { }).shell.nodeDependencies;\n\nin packages.mixRelease {\n  inherit src pname version mixFodDeps;\n  # if you have build time environment variables add them here\n  MY_ENV_VAR=\"my_value\";\n\n  postBuild = ''\n    ln -sf ${nodeDependencies}/lib/node_modules assets/node_modules\n    npm run deploy --prefix ./assets\n\n    # for external task you need a workaround for the no deps check flag\n    # https://github.com/phoenixframework/phoenix/issues/2690\n    mix do deps.loadpaths --no-deps-check, phx.digest\n    mix phx.digest --no-deps-check\n  '';\n}\n</code></pre> <p>Setup will require the following steps:</p> <ul> <li>Move your secrets to runtime environment variables. For more information refer to the runtime.exs docs. On a fresh Phoenix build that would mean that both <code>DATABASE_URL</code> and <code>SECRET_KEY</code> need to be moved to <code>runtime.exs</code>.</li> <li><code>cd assets</code> and <code>nix-shell -p node2nix --run \"node2nix --development\"</code> will generate a Nix expression containing your frontend dependencies</li> <li>commit and push those changes</li> <li>you can now <code>nix-build .</code></li> <li>To run the release, set the <code>RELEASE_TMP</code> environment variable to a directory that your program has write access to. It will be used to store the BEAM settings.</li> </ul>"},{"location":"languages-frameworks/beam.section.html#example-of-creating-a-service-for-an-elixir---phoenix-project","title":"Example of creating a service for an Elixir - Phoenix project","text":"<p>In order to create a service with your release, you could add a <code>service.nix</code> in your project with the following</p> <pre><code>{config, pkgs, lib, ...}:\n\nlet\n  release = pkgs.callPackage ./default.nix;\n  release_name = \"app\";\n  working_directory = \"/home/app\";\nin\n{\n  systemd.services.${release_name} = {\n    wantedBy = [ \"multi-user.target\" ];\n    after = [ \"network.target\" \"postgresql.service\" ];\n    # note that if you are connecting to a postgres instance on a different host\n    # postgresql.service should not be included in the requires.\n    requires = [ \"network-online.target\" \"postgresql.service\" ];\n    description = \"my app\";\n    environment = {\n      # RELEASE_TMP is used to write the state of the\n      # VM configuration when the system is running\n      # it needs to be a writable directory\n      RELEASE_TMP = working_directory;\n      # can be generated in an elixir console with\n      # Base.encode32(:crypto.strong_rand_bytes(32))\n      RELEASE_COOKIE = \"my_cookie\";\n      MY_VAR = \"my_var\";\n    };\n    serviceConfig = {\n      Type = \"exec\";\n      DynamicUser = true;\n      WorkingDirectory = working_directory;\n      # Implied by DynamicUser, but just to emphasize due to RELEASE_TMP\n      PrivateTmp = true;\n      ExecStart = ''\n        ${release}/bin/${release_name} start\n      '';\n      ExecStop = ''\n        ${release}/bin/${release_name} stop\n      '';\n      ExecReload = ''\n        ${release}/bin/${release_name} restart\n      '';\n      Restart = \"on-failure\";\n      RestartSec = 5;\n      StartLimitBurst = 3;\n      StartLimitInterval = 10;\n    };\n    # disksup requires bash\n    path = [ pkgs.bash ];\n  };\n\n  # in case you have migration scripts or you want to use a remote shell\n  environment.systemPackages = [ release ];\n}\n</code></pre>"},{"location":"languages-frameworks/beam.section.html#how-to-develop","title":"How to Develop","text":""},{"location":"languages-frameworks/beam.section.html#creating-a-shell","title":"Creating a Shell","text":"<p>Usually, we need to create a <code>shell.nix</code> file and do our development inside of the environment specified therein. Just install your version of Erlang and any other interpreters, and then use your normal build tools. As an example with Elixir:</p> <pre><code>{ pkgs ? import &lt;nixpkgs&gt; {} }:\n\nwith pkgs;\nlet\n  elixir = beam.packages.erlang_24.elixir_1_12;\nin\nmkShell {\n  buildInputs = [ elixir ];\n}\n</code></pre>"},{"location":"languages-frameworks/beam.section.html#beam-using-overlays","title":"Using an overlay","text":"<p>If you need to use an overlay to change some attributes of a derivation, e.g. if you need a bugfix from a version that is not yet available in nixpkgs, you can override attributes such as <code>version</code> (and the corresponding <code>hash</code>) and then use this overlay in your development environment:</p>"},{"location":"languages-frameworks/beam.section.html#beam-using-overlays-shell.nix","title":"<code>shell.nix</code>","text":"<pre><code>let\n  elixir_1_13_1_overlay = (self: super: {\n      elixir_1_13 = super.elixir_1_13.override {\n        version = \"1.13.1\";\n        sha256 = \"sha256-t0ic1LcC7EV3avWGdR7VbyX7pGDpnJSW1ZvwvQUPC3w=\";\n      };\n    });\n  pkgs = import &lt;nixpkgs&gt; { overlays = [ elixir_1_13_1_overlay ]; };\nin\nwith pkgs;\nmkShell {\n  buildInputs = [\n    elixir_1_13\n  ];\n}\n</code></pre>"},{"location":"languages-frameworks/beam.section.html#elixir---phoenix-project","title":"Elixir - Phoenix project","text":"<p>Here is an example <code>shell.nix</code>.</p> <pre><code>with import &lt;nixpkgs&gt; { };\n\nlet\n  # define packages to install\n  basePackages = [\n    git\n    # replace with beam.packages.erlang.elixir_1_13 if you need\n    beam.packages.erlang.elixir\n    nodejs\n    postgresql_14\n    # only used for frontend dependencies\n    # you are free to use yarn2nix as well\n    nodePackages.node2nix\n    # formatting js file\n    nodePackages.prettier\n  ];\n\n  inputs = basePackages ++ lib.optionals stdenv.isLinux [ inotify-tools ]\n    ++ lib.optionals stdenv.isDarwin\n    (with darwin.apple_sdk.frameworks; [ CoreFoundation CoreServices ]);\n\n  # define shell startup command\n  hooks = ''\n    # this allows mix to work on the local directory\n    mkdir -p .nix-mix .nix-hex\n    export MIX_HOME=$PWD/.nix-mix\n    export HEX_HOME=$PWD/.nix-mix\n    # make hex from Nixpkgs available\n    # `mix local.hex` will install hex into MIX_HOME and should take precedence\n    export MIX_PATH=\"${beam.packages.erlang.hex}/lib/erlang/lib/hex/ebin\"\n    export PATH=$MIX_HOME/bin:$HEX_HOME/bin:$PATH\n    export LANG=C.UTF-8\n    # keep your shell history in iex\n    export ERL_AFLAGS=\"-kernel shell_history enabled\"\n\n    # postges related\n    # keep all your db data in a folder inside the project\n    export PGDATA=\"$PWD/db\"\n\n    # phoenix related env vars\n    export POOL_SIZE=15\n    export DB_URL=\"postgresql://postgres:postgres@localhost:5432/db\"\n    export PORT=4000\n    export MIX_ENV=dev\n    # add your project env vars here, word readable in the nix store.\n    export ENV_VAR=\"your_env_var\"\n  '';\n\nin mkShell {\n  buildInputs = inputs;\n  shellHook = hooks;\n}\n</code></pre> <p>Initializing the project will require the following steps:</p> <ul> <li>create the db directory <code>initdb ./db</code> (inside your mix project folder)</li> <li>create the postgres user <code>createuser postgres -ds</code></li> <li>create the db <code>createdb db</code></li> <li>start the postgres instance <code>pg_ctl -l \"$PGDATA/server.log\" start</code></li> <li>add the <code>/db</code> folder to your <code>.gitignore</code></li> <li>you can start your phoenix server and get a shell with <code>iex -S mix phx.server</code></li> </ul>"},{"location":"languages-frameworks/bower.section.html","title":"Bower","text":"<p>Bower is a package manager for web site front-end components. Bower packages (comprising of build artifacts and sometimes sources) are stored in <code>git</code> repositories, typically on Github. The package registry is run by the Bower team with package metadata coming from the <code>bower.json</code> file within each package.</p> <p>The end result of running Bower is a <code>bower_components</code> directory which can be included in the web app's build process.</p> <p>Bower can be run interactively, by installing <code>nodePackages.bower</code>. More interestingly, the Bower components can be declared in a Nix derivation, with the help of <code>nodePackages.bower2nix</code>.</p>"},{"location":"languages-frameworks/bower.section.html#ssec-bower2nix-usage","title":"bower2nix usage","text":"<p>Suppose you have a <code>bower.json</code> with the following contents:</p>"},{"location":"languages-frameworks/bower.section.html#ex-bowerJson","title":"Example bower.json","text":"<pre><code>  \"name\": \"my-web-app\",\n  \"dependencies\": {\n    \"angular\": \"~1.5.0\",\n    \"bootstrap\": \"~3.3.6\"\n  }\n</code></pre> <p>Running <code>bower2nix</code> will produce something like the following output:</p> <pre><code>{ fetchbower, buildEnv }:\nbuildEnv { name = \"bower-env\"; ignoreCollisions = true; paths = [\n  (fetchbower \"angular\" \"1.5.3\" \"~1.5.0\" \"1749xb0firxdra4rzadm4q9x90v6pzkbd7xmcyjk6qfza09ykk9y\")\n  (fetchbower \"bootstrap\" \"3.3.6\" \"~3.3.6\" \"1vvqlpbfcy0k5pncfjaiskj3y6scwifxygfqnw393sjfxiviwmbv\")\n  (fetchbower \"jquery\" \"2.2.2\" \"1.9.1 - 2\" \"10sp5h98sqwk90y4k6hbdviwqzvzwqf47r3r51pakch5ii2y7js1\")\n];\n</code></pre> <p>Using the <code>bower2nix</code> command line arguments, the output can be redirected to a file. A name like <code>bower-packages.nix</code> would be fine.</p> <p>The resulting derivation is a union of all the downloaded Bower packages (and their dependencies). To use it, they still need to be linked together by Bower, which is where <code>buildBowerComponents</code> is useful.</p>"},{"location":"languages-frameworks/bower.section.html#ssec-build-bower-components","title":"buildBowerComponents function","text":"<p>The function is implemented in pkgs/development/bower-modules/generic/default.nix.</p>"},{"location":"languages-frameworks/bower.section.html#ex-buildBowerComponents","title":"Example buildBowerComponents","text":"<pre><code>bowerComponents = buildBowerComponents {\n  name = \"my-web-app\";\n  generated = ./bower-packages.nix; # note 1\n  src = myWebApp; # note 2\n};\n</code></pre> <p>In \"buildBowerComponents\" example the following arguments are of special significance to the function:</p> <ol> <li><code>generated</code> specifies the file which was created by {command}<code>bower2nix</code>.</li> <li><code>src</code> is your project's sources. It needs to contain a {file}<code>bower.json</code> file.</li> </ol> <p><code>buildBowerComponents</code> will run Bower to link together the output of <code>bower2nix</code>, resulting in a <code>bower_components</code> directory which can be used.</p> <p>Here is an example of a web frontend build process using <code>gulp</code>. You might use <code>grunt</code>, or anything else.</p>"},{"location":"languages-frameworks/bower.section.html#ex-bowerGulpFile","title":"Example build script (gulpfile.js)","text":"<pre><code>var gulp = require('gulp');\n\ngulp.task('default', [], function () {\n  gulp.start('build');\n});\n\ngulp.task('build', [], function () {\n  console.log(\"Just a dummy gulp build\");\n  gulp\n    .src([\"./bower_components/**/*\"])\n    .pipe(gulp.dest(\"./gulpdist/\"));\n});\n</code></pre>"},{"location":"languages-frameworks/bower.section.html#ex-buildBowerComponentsDefaultNix","title":"Example Full example \u2014 default.nix","text":"<pre><code>{ myWebApp ? { outPath = ./.; name = \"myWebApp\"; }\n, pkgs ? import &lt;nixpkgs&gt; {}\n}:\n\npkgs.stdenv.mkDerivation {\n  name = \"my-web-app-frontend\";\n  src = myWebApp;\n\n  buildInputs = [ pkgs.nodePackages.gulp ];\n\n  bowerComponents = pkgs.buildBowerComponents { # note 1\n    name = \"my-web-app\";\n    generated = ./bower-packages.nix;\n    src = myWebApp;\n  };\n\n  buildPhase = ''\n    cp --reflink=auto --no-preserve=mode -R $bowerComponents/bower_components . # note 2\n    export HOME=$PWD # note 3\n    ${pkgs.nodePackages.gulp}/bin/gulp build # note 4\n  '';\n\n  installPhase = \"mv gulpdist $out\";\n}\n</code></pre> <p>A few notes about Full example \u2014 <code>default.nix</code>:</p> <ol> <li>The result of <code>buildBowerComponents</code> is an input to the frontend build.</li> <li>Whether to symlink or copy the {file}<code>bower_components</code> directory depends on the build tool in use.    In this case a copy is used to avoid {command}<code>gulp</code> silliness with permissions.</li> <li>{command}<code>gulp</code> requires <code>HOME</code> to refer to a writeable directory.</li> <li>The actual build command in this example is {command}<code>gulp</code>. Other tools could be used instead.</li> </ol>"},{"location":"languages-frameworks/bower.section.html#ssec-bower2nix-troubleshooting","title":"Troubleshooting","text":""},{"location":"languages-frameworks/bower.section.html#enocache-errors-from-buildbowercomponents","title":"ENOCACHE errors from buildBowerComponents","text":"<p>This means that Bower was looking for a package version which doesn't exist in the generated <code>bower-packages.nix</code>.</p> <p>If <code>bower.json</code> has been updated, then run <code>bower2nix</code> again.</p> <p>It could also be a bug in <code>bower2nix</code> or <code>fetchbower</code>. If possible, try reformulating the version specification in <code>bower.json</code>.</p>"},{"location":"languages-frameworks/chicken.section.html","title":"CHICKEN","text":"<p>CHICKEN is a R\u2075RS-compliant Scheme compiler. It includes an interactive mode and a custom package format, \"eggs\".</p>"},{"location":"languages-frameworks/chicken.section.html#sec-chicken-using","title":"Using Eggs","text":"<p>Eggs described in nixpkgs are available inside the <code>chickenPackages.chickenEggs</code> attrset. Including an egg as a build input is done in the typical Nix fashion. For example, to include support for SRFI 189 in a derivation, one might write:</p> <pre><code>  buildInputs = [\n    chicken\n    chickenPackages.chickenEggs.srfi-189\n  ];\n</code></pre> <p>Both <code>chicken</code> and its eggs have a setup hook which configures the environment variables <code>CHICKEN_INCLUDE_PATH</code> and <code>CHICKEN_REPOSITORY_PATH</code>.</p>"},{"location":"languages-frameworks/chicken.section.html#sec-chicken-updating-eggs","title":"Updating Eggs","text":"<p>nixpkgs only knows about a subset of all published eggs. It uses egg2nix to generate a package set from a list of eggs to include.</p> <p>The package set is regenerated by running the following shell commands:</p> <pre><code>$ nix-shell -p chickenPackages.egg2nix\n$ cd pkgs/development/compilers/chicken/5/\n$ egg2nix eggs.scm &gt; eggs.nix\n</code></pre>"},{"location":"languages-frameworks/chicken.section.html#sec-chicken-adding-eggs","title":"Adding Eggs","text":"<p>When we run <code>egg2nix</code>, we obtain one collection of eggs with mutually-compatible versions. This means that when we add new eggs, we may need to update existing eggs. To keep those separate, follow the procedure for updating eggs before including more eggs.</p> <p>To include more eggs, edit <code>pkgs/development/compilers/chicken/5/eggs.scm</code>. The first section of this file lists eggs which are required by <code>egg2nix</code> itself; all other eggs go into the second section. After editing, follow the procedure for updating eggs.</p>"},{"location":"languages-frameworks/chicken.section.html#sec-chicken-override-scope","title":"Override Scope","text":"<p>The chicken package and its eggs, respectively, reside in a scope. This means, the scope can be overridden to effect other packages in it.</p> <p>This example shows how to use a local copy of <code>srfi-180</code> and have it affect all the other eggs:</p> <pre><code>let\n  myChickenPackages = pkgs.chickenPackages.overrideScope' (self: super: {\n      # The chicken package itself can be overridden to effect the whole ecosystem.\n      # chicken = super.chicken.overrideAttrs {\n      #   src = ...\n      # };\n\n      chickenEggs = super.chickenEggs.overrideScope' (eggself: eggsuper: {\n        srfi-180 = eggsuper.srfi-180.overrideAttrs {\n          # path to a local copy of srfi-180\n          src = ...\n        };\n      });\n  });\nin\n# Here, `myChickenPackages.chickenEggs.json-rpc`, which depends on `srfi-180` will use\n# the local copy of `srfi-180`.\n# ...\n</code></pre>"},{"location":"languages-frameworks/coq.section.html","title":"Coq and coq packages","text":""},{"location":"languages-frameworks/coq.section.html#coq-derivation-coq","title":"Coq derivation: <code>coq</code>","text":"<p>The Coq derivation is overridable through the <code>coq.override overrides</code>, where overrides is an attribute set which contains the arguments to override. We recommend overriding either of the following</p> <ul> <li><code>version</code> (optional, defaults to the latest version of Coq selected for nixpkgs, see <code>pkgs/top-level/coq-packages</code> to witness this choice), which follows the conventions explained in the <code>coqPackages</code> section below,</li> <li><code>customOCamlPackages</code> (optional, defaults to <code>null</code>, which lets Coq choose a version automatically), which can be set to any of the ocaml packages attribute of <code>ocaml-ng</code> (such as <code>ocaml-ng.ocamlPackages_4_10</code> which is the default for Coq 8.11 for example).</li> <li><code>coq-version</code> (optional, defaults to the short version e.g. \"8.10\"), is a version number of the form \"x.y\" that indicates which Coq's version build behavior to mimic when using a source which is not a release. E.g. <code>coq.override { version = \"d370a9d1328a4e1cdb9d02ee032f605a9d94ec7a\"; coq-version = \"8.10\"; }</code>.</li> </ul> <p>The associated package set can be obtained using <code>mkCoqPackages coq</code>, where <code>coq</code> is the derivation to use.</p>"},{"location":"languages-frameworks/coq.section.html#coq-packages-attribute-sets-coqpackages","title":"Coq packages attribute sets: <code>coqPackages</code>","text":"<p>The recommended way of defining a derivation for a Coq library, is to use the <code>coqPackages.mkCoqDerivation</code> function, which is essentially a specialization of <code>mkDerivation</code> taking into account most of the specifics of Coq libraries. The following attributes are supported:</p> <ul> <li><code>pname</code> (required) is the name of the package,</li> <li><code>version</code> (optional, defaults to <code>null</code>), is the version to fetch and build,   this attribute is interpreted in several ways depending on its type and pattern:</li> <li>if it is a known released version string, i.e. from the <code>release</code> attribute below, the according release is picked, and the <code>version</code> attribute of the resulting derivation is set to this release string,</li> <li>if it is a majorMinor <code>\"x.y\"</code> prefix of a known released version (as defined above), then the latest <code>\"x.y.z\"</code> known released version is selected (for the ordering given by <code>versionAtLeast</code>),</li> <li>if it is a path or a string representing an absolute path (i.e. starting with <code>\"/\"</code>), the provided path is selected as a source, and the <code>version</code> attribute of the resulting derivation is set to <code>\"dev\"</code>,</li> <li>if it is a string of the form <code>owner:branch</code> then it tries to download the <code>branch</code> of owner <code>owner</code> for a project of the same name using the same vcs, and the <code>version</code> attribute of the resulting derivation is set to <code>\"dev\"</code>, additionally if the owner is not provided (i.e. if the <code>owner:</code> prefix is missing), it defaults to the original owner of the package (see below),</li> <li>if it is a string of the form <code>\"#N\"</code>, and the domain is github, then it tries to download the current head of the pull request <code>#N</code> from github,</li> <li><code>defaultVersion</code> (optional). Coq libraries may be compatible with some specific versions of Coq only. The <code>defaultVersion</code> attribute is used when no <code>version</code> is provided (or if <code>version = null</code>) to select the version of the library to use by default, depending on the context. This selection will mainly depend on a <code>coq</code> version number but also possibly on other packages versions (e.g. <code>mathcomp</code>). If its value ends up to be <code>null</code>, the package is marked for removal in end-user <code>coqPackages</code> attribute set.</li> <li><code>release</code> (optional, defaults to <code>{}</code>), lists all the known releases of the library and for each of them provides an attribute set with at least a <code>sha256</code> attribute (you may put the empty string <code>\"\"</code> in order to automatically insert a fake sha256, this will trigger an error which will allow you to find the correct sha256), each attribute set of the list of releases also takes optional overloading arguments for the fetcher as below (i.e.<code>domain</code>, <code>owner</code>, <code>repo</code>, <code>rev</code> assuming the default fetcher is used) and optional overrides for the result of the fetcher (i.e. <code>version</code> and <code>src</code>).</li> <li><code>fetcher</code> (optional, defaults to a generic fetching mechanism supporting github or gitlab based infrastructures), is a function that takes at least an <code>owner</code>, a <code>repo</code>, a <code>rev</code>, and a <code>hash</code> and returns an attribute set with a <code>version</code> and <code>src</code>.</li> <li><code>repo</code> (optional, defaults to the value of <code>pname</code>),</li> <li><code>owner</code> (optional, defaults to <code>\"coq-community\"</code>).</li> <li><code>domain</code> (optional, defaults to <code>\"github.com\"</code>), domains including the strings <code>\"github\"</code> or <code>\"gitlab\"</code> in their names are automatically supported, otherwise, one must change the <code>fetcher</code> argument to support them (cf <code>pkgs/development/coq-modules/heq/default.nix</code> for an example),</li> <li><code>releaseRev</code> (optional, defaults to <code>(v: v)</code>), provides a default mapping from release names to revision hashes/branch names/tags,</li> <li><code>displayVersion</code> (optional), provides a way to alter the computation of <code>name</code> from <code>pname</code>, by explaining how to display version numbers,</li> <li><code>namePrefix</code> (optional, defaults to <code>[ \"coq\" ]</code>), provides a way to alter the computation of <code>name</code> from <code>pname</code>, by explaining which dependencies must occur in <code>name</code>,</li> <li><code>nativeBuildInputs</code> (optional), is a list of executables that are required to build the current derivation, in addition to the default ones (namely <code>which</code>, <code>dune</code> and <code>ocaml</code> depending on whether <code>useDune</code>, <code>useDuneifVersion</code> and <code>mlPlugin</code> are set).</li> <li><code>extraNativeBuildInputs</code> (optional, deprecated), an additional list of derivation to add to <code>nativeBuildInputs</code>,</li> <li><code>overrideNativeBuildInputs</code> (optional) replaces the default list of derivation to which <code>nativeBuildInputs</code> and <code>extraNativeBuildInputs</code> adds extra elements,</li> <li><code>buildInputs</code> (optional), is a list of libraries and dependencies that are required to build and run the current derivation, in addition to the default one <code>[ coq ]</code>,</li> <li><code>extraBuildInputs</code> (optional, deprecated), an additional list of derivation to add to <code>buildInputs</code>,</li> <li><code>overrideBuildInputs</code> (optional) replaces the default list of derivation to which <code>buildInputs</code> and <code>extraBuildInputs</code> adds extras elements,</li> <li><code>propagatedBuildInputs</code> (optional) is passed as is to <code>mkDerivation</code>, we recommend to use this for Coq libraries and Coq plugin dependencies, as this makes sure the paths of the compiled libraries and plugins will always be added to the build environments of subsequent derivation, which is necessary for Coq packages to work correctly,</li> <li><code>mlPlugin</code> (optional, defaults to <code>false</code>). Some extensions (plugins) might require OCaml and sometimes other OCaml packages. Standard dependencies can be added by setting the current option to <code>true</code>. For a finer grain control, the <code>coq.ocamlPackages</code> attribute can be used in <code>nativeBuildInputs</code>, <code>buildInputs</code>, and <code>propagatedBuildInputs</code> to depend on the same package set Coq was built against.</li> <li><code>useDuneifVersion</code> (optional, default to <code>(x: false)</code> uses Dune to build the package if the provided predicate evaluates to true on the version, e.g. <code>useDuneifVersion = versions.isGe \"1.1\"</code>  will use dune if the version of the package is greater or equal to <code>\"1.1\"</code>,</li> <li><code>useDune</code> (optional, defaults to <code>false</code>) uses Dune to build the package if set to true, the presence of this attribute overrides the behavior of the previous one.</li> <li><code>opam-name</code> (optional, defaults to concatenating with a dash separator the components of <code>namePrefix</code> and <code>pname</code>), name of the Dune package to build.</li> <li><code>enableParallelBuilding</code> (optional, defaults to <code>true</code>), since it is activated by default, we provide a way to disable it.</li> <li><code>extraInstallFlags</code> (optional), allows to extend <code>installFlags</code> which initializes the variable <code>COQMF_COQLIB</code> so as to install in the proper subdirectory. Indeed Coq libraries should be installed in <code>$(out)/lib/coq/${coq.coq-version}/user-contrib/</code>. Such directories are automatically added to the <code>$COQPATH</code> environment variable by the hook defined in the Coq derivation.</li> <li><code>setCOQBIN</code> (optional, defaults to <code>true</code>), by default, the environment variable <code>$COQBIN</code> is set to the current Coq's binary, but one can disable this behavior by setting it to <code>false</code>,</li> <li><code>useMelquiondRemake</code> (optional, default to <code>null</code>) is an attribute set, which, if given, overloads the <code>preConfigurePhases</code>, <code>configureFlags</code>, <code>buildPhase</code>, and <code>installPhase</code> attributes of the derivation for a specific use in libraries using <code>remake</code> as set up by Guillaume Melquiond for <code>flocq</code>, <code>gappalib</code>, <code>interval</code>, and <code>coquelicot</code> (see the corresponding derivation for concrete examples of use of this option). For backward compatibility, the attribute <code>useMelquiondRemake.logpath</code> must be set to the logical root of the library (otherwise, one can pass <code>useMelquiondRemake = {}</code> to activate this without backward compatibility).</li> <li><code>dropAttrs</code>, <code>keepAttrs</code>, <code>dropDerivationAttrs</code> are all optional and allow to tune which attribute is added or removed from the final call to <code>mkDerivation</code>.</li> </ul> <p>It also takes other standard <code>mkDerivation</code> attributes, they are added as such, except for <code>meta</code> which extends an automatically computed <code>meta</code> (where the <code>platform</code> is the same as <code>coq</code> and the homepage is automatically computed).</p> <p>Here is a simple package example. It is a pure Coq library, thus it depends on Coq. It builds on the Mathematical Components library, thus it also takes some <code>mathcomp</code> derivations as <code>extraBuildInputs</code>.</p> <pre><code>{ lib, mkCoqDerivation, version ? null\n, coq, mathcomp, mathcomp-finmap, mathcomp-bigenough }:\nwith lib; mkCoqDerivation {\n  /* namePrefix leads to e.g. `name = coq8.11-mathcomp1.11-multinomials-1.5.2` */\n  namePrefix = [ \"coq\" \"mathcomp\" ];\n  pname = \"multinomials\";\n  owner = \"math-comp\";\n  inherit version;\n  defaultVersion =  with versions; switch [ coq.version mathcomp.version ] [\n      { cases = [ (range \"8.7\" \"8.12\")  \"1.11.0\" ];             out = \"1.5.2\"; }\n      { cases = [ (range \"8.7\" \"8.11\")  (range \"1.8\" \"1.10\") ]; out = \"1.5.0\"; }\n      { cases = [ (range \"8.7\" \"8.10\")  (range \"1.8\" \"1.10\") ]; out = \"1.4\"; }\n      { cases = [ \"8.6\"                 (range \"1.6\" \"1.7\") ];  out = \"1.1\"; }\n    ] null;\n  release = {\n    \"1.5.2\".sha256 = \"15aspf3jfykp1xgsxf8knqkxv8aav2p39c2fyirw7pwsfbsv2c4s\";\n    \"1.5.1\".sha256 = \"13nlfm2wqripaq671gakz5mn4r0xwm0646araxv0nh455p9ndjs3\";\n    \"1.5.0\".sha256 = \"064rvc0x5g7y1a0nip6ic91vzmq52alf6in2bc2dmss6dmzv90hw\";\n    \"1.5.0\".rev    = \"1.5\";\n    \"1.4\".sha256   = \"0vnkirs8iqsv8s59yx1fvg1nkwnzydl42z3scya1xp1b48qkgn0p\";\n    \"1.3\".sha256   = \"0l3vi5n094nx3qmy66hsv867fnqm196r8v605kpk24gl0aa57wh4\";\n    \"1.2\".sha256   = \"1mh1w339dslgv4f810xr1b8v2w7rpx6fgk9pz96q0fyq49fw2xcq\";\n    \"1.1\".sha256   = \"1q8alsm89wkc0lhcvxlyn0pd8rbl2nnxg81zyrabpz610qqjqc3s\";\n    \"1.0\".sha256   = \"1qmbxp1h81cy3imh627pznmng0kvv37k4hrwi2faa101s6bcx55m\";\n  };\n\n  propagatedBuildInputs =\n    [ mathcomp.ssreflect mathcomp.algebra mathcomp-finmap mathcomp-bigenough ];\n\n  meta = {\n    description = \"A Coq/SSReflect Library for Monoidal Rings and Multinomials\";\n    license = licenses.cecill-c;\n  };\n}\n</code></pre>"},{"location":"languages-frameworks/coq.section.html#coq-overriding-packages","title":"Three ways of overriding Coq packages","text":"<p>There are three distinct ways of changing a Coq package by overriding one of its values: <code>.override</code>, <code>overrideCoqDerivation</code>, and <code>.overrideAttrs</code>.  This section explains what sort of values can be overridden with each of these methods.</p>"},{"location":"languages-frameworks/coq.section.html#coq-override","title":"<code>.override</code>","text":"<p><code>.override</code> lets you change arguments to a Coq derivation.  In the case of the <code>multinomials</code> package above, <code>.override</code> would let you override arguments like <code>mkCoqDerivation</code>, <code>version</code>, <code>coq</code>, <code>mathcomp</code>, <code>mathcom-finmap</code>, etc.</p> <p>For example, assuming you have a special <code>mathcomp</code> dependency you want to use, here is how you could override the <code>mathcomp</code> dependency:</p> <pre><code>multinomials.override {\n  mathcomp = my-special-mathcomp;\n}\n</code></pre> <p>In Nixpkgs, all Coq derivations take a <code>version</code> argument.  This can be overridden in order to easily use a different version:</p> <pre><code>coqPackages.multinomials.override {\n  version = \"1.5.1\";\n}\n</code></pre> <p>Refer to  for all the different formats that you can potentially pass to <code>version</code>, as well as the restrictions.</p>"},{"location":"languages-frameworks/coq.section.html#coq-overrideCoqDerivation","title":"<code>overrideCoqDerivation</code>","text":"<p>The <code>overrideCoqDerivation</code> function lets you easily change arguments to <code>mkCoqDerivation</code>.  These arguments are described in .</p> <p>For example, here is how you could locally add a new release of the <code>multinomials</code> library, and set the <code>defaultVersion</code> to use this release:</p> <pre><code>coqPackages.lib.overrideCoqDerivation\n  {\n    defaultVersion = \"2.0\";\n    release.\"2.0\".sha256 = \"1lq8x86vd3vqqh2yq6hvyagpnhfq5wmk5pg2z0xq7b7dbbbhyfkk\";\n  }\n  coqPackages.multinomials\n</code></pre>"},{"location":"languages-frameworks/coq.section.html#coq-overrideAttrs","title":"<code>.overrideAttrs</code>","text":"<p><code>.overrideAttrs</code> lets you override arguments to the underlying <code>stdenv.mkDerivation</code> call. Internally, <code>mkCoqDerivation</code> uses <code>stdenv.mkDerivation</code> to create derivations for Coq libraries.  You can override arguments to <code>stdenv.mkDerivation</code> with <code>.overrideAttrs</code>.</p> <p>For instance, here is how you could add some code to be performed in the derivation after installation is complete:</p> <pre><code>coqPackages.multinomials.overrideAttrs (oldAttrs: {\n  postInstall = oldAttrs.postInstall or \"\" + ''\n    echo \"you can do anything you want here\"\n  '';\n})\n</code></pre>"},{"location":"languages-frameworks/crystal.section.html","title":"Crystal","text":""},{"location":"languages-frameworks/crystal.section.html#building-a-crystal-package","title":"Building a Crystal package","text":"<p>This section uses Mint as an example for how to build a Crystal package.</p> <p>If the Crystal project has any dependencies, the first step is to get a <code>shards.nix</code> file encoding those. Get a copy of the project and go to its root directory such that its <code>shard.lock</code> file is in the current directory. Executable projects should usually commit the <code>shard.lock</code> file, but sometimes that's not the case, which means you need to generate it yourself. With an existing <code>shard.lock</code> file, <code>crystal2nix</code> can be run. <pre><code>$ git clone https://github.com/mint-lang/mint\n$ cd mint\n$ git checkout 0.5.0\n$ if [ ! -f shard.lock ]; then nix-shell -p shards --run \"shards lock\"; fi\n$ nix-shell -p crystal2nix --run crystal2nix\n</code></pre></p> <p>This should have generated a <code>shards.nix</code> file.</p> <p>Next create a Nix file for your derivation and use <code>pkgs.crystal.buildCrystalPackage</code> as follows:</p> <pre><code>with import &lt;nixpkgs&gt; {};\ncrystal.buildCrystalPackage rec {\n  pname = \"mint\";\n  version = \"0.5.0\";\n\n  src = fetchFromGitHub {\n    owner = \"mint-lang\";\n    repo = \"mint\";\n    rev = version;\n    hash = \"sha256-dFN9l5fgrM/TtOPqlQvUYgixE4KPr629aBmkwdDoq28=\";\n  };\n\n  # Insert the path to your shards.nix file here\n  shardsFile = ./shards.nix;\n\n  ...\n}\n</code></pre> <p>This won't build anything yet, because we haven't told it what files build. We can specify a mapping from binary names to source files with the <code>crystalBinaries</code> attribute. The project's compilation instructions should show this. For Mint, the binary is called \"mint\", which is compiled from the source file <code>src/mint.cr</code>, so we'll specify this as follows:</p> <pre><code>  crystalBinaries.mint.src = \"src/mint.cr\";\n\n  # ...\n</code></pre> <p>Additionally you can override the default <code>crystal build</code> options (which are currently <code>--release --progress --no-debug --verbose</code>) with</p> <pre><code>  crystalBinaries.mint.options = [ \"--release\" \"--verbose\" ];\n</code></pre> <p>Depending on the project, you might need additional steps to get it to compile successfully. In Mint's case, we need to link against openssl, so in the end the Nix file looks as follows:</p> <pre><code>with import &lt;nixpkgs&gt; {};\ncrystal.buildCrystalPackage rec {\n  version = \"0.5.0\";\n  pname = \"mint\";\n  src = fetchFromGitHub {\n    owner = \"mint-lang\";\n    repo = \"mint\";\n    rev = version;\n    hash = \"sha256-dFN9l5fgrM/TtOPqlQvUYgixE4KPr629aBmkwdDoq28=\";\n  };\n\n  shardsFile = ./shards.nix;\n  crystalBinaries.mint.src = \"src/mint.cr\";\n\n  buildInputs = [ openssl ];\n}\n</code></pre>"},{"location":"languages-frameworks/cuda.section.html","title":"CUDA","text":"<p>CUDA-only packages are stored in the <code>cudaPackages</code> packages set. This set includes the <code>cudatoolkit</code>, portions of the toolkit in separate derivations, <code>cudnn</code>, <code>cutensor</code> and <code>nccl</code>.</p> <p>A package set is available for each CUDA version, so for example <code>cudaPackages_11_6</code>. Within each set is a matching version of the above listed packages. Additionally, other versions of the packages that are packaged and compatible are available as well. For example, there can be a <code>cudaPackages.cudnn_8_3</code> package.</p> <p>To use one or more CUDA packages in an expression, give the expression a <code>cudaPackages</code> parameter, and in case CUDA is optional <pre><code>{ config\n, cudaSupport ? config.cudaSupport\n, cudaPackages ? { }\n, ...\n}:\n</code></pre></p> <p>When using <code>callPackage</code>, you can choose to pass in a different variant, e.g. when a different version of the toolkit suffices <pre><code>mypkg = callPackage { cudaPackages = cudaPackages_11_5; }\n</code></pre></p> <p>If another version of say <code>cudnn</code> or <code>cutensor</code> is needed, you can override the package set to make it the default. This guarantees you get a consistent package set. <pre><code>mypkg = let\n  cudaPackages = cudaPackages_11_5.overrideScope (final: prev: {\n    cudnn = prev.cudnn_8_3;\n  }});\nin callPackage { inherit cudaPackages; };\n</code></pre></p> <p>The CUDA NVCC compiler requires flags to determine which hardware you want to target for in terms of SASS (real hardware) or PTX (JIT kernels).</p> <p>Nixpkgs tries to target support real architecture defaults based on the CUDA toolkit version with PTX support for future hardware.  Experienced users may optimize this configuration for a variety of reasons such as reducing binary size and compile time, supporting legacy hardware, or optimizing for specific hardware.</p> <p>You may provide capabilities to add support or reduce binary size through <code>config</code> using <code>cudaCapabilities = [ \"6.0\" \"7.0\" ];</code> and <code>cudaForwardCompat = true;</code> if you want PTX support for future hardware.</p> <p>Please consult GPUs supported for your specific card(s).</p> <p>Library maintainers should consult NVCC Docs and release notes for their software package.</p>"},{"location":"languages-frameworks/cuda.section.html#adding-a-new-cuda-release","title":"Adding a new CUDA release","text":"<p>WARNING</p> <p>This section of the docs is still very much in progress. Feedback is welcome in GitHub Issues tagging @NixOS/cuda-maintainers or on Matrix.</p> <p>The CUDA Toolkit is a suite of CUDA libraries and software meant to provide a development environment for CUDA-accelerated applications. Until the release of CUDA 11.4, NVIDIA had only made the CUDA Toolkit available as a multi-gigabyte runfile installer, which we provide through the <code>cudaPackages.cudatoolkit</code> attribute. From CUDA 11.4 and onwards, NVIDIA has also provided CUDA redistributables (\u201cCUDA-redist\u201d): individually packaged CUDA Toolkit components meant to facilitate redistribution and inclusion in downstream projects. These packages are available in the <code>cudaPackages</code> package set.</p> <p>All new projects should use the CUDA redistributables available in <code>cudaPackages</code> in place of <code>cudaPackages.cudatoolkit</code>, as they are much easier to maintain and update.</p>"},{"location":"languages-frameworks/cuda.section.html#updating-cuda-redistributables","title":"Updating CUDA redistributables","text":"<ol> <li>Go to NVIDIA's index of CUDA redistributables: https://developer.download.nvidia.com/compute/cuda/redist/</li> <li>Make a note of the new version of CUDA available.</li> <li>Run</li> </ol> <pre><code>nix run github:connorbaker/cuda-redist-find-features -- \\\n   download-manifests \\\n   --log-level DEBUG \\\n   --version &lt;newest CUDA version&gt; \\\n   https://developer.download.nvidia.com/compute/cuda/redist \\\n   ./pkgs/development/cuda-modules/cuda/manifests\n</code></pre> <p>This will download a copy of the manifest for the new version of CUDA. 4. Run</p> <pre><code>nix run github:connorbaker/cuda-redist-find-features -- \\\n   process-manifests \\\n   --log-level DEBUG \\\n   --version &lt;newest CUDA version&gt; \\\n   https://developer.download.nvidia.com/compute/cuda/redist \\\n   ./pkgs/development/cuda-modules/cuda/manifests\n</code></pre> <p>This will generate a <code>redistrib_features_&lt;newest CUDA version&gt;.json</code> file in the same directory as the manifest. 5. Update the <code>cudaVersionMap</code> attribute set in <code>pkgs/development/cuda-modules/cuda/extension.nix</code>.</p>"},{"location":"languages-frameworks/cuda.section.html#updating-cutensor","title":"Updating cuTensor","text":"<ol> <li>Repeat the steps present in Updating CUDA redistributables with the following changes:</li> <li>Use the index of cuTensor redistributables: https://developer.download.nvidia.com/compute/cutensor/redist</li> <li>Use the newest version of cuTensor available instead of the newest version of CUDA.</li> <li>Use <code>pkgs/development/cuda-modules/cutensor/manifests</code> instead of <code>pkgs/development/cuda-modules/cuda/manifests</code>.</li> <li>Skip the step of updating <code>cudaVersionMap</code> in <code>pkgs/development/cuda-modules/cuda/extension.nix</code>.</li> </ol>"},{"location":"languages-frameworks/cuda.section.html#updating-supported-compilers-and-gpus","title":"Updating supported compilers and GPUs","text":"<ol> <li>Update <code>nvcc-compatibilities.nix</code> in <code>pkgs/development/cuda-modules/</code> to include the newest release of NVCC, as well as any newly supported host compilers.</li> <li>Update <code>gpus.nix</code> in <code>pkgs/development/cuda-modules/</code> to include any new GPUs supported by the new release of CUDA.</li> </ol>"},{"location":"languages-frameworks/cuda.section.html#updating-the-cuda-toolkit","title":"Updating the CUDA Toolkit runfile installer","text":"<p>WARNING</p> <p>While the CUDA Toolkit runfile installer is still available in Nixpkgs as the <code>cudaPackages.cudatoolkit</code> attribute, its use is not recommended and should it be considered deprecated. Please migrate to the CUDA redistributables provided by the <code>cudaPackages</code> package set.</p> <p>To ensure packages relying on the CUDA Toolkit runfile installer continue to build, it will continue to be updated until a migration path is available.</p> <ol> <li>Go to NVIDIA's CUDA Toolkit runfile installer download page: https://developer.nvidia.com/cuda-downloads</li> <li> <p>Select the appropriate OS, architecture, distribution, and version, and installer type.</p> </li> <li> <p>For example: Linux, x86_64, Ubuntu, 22.04, runfile (local)</p> </li> <li> <p>NOTE: Typically, we use the Ubuntu runfile. It is unclear if the runfile for other distributions will work.</p> </li> <li> <p>Take the link provided by the installer instructions on the webpage after selecting the installer type and get its hash by running:</p> </li> </ol> <pre><code>nix store prefetch-file --hash-type sha256 &lt;link&gt;\n</code></pre> <ol> <li>Update <code>pkgs/development/cuda-modules/cudatoolkit/releases.nix</code> to include the release.</li> </ol>"},{"location":"languages-frameworks/cuda.section.html#updating-the-cuda-package-set","title":"Updating the CUDA package set","text":"<ol> <li> <p>Include a new <code>cudaPackages_&lt;major&gt;_&lt;minor&gt;</code> package set in <code>pkgs/top-level/all-packages.nix</code>.</p> </li> <li> <p>NOTE: Changing the default CUDA package set should occur in a separate PR, allowing time for additional testing.</p> </li> <li> <p>Successfully build the closure of the new package set, updating <code>pkgs/development/cuda-modules/cuda/overrides.nix</code> as needed. Below are some common failures:</p> </li> </ol> Unable to ... During ... Reason Solution Note Find headers <code>configurePhase</code> or <code>buildPhase</code> Missing dependency on a <code>dev</code> output Add the missing dependency The <code>dev</code> output typically contain the headers Find libraries <code>configurePhase</code> Missing dependency on a <code>dev</code> output Add the missing dependency The <code>dev</code> output typically contain CMake configuration files Find libraries <code>buildPhase</code> or <code>patchelf</code> Missing dependency on a <code>lib</code> or <code>static</code> output Add the missing dependency The <code>lib</code> or <code>static</code> output typically contain the libraries <p>In the scenario you are unable to run the resulting binary: this is arguably the most complicated as it could be any combination of the previous reasons. This type of failure typically occurs when a library attempts to load or open a library it depends on that it does not declare in its <code>DT_NEEDED</code> section. As a first step, ensure that dependencies are patched with <code>cudaPackages.autoAddOpenGLRunpath</code>. Failing that, try running the application with <code>nixGL</code> or a similar wrapper tool. If that works, it likely means that the application is attempting to load a library that is not in the <code>RPATH</code> or <code>RUNPATH</code> of the binary.</p>"},{"location":"languages-frameworks/cuelang.section.html","title":"Cue (Cuelang)","text":"<p>Cuelang is a language to:</p> <ul> <li>describe schemas and validate backward-compatibility</li> <li>generate code and schemas in various formats (e.g. JSON Schema, OpenAPI)</li> <li>do configuration akin to Dhall Lang</li> <li>perform data validation</li> </ul>"},{"location":"languages-frameworks/cuelang.section.html#cuelang-quickstart","title":"Cuelang schema quick start","text":"<p>Cuelang schemas are similar to JSON, here is a quick cheatsheet:</p> <ul> <li>Default types includes: <code>null</code>, <code>string</code>, <code>bool</code>, <code>bytes</code>, <code>number</code>, <code>int</code>, <code>float</code>, lists as <code>[...T]</code> where <code>T</code> is a type.</li> <li>All structures, defined by: <code>myStructName: { &lt;fields&gt; }</code> are open -- they accept fields which are not specified.</li> <li>Closed structures can be built by doing <code>myStructName: close({ &lt;fields&gt; })</code> -- they are strict in what they accept.</li> <li><code>#X</code> are definitions, referenced definitions are recursively closed, i.e. all its children structures are closed.</li> <li><code>&amp;</code> operator is the unification operator (similar to a type-level merging operator), <code>|</code> is the disjunction operator (similar to a type-level union operator).</li> <li> <p>Values are types, i.e. <code>myStruct: { a: 3 }</code> is a valid type definition that only allows <code>3</code> as value.</p> </li> <li> <p>Read https://cuelang.org/docs/concepts/logic/ to learn more about the semantics.</p> </li> <li>Read https://cuelang.org/docs/references/spec/ to learn about the language specification.</li> </ul>"},{"location":"languages-frameworks/cuelang.section.html#cuelang-writeCueValidator","title":"<code>writeCueValidator</code>","text":"<p>Nixpkgs provides a <code>pkgs.writeCueValidator</code> helper, which will write a validation script based on the provided Cuelang schema.</p> <p>Here is an example: <pre><code>pkgs.writeCueValidator\n  (pkgs.writeText \"schema.cue\" ''\n    #Def1: {\n      field1: string\n    }\n  '')\n  { document = \"#Def1\"; }\n</code></pre></p> <ul> <li>The first parameter is the Cue schema file.</li> <li>The second parameter is an options parameter, currently, only: <code>document</code> can be passed.</li> </ul> <p><code>document</code> : match your input data against this fragment of structure or definition, e.g. you may use the same schema file but different documents based on the data you are validating.</p> <p>Another example, given the following <code>validator.nix</code> : <pre><code>{ pkgs ? import &lt;nixpkgs&gt; {} }:\nlet\n  genericValidator = version:\n  pkgs.writeCueValidator\n    (pkgs.writeText \"schema.cue\" ''\n      #Version1: {\n        field1: string\n      }\n      #Version2: #Version1 &amp; {\n        field1: \"unused\"\n      }''\n    )\n    { document = \"#Version${toString version}\"; };\nin\n{\n  validateV1 = genericValidator 1;\n  validateV2 = genericValidator 2;\n}\n</code></pre></p> <p>The result is a script that will validate the file you pass as the first argument against the schema you provided <code>writeCueValidator</code>.</p> <p>It can be any format that <code>cue vet</code> supports, i.e. YAML or JSON for example.</p> <p>Here is an example, named <code>example.json</code>, given the following JSON: <pre><code>{ \"field1\": \"abc\" }\n</code></pre></p> <p>You can run the result script (named <code>validate</code>) as the following:</p> <pre><code>$ nix-build validator.nix\n$ ./result example.json\n$ ./result-2 example.json\nfield1: conflicting values \"unused\" and \"abc\":\n    ./example.json:1:13\n    ../../../../../../nix/store/v64dzx3vr3glpk0cq4hzmh450lrwh6sg-schema.cue:5:11\n$ sed -i 's/\"abc\"/3/' example.json\n$ ./result example.json\nfield1: conflicting values 3 and string (mismatched types int and string):\n    ./example.json:1:13\n    ../../../../../../nix/store/v64dzx3vr3glpk0cq4hzmh450lrwh6sg-schema.cue:5:11\n</code></pre> <p>Known limitations</p> <ul> <li>The script will enforce concrete values and will not accept lossy transformations (strictness). You can add these options if you need them.</li> </ul>"},{"location":"languages-frameworks/dart.section.html","title":"Dart","text":""},{"location":"languages-frameworks/dart.section.html#ssec-dart-applications","title":"Dart applications","text":"<p>The function <code>buildDartApplication</code> builds Dart applications managed with pub.</p> <p>It fetches its Dart dependencies automatically through <code>pub2nix</code>, and (through a series of hooks) builds and installs the executables specified in the pubspec file. The hooks can be used in other derivations, if needed. The phases can also be overridden to do something different from installing binaries.</p> <p>If you are packaging a Flutter desktop application, use <code>buildFlutterApplication</code> instead.</p> <p><code>pubspecLock</code> is the parsed pubspec.lock file. pub2nix uses this to download required packages. This can be converted to JSON from YAML with something like <code>yq . pubspec.lock</code>, and then read by Nix.</p> <p>Alternatively, <code>autoPubspecLock</code> can be used instead, and set to a path to a regular <code>pubspec.lock</code> file. This relies on import-from-derivation, and is not permitted in Nixpkgs, but can be useful at other times.</p> <p>::: {.warning} When using <code>autoPubspecLock</code> with a local source directory, make sure to use a concatenation operator (e.g. <code>autoPubspecLock = src + \"/pubspec.lock\";</code>), and not string interpolation.</p> <p>String interpolation will copy your entire source directory to the Nix store and use its store path, meaning that unrelated changes to your source tree will cause the generated <code>pubspec.lock</code> derivation to rebuild! :::</p> <p>If the package has Git package dependencies, the hashes must be provided in the <code>gitHashes</code> set. If a hash is missing, an error message prompting you to add it will be shown.</p> <p>The <code>dart</code> commands run can be overridden through <code>pubGetScript</code> and <code>dartCompileCommand</code>, you can also add flags using <code>dartCompileFlags</code> or <code>dartJitFlags</code>.</p> <p>Dart supports multiple outputs types, you can choose between them using <code>dartOutputType</code> (defaults to <code>exe</code>). If you want to override the binaries path or the source path they come from, you can use <code>dartEntryPoints</code>. Outputs that require a runtime will automatically be wrapped with the relevant runtime (<code>dartaotruntime</code> for <code>aot-snapshot</code>, <code>dart run</code> for <code>jit-snapshot</code> and <code>kernel</code>, <code>node</code> for <code>js</code>), this can be overridden through <code>dartRuntimeCommand</code>.</p> <pre><code>{ lib, buildDartApplication, fetchFromGitHub }:\n\nbuildDartApplication rec {\n  pname = \"dart-sass\";\n  version = \"1.62.1\";\n\n  src = fetchFromGitHub {\n    owner = \"sass\";\n    repo = pname;\n    rev = version;\n    hash = \"sha256-U6enz8yJcc4Wf8m54eYIAnVg/jsGi247Wy8lp1r1wg4=\";\n  };\n\n  pubspecLock = lib.importJSON ./pubspec.lock.json;\n}\n</code></pre>"},{"location":"languages-frameworks/dart.section.html#ssec-dart-applications-patching-dependencies","title":"Patching dependencies","text":"<p>Some Dart packages require patches or build environment changes. Package derivations can be customised with the <code>customSourceBuilders</code> argument.</p> <p>A collection of such customisations can be found in Nixpkgs, in the <code>development/compilers/dart/package-source-builders</code> directory.</p> <p>This allows fixes for packages to be shared between all applications that use them. It is strongly recommended to add to this collection instead of including fixes in your application derivation itself.</p>"},{"location":"languages-frameworks/dart.section.html#ssec-dart-applications-build-tools","title":"Running executables from dev_dependencies","text":"<p>Many Dart applications require executables from the <code>dev_dependencies</code> section in <code>pubspec.yaml</code> to be run before building them.</p> <p>This can be done in <code>preBuild</code>, in one of two ways:</p> <ol> <li>Packaging the tool with <code>buildDartApplication</code>, adding it to Nixpkgs, and running it like any other application</li> <li>Running the tool from the package cache</li> </ol> <p>Of these methods, the first is recommended when using a tool that does not need to be of a specific version.</p> <p>For the second method, the <code>packageRun</code> function from the <code>dartConfigHook</code> can be used. This is an alternative to <code>dart run</code> that does not rely on Pub.</p> <p>e.g., for <code>build_runner</code>:</p> <pre><code>packageRun build_runner build\n</code></pre> <p>Do not use <code>dart run &lt;package_name&gt;</code>, as this will attempt to download dependencies with Pub.</p>"},{"location":"languages-frameworks/dart.section.html#ssec-dart-applications-nix-shell","title":"Usage with nix-shell","text":""},{"location":"languages-frameworks/dart.section.html#ssec-dart-applications-nix-shell-deps","title":"Using dependencies from the Nix store","text":"<p>As <code>buildDartApplication</code> provides dependencies instead of <code>pub get</code>, Dart needs to be explicitly told where to find them.</p> <p>Run the following commands in the source directory to configure Dart appropriately. Do not use <code>pub</code> after doing so; it will download the dependencies itself and overwrite these changes.</p> <pre><code>cp --no-preserve=all \"$pubspecLockFilePath\" pubspec.lock\nmkdir -p .dart_tool &amp;&amp; cp --no-preserve=all \"$packageConfig\" .dart_tool/package_config.json\n</code></pre>"},{"location":"languages-frameworks/dart.section.html#ssec-dart-flutter","title":"Flutter applications","text":"<p>The function <code>buildFlutterApplication</code> builds Flutter applications.</p> <p>See the Dart documentation for more details on required files and arguments.</p> <pre><code>{  flutter, fetchFromGitHub }:\n\nflutter.buildFlutterApplication {\n  pname = \"firmware-updater\";\n  version = \"0-unstable-2023-04-30\";\n\n  # To build for the Web, use the targetFlutterPlatform argument.\n  # targetFlutterPlatform = \"web\";\n\n  src = fetchFromGitHub {\n    owner = \"canonical\";\n    repo = \"firmware-updater\";\n    rev = \"6e7dbdb64e344633ea62874b54ff3990bd3b8440\";\n    sha256 = \"sha256-s5mwtr5MSPqLMN+k851+pFIFFPa0N1hqz97ys050tFA=\";\n    fetchSubmodules = true;\n  };\n\n  pubspecLock = lib.importJSON ./pubspec.lock.json;\n}\n</code></pre>"},{"location":"languages-frameworks/dart.section.html#ssec-dart-flutter-nix-shell","title":"Usage with nix-shell","text":"<p>Flutter-specific <code>nix-shell</code> usage notes are included here. See the Dart documentation for general <code>nix-shell</code> instructions.</p>"},{"location":"languages-frameworks/dart.section.html#ssec-dart-flutter-nix-shell-enter","title":"Entering the shell","text":"<p>By default, dependencies for only the <code>targetFlutterPlatform</code> are available in the build environment. This is useful for keeping closures small, but be problematic during development. It's common, for example, to build Web apps for Linux during development to take advantage of native features such as stateful hot reload.</p> <p>To enter a shell with all the usual target platforms available, use the <code>multiShell</code> attribute.</p> <p>e.g. <code>nix-shell '&lt;nixpkgs&gt;' -A fluffychat-web.multiShell</code>.</p>"},{"location":"languages-frameworks/dhall.section.html","title":"Dhall","text":"<p>The Nixpkgs support for Dhall assumes some familiarity with Dhall's language support for importing Dhall expressions, which is documented here:</p> <ul> <li><code>dhall-lang.org</code> - Installing packages</li> </ul>"},{"location":"languages-frameworks/dhall.section.html#ssec-dhall-remote-imports","title":"Remote imports","text":"<p>Nixpkgs bypasses Dhall's support for remote imports using Dhall's semantic integrity checks.  Specifically, any Dhall import can be protected by an integrity check like:</p> <pre><code>https://prelude.dhall-lang.org/v20.1.0/package.dhall\n  sha256:26b0ef498663d269e4dc6a82b0ee289ec565d683ef4c00d0ebdd25333a5a3c98\n</code></pre> <p>\u2026 and if the import is cached then the interpreter will load the import from cache instead of fetching the URL.</p> <p>Nixpkgs uses this trick to add all of a Dhall expression's dependencies into the cache so that the Dhall interpreter never needs to resolve any remote URLs.  In fact, Nixpkgs uses a Dhall interpreter with remote imports disabled when packaging Dhall expressions to enforce that the interpreter never resolves a remote import.  This means that Nixpkgs only supports building Dhall expressions if all of their remote imports are protected by semantic integrity checks.</p> <p>Instead of remote imports, Nixpkgs uses Nix to fetch remote Dhall code.  For example, the Prelude Dhall package uses <code>pkgs.fetchFromGitHub</code> to fetch the <code>dhall-lang</code> repository containing the Prelude.  Relying exclusively on Nix to fetch Dhall code ensures that Dhall packages built using Nix remain pure and also behave well when built within a sandbox.</p>"},{"location":"languages-frameworks/dhall.section.html#ssec-dhall-packaging-expression","title":"Packaging a Dhall expression from scratch","text":"<p>We can illustrate how Nixpkgs integrates Dhall by beginning from the following trivial Dhall expression with one dependency (the Prelude):</p> <pre><code>-- ./true.dhall\n\nlet Prelude = https://prelude.dhall-lang.org/v20.1.0/package.dhall\n\nin  Prelude.Bool.not False\n</code></pre> <p>As written, this expression cannot be built using Nixpkgs because the expression does not protect the Prelude import with a semantic integrity check, so the first step is to freeze the expression using <code>dhall freeze</code>, like this:</p> <pre><code>$ dhall freeze --inplace ./true.dhall\n</code></pre> <p>\u2026 which gives us:</p> <pre><code>-- ./true.dhall\n\nlet Prelude =\n      https://prelude.dhall-lang.org/v20.1.0/package.dhall\n        sha256:26b0ef498663d269e4dc6a82b0ee289ec565d683ef4c00d0ebdd25333a5a3c98\n\nin  Prelude.Bool.not False\n</code></pre> <p>To package that expression, we create a <code>./true.nix</code> file containing the following specification for the Dhall package:</p> <pre><code># ./true.nix\n\n{ buildDhallPackage, Prelude }:\n\nbuildDhallPackage {\n  name = \"true\";\n  code = ./true.dhall;\n  dependencies = [ Prelude ];\n  source = true;\n}\n</code></pre> <p>\u2026 and we complete the build by incorporating that Dhall package into the <code>pkgs.dhallPackages</code> hierarchy using an overlay, like this:</p> <pre><code># ./example.nix\n\nlet\n  nixpkgs = builtins.fetchTarball {\n    url    = \"https://github.com/NixOS/nixpkgs/archive/94b2848559b12a8ed1fe433084686b2a81123c99.tar.gz\";\n    hash = \"sha256-B4Q3c6IvTLg3Q92qYa8y+i4uTaphtFdjp+Ir3QQjdN0=\";\n  };\n\n  dhallOverlay = self: super: {\n    true = self.callPackage ./true.nix { };\n  };\n\n  overlay = self: super: {\n    dhallPackages = super.dhallPackages.override (old: {\n      overrides =\n        self.lib.composeExtensions (old.overrides or (_: _: {})) dhallOverlay;\n    });\n  };\n\n  pkgs = import nixpkgs { config = {}; overlays = [ overlay ]; };\n\nin\n  pkgs\n</code></pre> <p>\u2026 which we can then build using this command:</p> <pre><code>$ nix build --file ./example.nix dhallPackages.true\n</code></pre>"},{"location":"languages-frameworks/dhall.section.html#ssec-dhall-package-contents","title":"Contents of a Dhall package","text":"<p>The above package produces the following directory tree:</p> <pre><code>$ tree -a ./result\nresult\n\u251c\u2500\u2500 .cache\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 dhall\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 122027abdeddfe8503496adeb623466caa47da5f63abd2bc6fa19f6cfcb73ecfed70\n\u251c\u2500\u2500 binary.dhall\n\u2514\u2500\u2500 source.dhall\n</code></pre> <p>\u2026 where:</p> <ul> <li><code>source.dhall</code> contains the result of interpreting our Dhall package:</li> </ul> <pre><code>$ cat ./result/source.dhall\nTrue\n</code></pre> <ul> <li>The <code>.cache</code> subdirectory contains one binary cache product encoding the   same result as <code>source.dhall</code>:</li> </ul> <pre><code>$ dhall decode &lt; ./result/.cache/dhall/122027abdeddfe8503496adeb623466caa47da5f63abd2bc6fa19f6cfcb73ecfed70\nTrue\n</code></pre> <ul> <li><code>binary.dhall</code> contains a Dhall expression which handles fetching and decoding   the same cache product:</li> </ul> <pre><code>$ cat ./result/binary.dhall\nmissing sha256:27abdeddfe8503496adeb623466caa47da5f63abd2bc6fa19f6cfcb73ecfed70\n$ cp -r ./result/.cache .cache\n\n$ chmod -R u+w .cache\n\n$ XDG_CACHE_HOME=.cache dhall --file ./result/binary.dhall\nTrue\n</code></pre> <p>The <code>source.dhall</code> file is only present for packages that specify <code>source = true;</code>.  By default, Dhall packages omit the <code>source.dhall</code> in order to conserve disk space when they are used exclusively as dependencies.  For example, if we build the Prelude package it will only contain the binary encoding of the expression:</p> <pre><code>$ nix build --file ./example.nix dhallPackages.Prelude\n\n$ tree -a result\nresult\n\u251c\u2500\u2500 .cache\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 dhall\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 122026b0ef498663d269e4dc6a82b0ee289ec565d683ef4c00d0ebdd25333a5a3c98\n\u2514\u2500\u2500 binary.dhall\n\n2 directories, 2 files\n</code></pre> <p>Typically, you only specify <code>source = true;</code> for the top-level Dhall expression of interest (such as our example <code>true.nix</code> Dhall package).  However, if you wish to specify <code>source = true</code> for all Dhall packages, then you can amend the Dhall overlay like this:</p> <pre><code>  dhallOverrides = self: super: {\n    # Enable source for all Dhall packages\n    buildDhallPackage =\n      args: super.buildDhallPackage (args // { source = true; });\n\n    true = self.callPackage ./true.nix { };\n  };\n</code></pre> <p>\u2026 and now the Prelude will contain the fully decoded result of interpreting the Prelude:</p> <pre><code>$ nix build --file ./example.nix dhallPackages.Prelude\n\n$ tree -a result\nresult\n\u251c\u2500\u2500 .cache\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 dhall\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 122026b0ef498663d269e4dc6a82b0ee289ec565d683ef4c00d0ebdd25333a5a3c98\n\u251c\u2500\u2500 binary.dhall\n\u2514\u2500\u2500 source.dhall\n\n$ cat ./result/source.dhall\n{ Bool =\n  { and =\n      \\(_ : List Bool) -&gt;\n        List/fold Bool _ Bool (\\(_ : Bool) -&gt; \\(_ : Bool) -&gt; _@1 &amp;&amp; _) True\n  , build = \\(_ : Type -&gt; _ -&gt; _@1 -&gt; _@2) -&gt; _ Bool True False\n  , even =\n      \\(_ : List Bool) -&gt;\n        List/fold Bool _ Bool (\\(_ : Bool) -&gt; \\(_ : Bool) -&gt; _@1 == _) True\n  , fold =\n      \\(_ : Bool) -&gt;\n\u2026\n</code></pre>"},{"location":"languages-frameworks/dhall.section.html#ssec-dhall-packaging-functions","title":"Packaging functions","text":"<p>We already saw an example of using <code>buildDhallPackage</code> to create a Dhall package from a single file, but most Dhall packages consist of more than one file and there are two derived utilities that you may find more useful when packaging multiple files:</p> <ul> <li> <p><code>buildDhallDirectoryPackage</code> - build a Dhall package from a local directory</p> </li> <li> <p><code>buildDhallGitHubPackage</code> - build a Dhall package from a GitHub repository</p> </li> </ul> <p>The <code>buildDhallPackage</code> is the lowest-level function and accepts the following arguments:</p> <ul> <li> <p><code>name</code>: The name of the derivation</p> </li> <li> <p><code>dependencies</code>: Dhall dependencies to build and cache ahead of time</p> </li> <li> <p><code>code</code>: The top-level expression to build for this package</p> </li> </ul> <p>Note that the <code>code</code> field accepts an arbitrary Dhall expression.  You're   not limited to just a file.</p> <ul> <li> <p><code>source</code>: Set to <code>true</code> to include the decoded result as <code>source.dhall</code> in the   build product, at the expense of requiring more disk space</p> </li> <li> <p><code>documentationRoot</code>: Set to the root directory of the package if you want   <code>dhall-docs</code> to generate documentation underneath the <code>docs</code> subdirectory of   the build product</p> </li> </ul> <p>The <code>buildDhallDirectoryPackage</code> is a higher-level function implemented in terms of <code>buildDhallPackage</code> that accepts the following arguments:</p> <ul> <li> <p><code>name</code>: Same as <code>buildDhallPackage</code></p> </li> <li> <p><code>dependencies</code>: Same as <code>buildDhallPackage</code></p> </li> <li> <p><code>source</code>: Same as <code>buildDhallPackage</code></p> </li> <li> <p><code>src</code>: The directory containing Dhall code that you want to turn into a Dhall   package</p> </li> <li> <p><code>file</code>: The top-level file (<code>package.dhall</code> by default) that is the entrypoint   to the rest of the package</p> </li> <li> <p><code>document</code>: Set to <code>true</code> to generate documentation for the package</p> </li> </ul> <p>The <code>buildDhallGitHubPackage</code> is another higher-level function implemented in terms of <code>buildDhallPackage</code> that accepts the following arguments:</p> <ul> <li> <p><code>name</code>: Same as <code>buildDhallPackage</code></p> </li> <li> <p><code>dependencies</code>: Same as <code>buildDhallPackage</code></p> </li> <li> <p><code>source</code>: Same as <code>buildDhallPackage</code></p> </li> <li> <p><code>owner</code>: The owner of the repository</p> </li> <li> <p><code>repo</code>: The repository name</p> </li> <li> <p><code>rev</code>: The desired revision (or branch, or tag)</p> </li> <li> <p><code>directory</code>: The subdirectory of the Git repository to package (if a   directory other than the root of the repository)</p> </li> <li> <p><code>file</code>: The top-level file (<code>${directory}/package.dhall</code> by default) that is   the entrypoint to the rest of the package</p> </li> <li> <p><code>document</code>: Set to <code>true</code> to generate documentation for the package</p> </li> </ul> <p>Additionally, <code>buildDhallGitHubPackage</code> accepts the same arguments as <code>fetchFromGitHub</code>, such as <code>hash</code> or <code>fetchSubmodules</code>.</p>"},{"location":"languages-frameworks/dhall.section.html#ssec-dhall-dhall-to-nixpkgs","title":"<code>dhall-to-nixpkgs</code>","text":"<p>You can use the <code>dhall-to-nixpkgs</code> command-line utility to automate packaging Dhall code.  For example:</p> <pre><code>$ nix-shell -p haskellPackages.dhall-nixpkgs nix-prefetch-git\n[nix-shell]$ dhall-to-nixpkgs github https://github.com/Gabriella439/dhall-semver.git\n{ buildDhallGitHubPackage, Prelude }:\n  buildDhallGitHubPackage {\n    name = \"dhall-semver\";\n    githubBase = \"github.com\";\n    owner = \"Gabriella439\";\n    repo = \"dhall-semver\";\n    rev = \"2d44ae605302ce5dc6c657a1216887fbb96392a4\";\n    fetchSubmodules = false;\n    hash = \"sha256-n0nQtswVapWi/x7or0O3MEYmAkt/a1uvlOtnje6GGnk=\";\n    directory = \"\";\n    file = \"package.dhall\";\n    source = false;\n    document = false;\n    dependencies = [ (Prelude.overridePackage { file = \"package.dhall\"; }) ];\n    }\n</code></pre> <p>:::{.note} <code>nix-prefetch-git</code> is added to the <code>nix-shell -p</code> invocation above, because it has to be in <code>$PATH</code> for <code>dhall-to-nixpkgs</code> to work. :::</p> <p>The utility takes care of automatically detecting remote imports and converting them to package dependencies.  You can also use the utility on local Dhall directories, too:</p> <pre><code>$ dhall-to-nixpkgs directory ~/proj/dhall-semver\n{ buildDhallDirectoryPackage, Prelude }:\n  buildDhallDirectoryPackage {\n    name = \"proj\";\n    src = ~/proj/dhall-semver;\n    file = \"package.dhall\";\n    source = false;\n    document = false;\n    dependencies = [ (Prelude.overridePackage { file = \"package.dhall\"; }) ];\n    }\n</code></pre>"},{"location":"languages-frameworks/dhall.section.html#ssec-dhall-remote-imports-as-fod","title":"Remote imports as fixed-output derivations","text":"<p><code>dhall-to-nixpkgs</code> has the ability to fetch and build remote imports as fixed-output derivations by using their Dhall integrity check. This is sometimes easier than manually packaging all remote imports.</p> <p>This can be used like the following:</p> <pre><code>$ dhall-to-nixpkgs directory --fixed-output-derivations ~/proj/dhall-semver\n{ buildDhallDirectoryPackage, buildDhallUrl }:\n  buildDhallDirectoryPackage {\n    name = \"proj\";\n    src = ~/proj/dhall-semver;\n    file = \"package.dhall\";\n    source = false;\n    document = false;\n    dependencies = [\n      (buildDhallUrl {\n        url = \"https://prelude.dhall-lang.org/v17.0.0/package.dhall\";\n        hash = \"sha256-ENs8kZwl6QRoM9+Jeo/+JwHcOQ+giT2VjDQwUkvlpD4=\";\n        dhallHash = \"sha256:10db3c919c25e9046833df897a8ffe2701dc390fa0893d958c3430524be5a43e\";\n        })\n      ];\n    }\n</code></pre> <p>Here, <code>dhall-semver</code>'s <code>Prelude</code> dependency is fetched and built with the <code>buildDhallUrl</code> helper function, instead of being passed in as a function argument.</p>"},{"location":"languages-frameworks/dhall.section.html#ssec-dhall-overriding-dependency-versions","title":"Overriding dependency versions","text":"<p>Suppose that we change our <code>true.dhall</code> example expression to depend on an older version of the Prelude (19.0.0):</p> <pre><code>-- ./true.dhall\n\nlet Prelude =\n      https://prelude.dhall-lang.org/v19.0.0/package.dhall\n        sha256:eb693342eb769f782174157eba9b5924cf8ac6793897fc36a31ccbd6f56dafe2\n\nin  Prelude.Bool.not False\n</code></pre> <p>If we try to rebuild that expression the build will fail:</p> <pre><code>$ nix build --file ./example.nix dhallPackages.true\nbuilder for '/nix/store/0f1hla7ff1wiaqyk1r2ky4wnhnw114fi-true.drv' failed with exit code 1; last 10 log lines:\n\n  Dhall was compiled without the 'with-http' flag.\n\n  The requested URL was: https://prelude.dhall-lang.org/v19.0.0/package.dhall\n\n\n  4\u2502       https://prelude.dhall-lang.org/v19.0.0/package.dhall\n  5\u2502         sha256:eb693342eb769f782174157eba9b5924cf8ac6793897fc36a31ccbd6f56dafe2\n\n  /nix/store/rsab4y99h14912h4zplqx2iizr5n4rc2-true.dhall:4:7\n[1 built (1 failed), 0.0 MiB DL]\nerror: build of '/nix/store/0f1hla7ff1wiaqyk1r2ky4wnhnw114fi-true.drv' failed\n</code></pre> <p>\u2026 because the default Prelude selected by Nixpkgs revision <code>94b2848559b12a8ed1fe433084686b2a81123c99is</code> is version 20.1.0, which doesn't have the same integrity check as version 19.0.0.  This means that version 19.0.0 is not cached and the interpreter is not allowed to fall back to importing the URL.</p> <p>However, we can override the default Prelude version by using <code>dhall-to-nixpkgs</code> to create a Dhall package for our desired Prelude:</p> <pre><code>$ dhall-to-nixpkgs github https://github.com/dhall-lang/dhall-lang.git \\\n    --name Prelude \\\n    --directory Prelude \\\n    --rev v19.0.0 \\\n    &gt; Prelude.nix\n</code></pre> <p>\u2026 and then referencing that package in our Dhall overlay, by either overriding the Prelude globally for all packages, like this:</p> <pre><code>  dhallOverrides = self: super: {\n    true = self.callPackage ./true.nix { };\n\n    Prelude = self.callPackage ./Prelude.nix { };\n  };\n</code></pre> <p>\u2026 or selectively overriding the Prelude dependency for just the <code>true</code> package, like this:</p> <pre><code>  dhallOverrides = self: super: {\n    true = self.callPackage ./true.nix {\n      Prelude = self.callPackage ./Prelude.nix { };\n    };\n  };\n</code></pre>"},{"location":"languages-frameworks/dhall.section.html#ssec-dhall-overrides","title":"Overrides","text":"<p>You can override any of the arguments to <code>buildDhallGitHubPackage</code> or <code>buildDhallDirectoryPackage</code> using the <code>overridePackage</code> attribute of a package. For example, suppose we wanted to selectively enable <code>source = true</code> just for the Prelude.  We can do that like this:</p> <pre><code>  dhallOverrides = self: super: {\n    Prelude = super.Prelude.overridePackage { source = true; };\n\n    \u2026\n  };\n</code></pre>"},{"location":"languages-frameworks/dotnet.section.html","title":"Dotnet","text":""},{"location":"languages-frameworks/dotnet.section.html#local-development-workflow","title":"Local Development Workflow","text":"<p>For local development, it's recommended to use nix-shell to create a dotnet environment:</p> <pre><code># shell.nix\nwith import &lt;nixpkgs&gt; {};\n\nmkShell {\n  name = \"dotnet-env\";\n  packages = [\n    dotnet-sdk\n  ];\n}\n</code></pre>"},{"location":"languages-frameworks/dotnet.section.html#using-many-sdks-in-a-workflow","title":"Using many sdks in a workflow","text":"<p>It's very likely that more than one sdk will be needed on a given project. Dotnet provides several different frameworks (E.g dotnetcore, aspnetcore, etc.) as well as many versions for a given framework. Normally, dotnet is able to fetch a framework and install it relative to the executable. However, this would mean writing to the nix store in nixpkgs, which is read-only. To support the many-sdk use case, one can compose an environment using <code>dotnetCorePackages.combinePackages</code>:</p> <pre><code>with import &lt;nixpkgs&gt; {};\n\nmkShell {\n  name = \"dotnet-env\";\n  packages = [\n    (with dotnetCorePackages; combinePackages [\n      sdk_6_0\n      sdk_7_0\n    ])\n  ];\n}\n</code></pre> <p>This will produce a dotnet installation that has the dotnet 6.0 7.0 sdk. The first sdk listed will have it's cli utility present in the resulting environment. Example info output:</p> <pre><code>$ dotnet --info\n.NET SDK:\n Version:   7.0.202\n Commit:    6c74320bc3\n\n\u015arodowisko uruchomieniowe:\n OS Name:     nixos\n OS Version:  23.05\n OS Platform: Linux\n RID:         linux-x64\n Base Path:   /nix/store/n2pm44xq20hz7ybsasgmd7p3yh31gnh4-dotnet-sdk-7.0.202/sdk/7.0.202/\n\nHost:\n  Version:      7.0.4\n  Architecture: x64\n  Commit:       0a396acafe\n\n.NET SDKs installed:\n  6.0.407 [/nix/store/3b19303vwrhv0xxz1hg355c7f2hgxxgd-dotnet-core-combined/sdk]\n  7.0.202 [/nix/store/3b19303vwrhv0xxz1hg355c7f2hgxxgd-dotnet-core-combined/sdk]\n\n.NET runtimes installed:\n  Microsoft.AspNetCore.App 6.0.15 [/nix/store/3b19303vwrhv0xxz1hg355c7f2hgxxgd-dotnet-core-combined/shared/Microsoft.AspNetCore.App]\n  Microsoft.AspNetCore.App 7.0.4 [/nix/store/3b19303vwrhv0xxz1hg355c7f2hgxxgd-dotnet-core-combined/shared/Microsoft.AspNetCore.App]\n  Microsoft.NETCore.App 6.0.15 [/nix/store/3b19303vwrhv0xxz1hg355c7f2hgxxgd-dotnet-core-combined/shared/Microsoft.NETCore.App]\n  Microsoft.NETCore.App 7.0.4 [/nix/store/3b19303vwrhv0xxz1hg355c7f2hgxxgd-dotnet-core-combined/shared/Microsoft.NETCore.App]\n\nOther architectures found:\n  None\n\nEnvironment variables:\n  Not set\n\nglobal.json file:\n  Not found\n\nLearn more:\n  https://aka.ms/dotnet/info\n\nDownload .NET:\n  https://aka.ms/dotnet/download\n</code></pre>"},{"location":"languages-frameworks/dotnet.section.html#dotnet-sdk-vs-dotnetcorepackages.sdk","title":"dotnet-sdk vs dotnetCorePackages.sdk","text":"<p>The <code>dotnetCorePackages.sdk_X_Y</code> is preferred over the old dotnet-sdk as both major and minor version are very important for a dotnet environment. If a given minor version isn't present (or was changed), then this will likely break your ability to build a project.</p>"},{"location":"languages-frameworks/dotnet.section.html#dotnetcorepackages.sdk-vs-dotnetcorepackages.runtime-vs-dotnetcorepackages.aspnetcore","title":"dotnetCorePackages.sdk vs dotnetCorePackages.runtime vs dotnetCorePackages.aspnetcore","text":"<p>The <code>dotnetCorePackages.sdk</code> contains both a runtime and the full sdk of a given version. The <code>runtime</code> and <code>aspnetcore</code> packages are meant to serve as minimal runtimes to deploy alongside already built applications.</p>"},{"location":"languages-frameworks/dotnet.section.html#packaging-a-dotnet-application","title":"Packaging a Dotnet Application","text":"<p>To package Dotnet applications, you can use <code>buildDotnetModule</code>. This has similar arguments to <code>stdenv.mkDerivation</code>, with the following additions:</p> <ul> <li><code>projectFile</code> is used for specifying the dotnet project file, relative to the source root. These have <code>.sln</code> (entire solution) or <code>.csproj</code> (single project) file extensions. This can be a list of multiple projects as well. When omitted, will attempt to find and build the solution (<code>.sln</code>). If running into problems, make sure to set it to a file (or a list of files) with the <code>.csproj</code> extension - building applications as entire solutions is not fully supported by the .NET CLI.</li> <li> <p><code>nugetDeps</code> takes either a path to a <code>deps.nix</code> file, or a derivation. The <code>deps.nix</code> file can be generated using the script attached to <code>passthru.fetch-deps</code>. If the argument is a derivation, it will be used directly and assume it has the same output as <code>mkNugetDeps</code>. ::: {.note} For more detail about managing the <code>deps.nix</code> file, see Generating and updating NuGet dependencies :::</p> </li> <li> <p><code>packNupkg</code> is used to pack project as a <code>nupkg</code>, and installs it to <code>$out/share</code>. If set to <code>true</code>, the derivation can be used as a dependency for another dotnet project by adding it to <code>projectReferences</code>.</p> </li> <li><code>projectReferences</code> can be used to resolve <code>ProjectReference</code> project items. Referenced projects can be packed with <code>buildDotnetModule</code> by setting the <code>packNupkg = true</code> attribute and passing a list of derivations to <code>projectReferences</code>. Since we are sharing referenced projects as NuGets they must be added to csproj/fsproj files as <code>PackageReference</code> as well.  For example, your project has a local dependency:  <pre><code>    &lt;ProjectReference Include=\"../foo/bar.fsproj\" /&gt;\n</code></pre>  To enable discovery through <code>projectReferences</code> you would need to add:  <code>xml      &lt;ProjectReference Include=\"../foo/bar.fsproj\" /&gt;      &lt;PackageReference Include=\"bar\" Version=\"*\" Condition=\" '$(ContinuousIntegrationBuild)'=='true' \"/&gt;</code></li> <li><code>executables</code> is used to specify which executables get wrapped to <code>$out/bin</code>, relative to <code>$out/lib/$pname</code>. If this is unset, all executables generated will get installed. If you do not want to install any, set this to <code>[]</code>. This gets done in the <code>preFixup</code> phase.</li> <li><code>runtimeDeps</code> is used to wrap libraries into <code>LD_LIBRARY_PATH</code>. This is how dotnet usually handles runtime dependencies.</li> <li><code>buildType</code> is used to change the type of build. Possible values are <code>Release</code>, <code>Debug</code>, etc. By default, this is set to <code>Release</code>.</li> <li><code>selfContainedBuild</code> allows to enable the self-contained build flag. By default, it is set to false and generated applications have a dependency on the selected dotnet runtime. If enabled, the dotnet runtime is bundled into the executable and the built app has no dependency on .NET.</li> <li><code>useAppHost</code> will enable creation of a binary executable that runs the .NET application using the specified root. More info in Microsoft docs. Enabled by default.</li> <li><code>useDotnetFromEnv</code> will change the binary wrapper so that it uses the .NET from the environment. The runtime specified by <code>dotnet-runtime</code> is given as a fallback in case no .NET is installed in the user's environment. This is most useful for .NET global tools and LSP servers, which often extend the .NET CLI and their runtime should match the users' .NET runtime.</li> <li><code>dotnet-sdk</code> is useful in cases where you need to change what dotnet SDK is being used. You can also set this to the result of <code>dotnetSdkPackages.combinePackages</code>, if the project uses multiple SDKs to build.</li> <li><code>dotnet-runtime</code> is useful in cases where you need to change what dotnet runtime is being used. This can be either a regular dotnet runtime, or an aspnetcore.</li> <li><code>dotnet-test-sdk</code> is useful in cases where unit tests expect a different dotnet SDK. By default, this is set to the <code>dotnet-sdk</code> attribute.</li> <li><code>testProjectFile</code> is useful in cases where the regular project file does not contain the unit tests. It gets restored and build, but not installed. You may need to regenerate your nuget lockfile after setting this. Note that if set, only tests from this project are executed.</li> <li><code>disabledTests</code> is used to disable running specific unit tests. This gets passed as: <code>dotnet test --filter \"FullyQualifiedName!={}\"</code>, to ensure compatibility with all unit test frameworks.</li> <li><code>dotnetRestoreFlags</code> can be used to pass flags to <code>dotnet restore</code>.</li> <li><code>dotnetBuildFlags</code> can be used to pass flags to <code>dotnet build</code>.</li> <li><code>dotnetTestFlags</code> can be used to pass flags to <code>dotnet test</code>. Used only if <code>doCheck</code> is set to <code>true</code>.</li> <li><code>dotnetInstallFlags</code> can be used to pass flags to <code>dotnet install</code>.</li> <li><code>dotnetPackFlags</code> can be used to pass flags to <code>dotnet pack</code>. Used only if <code>packNupkg</code> is set to <code>true</code>.</li> <li><code>dotnetFlags</code> can be used to pass flags to all of the above phases.</li> </ul> <p>When packaging a new application, you need to fetch its dependencies. Create an empty <code>deps.nix</code>, set <code>nugetDeps = ./deps.nix</code>, then run <code>nix-build -A package.fetch-deps</code> to generate a script that will build the lockfile for you.</p> <p>Here is an example <code>default.nix</code>, using some of the previously discussed arguments: <pre><code>{ lib, buildDotnetModule, dotnetCorePackages, ffmpeg }:\n\nlet\n  referencedProject = import ../../bar { ... };\nin buildDotnetModule rec {\n  pname = \"someDotnetApplication\";\n  version = \"0.1\";\n\n  src = ./.;\n\n  projectFile = \"src/project.sln\";\n  # File generated with `nix-build -A package.passthru.fetch-deps`.\n  # To run fetch-deps when this file does not yet exist, set nugetDeps to null\n  nugetDeps = ./deps.nix;\n\n  projectReferences = [ referencedProject ]; # `referencedProject` must contain `nupkg` in the folder structure.\n\n  dotnet-sdk = dotnetCorePackages.sdk_6_0;\n  dotnet-runtime = dotnetCorePackages.runtime_6_0;\n\n  executables = [ \"foo\" ]; # This wraps \"$out/lib/$pname/foo\" to `$out/bin/foo`.\n  executables = []; # Don't install any executables.\n\n  packNupkg = true; # This packs the project as \"foo-0.1.nupkg\" at `$out/share`.\n\n  runtimeDeps = [ ffmpeg ]; # This will wrap ffmpeg's library path into `LD_LIBRARY_PATH`.\n}\n</code></pre></p> <p>Keep in mind that you can tag the <code>@NixOS/dotnet</code> team for help and code review.</p>"},{"location":"languages-frameworks/dotnet.section.html#dotnet-global-tools","title":"Dotnet global tools","text":"<p>.NET Global tools are a mechanism provided by the dotnet CLI to install .NET binaries from Nuget packages.</p> <p>They can be installed either as a global tool for the entire system, or as a local tool specific to project.</p> <p>The local installation is the easiest and works on NixOS in the same way as on other Linux distributions. See dotnet documentation to learn more.</p> <p>The global installation method should also work most of the time. You have to remember to update the <code>PATH</code> value to the location the tools are installed to (the CLI will inform you about it during installation) and also set the <code>DOTNET_ROOT</code> value, so that the tool can find the .NET SDK package. You can find the path to the SDK by running <code>nix eval --raw nixpkgs#dotnet-sdk</code> (substitute the <code>dotnet-sdk</code> package for another if a different SDK version is needed).</p> <p>This method is not recommended on NixOS, since it's not declarative and involves installing binaries not made for NixOS, which will not always work.</p> <p>The third, and preferred way, is packaging the tool into a Nix derivation.</p>"},{"location":"languages-frameworks/dotnet.section.html#packaging-dotnet-global-tools","title":"Packaging Dotnet global tools","text":"<p>Dotnet global tools are standard .NET binaries, just made available through a special NuGet package. Therefore, they can be built and packaged like every .NET application, using <code>buildDotnetModule</code>.</p> <p>If however the source is not available or difficult to build, the <code>buildDotnetGlobalTool</code> helper can be used, which will package the tool straight from its NuGet package.</p> <p>This helper has the same arguments as <code>buildDotnetModule</code>, with a few differences:</p> <ul> <li><code>pname</code> and <code>version</code> are required, and will be used to find the NuGet package of the tool</li> <li><code>nugetName</code> can be used to override the NuGet package name that will be downloaded, if it's different from <code>pname</code></li> <li><code>nugetSha256</code> is the hash of the fetched NuGet package. Set this to <code>lib.fakeHash256</code> for the first build, and it will error out, giving you the proper hash. Also remember to update it during version updates (it will not error out if you just change the version while having a fetched package in <code>/nix/store</code>)</li> <li><code>dotnet-runtime</code> is set to <code>dotnet-sdk</code> by default. When changing this, remember that .NET tools fetched from NuGet require an SDK.</li> </ul> <p>Here is an example of packaging <code>pbm</code>, an unfree binary without source available: <pre><code>{ buildDotnetGlobalTool, lib }:\n\nbuildDotnetGlobalTool {\n  pname = \"pbm\";\n  version = \"1.3.1\";\n\n  nugetSha256 = \"sha256-ZG2HFyKYhVNVYd2kRlkbAjZJq88OADe3yjxmLuxXDUo=\";\n\n  meta = with lib; {\n    homepage = \"https://cmd.petabridge.com/index.html\";\n    changelog = \"https://cmd.petabridge.com/articles/RELEASE_NOTES.html\";\n    license = licenses.unfree;\n    platforms = platforms.linux;\n  };\n}\n</code></pre></p>"},{"location":"languages-frameworks/dotnet.section.html#generating-and-updating-nuget-dependencies","title":"Generating and updating NuGet dependencies","text":"<p>First, restore the packages to the <code>out</code> directory, ensure you have cloned the upstream repository and you are inside it.</p> <pre><code>$ dotnet restore --packages out\n  Determining projects to restore...\n  Restored /home/lychee/Celeste64/Celeste64.csproj (in 1.21 sec).\n</code></pre> <p>Next, use <code>nuget-to-nix</code> tool provided in nixpkgs to generate a lockfile to <code>deps.nix</code> from the packages inside the <code>out</code> directory.</p> <p><pre><code>$ nuget-to-nix out &gt; deps.nix\n</code></pre> Which <code>nuget-to-nix</code> will generate an output similar to below <pre><code>{ fetchNuGet }: [\n  (fetchNuGet { pname = \"FosterFramework\"; version = \"0.1.15-alpha\"; sha256 = \"0pzsdfbsfx28xfqljcwy100xhbs6wyx0z1d5qxgmv3l60di9xkll\"; })\n  (fetchNuGet { pname = \"Microsoft.AspNetCore.App.Runtime.linux-x64\"; version = \"8.0.1\"; sha256 = \"1gjz379y61ag9whi78qxx09bwkwcznkx2mzypgycibxk61g11da1\"; })\n  (fetchNuGet { pname = \"Microsoft.NET.ILLink.Tasks\"; version = \"8.0.1\"; sha256 = \"1drbgqdcvbpisjn8mqfgba1pwb6yri80qc4mfvyczqwrcsj5k2ja\"; })\n  (fetchNuGet { pname = \"Microsoft.NETCore.App.Runtime.linux-x64\"; version = \"8.0.1\"; sha256 = \"1g5b30f4l8a1zjjr3b8pk9mcqxkxqwa86362f84646xaj4iw3a4d\"; })\n  (fetchNuGet { pname = \"SharpGLTF.Core\"; version = \"1.0.0-alpha0031\"; sha256 = \"0ln78mkhbcxqvwnf944hbgg24vbsva2jpih6q3x82d3h7rl1pkh6\"; })\n  (fetchNuGet { pname = \"SharpGLTF.Runtime\"; version = \"1.0.0-alpha0031\"; sha256 = \"0lvb3asi3v0n718qf9y367km7qpkb9wci38y880nqvifpzllw0jg\"; })\n  (fetchNuGet { pname = \"Sledge.Formats\"; version = \"1.2.2\"; sha256 = \"1y0l66m9rym0p1y4ifjlmg3j9lsmhkvbh38frh40rpvf1axn2dyh\"; })\n  (fetchNuGet { pname = \"Sledge.Formats.Map\"; version = \"1.1.5\"; sha256 = \"1bww60hv9xcyxpvkzz5q3ybafdxxkw6knhv97phvpkw84pd0jil6\"; })\n  (fetchNuGet { pname = \"System.Numerics.Vectors\"; version = \"4.5.0\"; sha256 = \"1kzrj37yzawf1b19jq0253rcs8hsq1l2q8g69d7ipnhzb0h97m59\"; })\n]\n</code></pre></p> <p>Finally, you move the <code>deps.nix</code> file to the appropriate location to be used by <code>nugetDeps</code>, then you're all set!</p> <p>If you ever need to update the dependencies of a package, you instead do</p> <ul> <li><code>nix-build -A package.fetch-deps</code> to generate the update script for <code>package</code></li> <li>Run <code>./result deps.nix</code> to regenerate the lockfile to <code>deps.nix</code>, keep in mind if a location isn't provided, it will write to a temporary path instead</li> <li>Finally, move the file where needed and look at its contents to confirm it has updated the dependencies.</li> </ul>"},{"location":"languages-frameworks/emscripten.section.html","title":"Emscripten","text":"<p>Emscripten: An LLVM-to-JavaScript Compiler</p> <p>If you want to work with <code>emcc</code>, <code>emconfigure</code> and <code>emmake</code> as you are used to from Ubuntu and similar distributions,</p> <pre><code>nix-shell -p emscripten\n</code></pre> <p>A few things to note:</p> <ul> <li><code>export EMCC_DEBUG=2</code> is nice for debugging</li> <li>The build artifact cache in <code>~/.emscripten</code> sometimes creates issues and needs to be removed from time to time</li> </ul>"},{"location":"languages-frameworks/emscripten.section.html#declarative-usage","title":"Examples","text":"<p>Let's see two different examples from <code>pkgs/top-level/emscripten-packages.nix</code>:</p> <ul> <li><code>pkgs.zlib.override</code></li> <li><code>pkgs.buildEmscriptenPackage</code></li> </ul> <p>A special requirement of the <code>pkgs.buildEmscriptenPackage</code> is the <code>doCheck = true</code>. This means each Emscripten package requires that a <code>checkPhase</code> is implemented.</p> <ul> <li>Use <code>export EMCC_DEBUG=2</code> from within a phase to get more detailed debug output what is going wrong.</li> <li>The cache at <code>~/.emscripten</code> requires to set <code>HOME=$TMPDIR</code> in individual phases.   This makes compilation slower but also more deterministic.</li> </ul> <p>::: {.example #usage-1-pkgs.zlib.override}</p>"},{"location":"languages-frameworks/emscripten.section.html#using-pkgszliboverride","title":"Using <code>pkgs.zlib.override {}</code>","text":"<p>This example uses <code>zlib</code> from Nixpkgs, but instead of compiling C to ELF it compiles C to JavaScript since we were using <code>pkgs.zlib.override</code> and changed <code>stdenv</code> to <code>pkgs.emscriptenStdenv</code>.</p> <p>A few adaptions and hacks were put in place to make it work. One advantage is that when <code>pkgs.zlib</code> is updated, it will automatically update this package as well.</p> <pre><code>(pkgs.zlib.override {\n  stdenv = pkgs.emscriptenStdenv;\n}).overrideAttrs\n(old: rec {\n  buildInputs = old.buildInputs ++ [ pkg-config ];\n  # we need to reset this setting!\n  env = (old.env or { }) // { NIX_CFLAGS_COMPILE = \"\"; };\n  configurePhase = ''\n    # FIXME: Some tests require writing at $HOME\n    HOME=$TMPDIR\n    runHook preConfigure\n\n    #export EMCC_DEBUG=2\n    emconfigure ./configure --prefix=$out --shared\n\n    runHook postConfigure\n  '';\n  dontStrip = true;\n  outputs = [ \"out\" ];\n  buildPhase = ''\n    emmake make\n  '';\n  installPhase = ''\n    emmake make install\n  '';\n  checkPhase = ''\n    echo \"================= testing zlib using node =================\"\n\n    echo \"Compiling a custom test\"\n    set -x\n    emcc -O2 -s EMULATE_FUNCTION_POINTER_CASTS=1 test/example.c -DZ_SOLO \\\n    libz.so.${old.version} -I . -o example.js\n\n    echo \"Using node to execute the test\"\n    ${pkgs.nodejs}/bin/node ./example.js\n\n    set +x\n    if [ $? -ne 0 ]; then\n      echo \"test failed for some reason\"\n      exit 1;\n    else\n      echo \"it seems to work! very good.\"\n    fi\n    echo \"================= /testing zlib using node =================\"\n  '';\n\n  postPatch = pkgs.lib.optionalString pkgs.stdenv.isDarwin ''\n    substituteInPlace configure \\\n      --replace-fail '/usr/bin/libtool' 'ar' \\\n      --replace-fail 'AR=\"libtool\"' 'AR=\"ar\"' \\\n      --replace-fail 'ARFLAGS=\"-o\"' 'ARFLAGS=\"-r\"'\n  '';\n})\n</code></pre> <p>:::{.example #usage-2-pkgs.buildemscriptenpackage}</p>"},{"location":"languages-frameworks/emscripten.section.html#using-pkgsbuildemscriptenpackage","title":"Using <code>pkgs.buildEmscriptenPackage {}</code>","text":"<p>This <code>xmlmirror</code> example features an Emscripten package that is defined completely from this context and no <code>pkgs.zlib.override</code> is used.</p> <pre><code>pkgs.buildEmscriptenPackage rec {\n  name = \"xmlmirror\";\n\n  buildInputs = [ pkg-config autoconf automake libtool gnumake libxml2 nodejs openjdk json_c ];\n  nativeBuildInputs = [ pkg-config zlib ];\n\n  src = pkgs.fetchgit {\n    url = \"https://gitlab.com/odfplugfest/xmlmirror.git\";\n    rev = \"4fd7e86f7c9526b8f4c1733e5c8b45175860a8fd\";\n    hash = \"sha256-i+QgY+5PYVg5pwhzcDnkfXAznBg3e8sWH2jZtixuWsk=\";\n  };\n\n  configurePhase = ''\n    rm -f fastXmlLint.js*\n    # a fix for ERROR:root:For asm.js, TOTAL_MEMORY must be a multiple of 16MB, was 234217728\n    # https://gitlab.com/odfplugfest/xmlmirror/issues/8\n    sed -e \"s/TOTAL_MEMORY=234217728/TOTAL_MEMORY=268435456/g\" -i Makefile.emEnv\n    # https://github.com/kripken/emscripten/issues/6344\n    # https://gitlab.com/odfplugfest/xmlmirror/issues/9\n    sed -e \"s/\\$(JSONC_LDFLAGS) \\$(ZLIB_LDFLAGS) \\$(LIBXML20_LDFLAGS)/\\$(JSONC_LDFLAGS) \\$(LIBXML20_LDFLAGS) \\$(ZLIB_LDFLAGS) /g\" -i Makefile.emEnv\n    # https://gitlab.com/odfplugfest/xmlmirror/issues/11\n    sed -e \"s/-o fastXmlLint.js/-s EXTRA_EXPORTED_RUNTIME_METHODS='[\\\"ccall\\\", \\\"cwrap\\\"]' -o fastXmlLint.js/g\" -i Makefile.emEnv\n  '';\n\n  buildPhase = ''\n    HOME=$TMPDIR\n    make -f Makefile.emEnv\n  '';\n\n  outputs = [ \"out\" \"doc\" ];\n\n  installPhase = ''\n    mkdir -p $out/share\n    mkdir -p $doc/share/${name}\n\n    cp Demo* $out/share\n    cp -R codemirror-5.12 $out/share\n    cp fastXmlLint.js* $out/share\n    cp *.xsd $out/share\n    cp *.js $out/share\n    cp *.xhtml $out/share\n    cp *.html $out/share\n    cp *.json $out/share\n    cp *.rng $out/share\n    cp README.md $doc/share/${name}\n  '';\n  checkPhase = ''\n\n  '';\n}\n</code></pre> <p>:::</p>"},{"location":"languages-frameworks/emscripten.section.html#declarative-debugging","title":"Debugging","text":"<p>Use <code>nix-shell -I nixpkgs=/some/dir/nixpkgs -A emscriptenPackages.libz</code> and from there you can go trough the individual steps. This makes it easy to build a good <code>unit test</code> or list the files of the project.</p> <ol> <li><code>nix-shell -I nixpkgs=/some/dir/nixpkgs -A emscriptenPackages.libz</code></li> <li><code>cd /tmp/</code></li> <li><code>unpackPhase</code></li> <li>cd libz-1.2.3</li> <li><code>configurePhase</code></li> <li><code>buildPhase</code></li> <li>... happy hacking...</li> </ol>"},{"location":"languages-frameworks/gnome.section.html","title":"GNOME","text":""},{"location":"languages-frameworks/gnome.section.html#ssec-gnome-packaging","title":"Packaging GNOME applications","text":"<p>Programs in the GNOME universe are written in various languages but they all use GObject-based libraries like GLib, GTK or GStreamer. These libraries are often modular, relying on looking into certain directories to find their modules. However, due to Nix\u2019s specific file system organization, this will fail without our intervention. Fortunately, the libraries usually allow overriding the directories through environment variables, either natively or thanks to a patch in nixpkgs. Wrapping the executables to ensure correct paths are available to the application constitutes a significant part of packaging a modern desktop application. In this section, we will describe various modules needed by such applications, environment variables needed to make the modules load, and finally a script that will do the work for us.</p>"},{"location":"languages-frameworks/gnome.section.html#ssec-gnome-settings","title":"Settings","text":"<p>GSettings API is often used for storing settings. GSettings schemas are required, to know the type and other metadata of the stored values. GLib looks for <code>glib-2.0/schemas/gschemas.compiled</code> files inside the directories of <code>XDG_DATA_DIRS</code>.</p> <p>On Linux, GSettings API is implemented using dconf backend. You will need to add <code>dconf</code> GIO module to <code>GIO_EXTRA_MODULES</code> variable, otherwise the <code>memory</code> backend will be used and the saved settings will not be persistent.</p> <p>Last you will need the dconf database D-Bus service itself. You can enable it using <code>programs.dconf.enable</code>.</p> <p>Some applications will also require <code>gsettings-desktop-schemas</code> for things like reading proxy configuration or user interface customization. This dependency is often not mentioned by upstream, you should grep for <code>org.gnome.desktop</code> and <code>org.gnome.system</code> to see if the schemas are needed.</p>"},{"location":"languages-frameworks/gnome.section.html#ssec-gnome-gio-modules","title":"GIO modules","text":"<p>GLib\u2019s GIO library supports several extension points. Notably, they allow:</p> <ul> <li>implementing settings backends (already mentioned)</li> <li>adding TLS support</li> <li>proxy settings</li> <li>virtual file systems</li> </ul> <p>The modules are typically installed to <code>lib/gio/modules/</code> directory of a package and you need to add them to <code>GIO_EXTRA_MODULES</code> if you need any of those features.</p> <p>In particular, we recommend:</p> <ul> <li>adding <code>dconf.lib</code> for any software on Linux that reads GSettings (even transitively through e.g. GTK\u2019s file manager)</li> <li>adding <code>glib-networking</code> for any software that accesses network using GIO or libsoup \u2013 glib-networking contains a module that implements TLS support and loads system-wide proxy settings</li> </ul> <p>To allow software to use various virtual file systems, <code>gvfs</code> package can be also added. But that is usually an optional feature so we typically use <code>gvfs</code> from the system (e.g. installed globally using NixOS module).</p>"},{"location":"languages-frameworks/gnome.section.html#ssec-gnome-gdk-pixbuf-loaders","title":"GdkPixbuf loaders","text":"<p>GTK applications typically use GdkPixbuf to load images. But <code>gdk-pixbuf</code> package only supports basic bitmap formats like JPEG, PNG or TIFF, requiring to use third-party loader modules for other formats. This is especially painful since GTK itself includes SVG icons, which cannot be rendered without a loader provided by <code>librsvg</code>.</p> <p>Unlike other libraries mentioned in this section, GdkPixbuf only supports a single value in its controlling environment variable <code>GDK_PIXBUF_MODULE_FILE</code>. It is supposed to point to a cache file containing information about the available loaders. Each loader package will contain a <code>lib/gdk-pixbuf-2.0/2.10.0/loaders.cache</code> file describing the default loaders in <code>gdk-pixbuf</code> package plus the loader contained in the package itself. If you want to use multiple third-party loaders, you will need to create your own cache file manually. Fortunately, this is pretty rare as not many loaders exist.</p> <p><code>gdk-pixbuf</code> contains a setup hook that sets <code>GDK_PIXBUF_MODULE_FILE</code> from dependencies but as mentioned in further section, it is pretty limited. Loaders should propagate this setup hook.</p>"},{"location":"languages-frameworks/gnome.section.html#ssec-gnome-icons","title":"Icons","text":"<p>When an application uses icons, an icon theme should be available in <code>XDG_DATA_DIRS</code> during runtime. The package for the default, icon-less hicolor-icon-theme (should be propagated by every icon theme) contains a setup hook that will pick up icon themes from <code>buildInputs</code> and add their datadirs to <code>XDG_ICON_DIRS</code> environment variable (this is Nixpkgs specific, not actually a XDG standard variable). Unfortunately, relying on that would mean every user has to download the theme included in the package expression no matter their preference. For that reason, we leave the installation of icon theme on the user. If you use one of the desktop environments, you probably already have an icon theme installed.</p> <p>In the rare case you need to use icons from dependencies (e.g. when an app forces an icon theme), you can use the following to pick them up:</p> <pre><code>  buildInputs = [\n    pantheon.elementary-icon-theme\n  ];\n  preFixup = ''\n    gappsWrapperArgs+=(\n      # The icon theme is hardcoded.\n      --prefix XDG_DATA_DIRS : \"$XDG_ICON_DIRS\"\n    )\n  '';\n</code></pre> <p>To avoid costly file system access when locating icons, GTK, as well as Qt, can rely on <code>icon-theme.cache</code> files from the themes' top-level directories. These files are generated using <code>gtk-update-icon-cache</code>, which is expected to be run whenever an icon is added or removed to an icon theme (typically an application icon into <code>hicolor</code> theme) and some programs do indeed run this after icon installation. However, since packages are installed into their own prefix by Nix, this would lead to conflicts. For that reason, <code>gtk3</code> provides a setup hook that will clean the file from installation. Since most applications only ship their own icon that will be loaded on start-up, it should not affect them too much. On the other hand, icon themes are much larger and more widely used so we need to cache them. Because we recommend installing icon themes globally, we will generate the cache files from all packages in a profile using a NixOS module. You can enable the cache generation using <code>gtk.iconCache.enable</code> option if your desktop environment does not already do that.</p>"},{"location":"languages-frameworks/gnome.section.html#ssec-icon-theme-packaging","title":"Packaging icon themes","text":"<p>Icon themes may inherit from other icon themes. The inheritance is specified using the <code>Inherits</code> key in the <code>index.theme</code> file distributed with the icon theme. According to the icon theme specification, icons not provided by the theme are looked for in its parent icon themes. Therefore the parent themes should be installed as dependencies for a more complete experience regarding the icon sets used.</p> <p>The package <code>hicolor-icon-theme</code> provides a setup hook which makes symbolic links for the parent themes into the directory <code>share/icons</code> of the current theme directory in the nix store, making sure they can be found at runtime. For that to work the packages providing parent icon themes should be listed as propagated build dependencies, together with <code>hicolor-icon-theme</code>.</p> <p>Also make sure that <code>icon-theme.cache</code> is installed for each theme provided by the package, and set <code>dontDropIconThemeCache</code> to <code>true</code> so that the cache file is not removed by the <code>gtk3</code> setup hook.</p>"},{"location":"languages-frameworks/gnome.section.html#ssec-gnome-themes","title":"GTK Themes","text":"<p>Previously, a GTK theme needed to be in <code>XDG_DATA_DIRS</code>. This is no longer necessary for most programs since GTK incorporated Adwaita theme. Some programs (for example, those designed for elementary HIG) might require a special theme like <code>pantheon.elementary-gtk-theme</code>.</p>"},{"location":"languages-frameworks/gnome.section.html#ssec-gnome-typelibs","title":"GObject introspection typelibs","text":"<p>GObject introspection allows applications to use C libraries in other languages easily. It does this through <code>typelib</code> files searched in <code>GI_TYPELIB_PATH</code>.</p>"},{"location":"languages-frameworks/gnome.section.html#ssec-gnome-plugins","title":"Various plug-ins","text":"<p>If your application uses GStreamer or Grilo, you should set <code>GST_PLUGIN_SYSTEM_PATH_1_0</code> and <code>GRL_PLUGIN_PATH</code>, respectively.</p>"},{"location":"languages-frameworks/gnome.section.html#ssec-gnome-hooks","title":"Onto <code>wrapGAppsHook</code>","text":"<p>Given the requirements above, the package expression would become messy quickly:</p> <pre><code>preFixup = ''\n  for f in $(find $out/bin/ $out/libexec/ -type f -executable); do\n    wrapProgram \"$f\" \\\n      --prefix GIO_EXTRA_MODULES : \"${getLib dconf}/lib/gio/modules\" \\\n      --prefix XDG_DATA_DIRS : \"$out/share\" \\\n      --prefix XDG_DATA_DIRS : \"$out/share/gsettings-schemas/${name}\" \\\n      --prefix XDG_DATA_DIRS : \"${gsettings-desktop-schemas}/share/gsettings-schemas/${gsettings-desktop-schemas.name}\" \\\n      --prefix XDG_DATA_DIRS : \"${hicolor-icon-theme}/share\" \\\n      --prefix GI_TYPELIB_PATH : \"${lib.makeSearchPath \"lib/girepository-1.0\" [ pango json-glib ]}\"\n  done\n'';\n</code></pre> <p>Fortunately, there is [<code>wrapGAppsHook</code>]{#ssec-gnome-hooks-wrapgappshook}. It works in conjunction with other setup hooks that populate environment variables, and it will then wrap all executables in <code>bin</code> and <code>libexec</code> directories using said variables.</p> <p>For convenience, it also adds <code>dconf.lib</code> for a GIO module implementing a GSettings backend using <code>dconf</code>, <code>gtk3</code> for GSettings schemas, and <code>librsvg</code> for GdkPixbuf loader to the closure. There is also [<code>wrapGAppsHook4</code>]{#ssec-gnome-hooks-wrapgappshook4}, which replaces GTK 3 with GTK 4. And in case you are packaging a program without a graphical interface, you might want to use [<code>wrapGAppsNoGuiHook</code>]{#ssec-gnome-hooks-wrapgappsnoguihook}, which runs the same script as <code>wrapGAppsHook</code> but does not bring <code>gtk3</code> and <code>librsvg</code> into the closure.</p> <ul> <li> <p><code>wrapGAppsHook</code> itself will add the package\u2019s <code>share</code> directory to <code>XDG_DATA_DIRS</code>.</p> </li> <li> <p>[]{#ssec-gnome-hooks-glib} <code>glib</code> setup hook will populate <code>GSETTINGS_SCHEMAS_PATH</code> and then <code>wrapGAppsHook</code> will prepend it to <code>XDG_DATA_DIRS</code>.</p> </li> <li> <p>[]{#ssec-gnome-hooks-gdk-pixbuf} <code>gdk-pixbuf</code> setup hook will populate <code>GDK_PIXBUF_MODULE_FILE</code> with the path to biggest <code>loaders.cache</code> file from the dependencies containing GdkPixbuf loaders. This works fine when there are only two packages containing loaders (<code>gdk-pixbuf</code> and e.g. <code>librsvg</code>) \u2013 it will choose the second one, reasonably expecting that it will be bigger since it describes extra loader in addition to the default ones. But when there are more than two loader packages, this logic will break. One possible solution would be constructing a custom cache file for each package containing a program like <code>services/x11/gdk-pixbuf.nix</code> NixOS module does. <code>wrapGAppsHook</code> copies the <code>GDK_PIXBUF_MODULE_FILE</code> environment variable into the produced wrapper.</p> </li> <li> <p>[]{#ssec-gnome-hooks-gtk-drop-icon-theme-cache} One of <code>gtk3</code>\u2019s setup hooks will remove <code>icon-theme.cache</code> files from package\u2019s icon theme directories to avoid conflicts. Icon theme packages should prevent this with <code>dontDropIconThemeCache = true;</code>.</p> </li> <li> <p>[]{#ssec-gnome-hooks-dconf} <code>dconf.lib</code> is a dependency of <code>wrapGAppsHook</code>, which then also adds it to the <code>GIO_EXTRA_MODULES</code> variable.</p> </li> <li> <p>[]{#ssec-gnome-hooks-hicolor-icon-theme} <code>hicolor-icon-theme</code>\u2019s setup hook will add icon themes to <code>XDG_ICON_DIRS</code>.</p> </li> <li> <p>[]{#ssec-gnome-hooks-gobject-introspection} <code>gobject-introspection</code> setup hook populates <code>GI_TYPELIB_PATH</code> variable with <code>lib/girepository-1.0</code> directories of dependencies, which is then added to wrapper by <code>wrapGAppsHook</code>. It also adds <code>share</code> directories of dependencies to <code>XDG_DATA_DIRS</code>, which is intended to promote GIR files but it also pollutes the closures of packages using <code>wrapGAppsHook</code>.</p> </li> <li> <p>[]{#ssec-gnome-hooks-gst-grl-plugins} Setup hooks of <code>gst_all_1.gstreamer</code> and <code>grilo</code> will populate the <code>GST_PLUGIN_SYSTEM_PATH_1_0</code> and <code>GRL_PLUGIN_PATH</code> variables, respectively, which will then be added to the wrapper by <code>wrapGAppsHook</code>.</p> </li> </ul> <p>You can also pass additional arguments to <code>makeWrapper</code> using <code>gappsWrapperArgs</code> in <code>preFixup</code> hook:</p> <pre><code>preFixup = ''\n  gappsWrapperArgs+=(\n    # Thumbnailers\n    --prefix XDG_DATA_DIRS : \"${gdk-pixbuf}/share\"\n    --prefix XDG_DATA_DIRS : \"${librsvg}/share\"\n    --prefix XDG_DATA_DIRS : \"${shared-mime-info}/share\"\n  )\n'';\n</code></pre>"},{"location":"languages-frameworks/gnome.section.html#ssec-gnome-updating","title":"Updating GNOME packages","text":"<p>Most GNOME package offer <code>updateScript</code>, it is therefore possible to update to latest source tarball by running <code>nix-shell maintainers/scripts/update.nix --argstr package gnome.nautilus</code> or even en masse with <code>nix-shell maintainers/scripts/update.nix --argstr path gnome</code>. Read the package\u2019s <code>NEWS</code> file to see what changed.</p>"},{"location":"languages-frameworks/gnome.section.html#ssec-gnome-common-issues","title":"Frequently encountered issues","text":""},{"location":"languages-frameworks/gnome.section.html#ssec-gnome-common-issues-no-schemas","title":"<code>GLib-GIO-ERROR **: 06:04:50.903: No GSettings schemas are installed on the system</code>","text":"<p>There are no schemas available in <code>XDG_DATA_DIRS</code>. Temporarily add a random package containing schemas like <code>gsettings-desktop-schemas</code> to <code>buildInputs</code>. <code>glib</code> and <code>wrapGAppsHook</code> setup hooks will take care of making the schemas available to application and you will see the actual missing schemas with the next error. Or you can try looking through the source code for the actual schemas used.</p>"},{"location":"languages-frameworks/gnome.section.html#ssec-gnome-common-issues-missing-schema","title":"<code>GLib-GIO-ERROR **: 06:04:50.903: Settings schema \u2018org.gnome.foo\u2019 is not installed</code>","text":"<p>Package is missing some GSettings schemas. You can find out the package containing the schema with <code>nix-locate org.gnome.foo.gschema.xml</code> and let the hooks handle the wrapping as above.</p>"},{"location":"languages-frameworks/gnome.section.html#ssec-gnome-common-issues-double-wrapped","title":"When using <code>wrapGAppsHook</code> with special derivers you can end up with double wrapped binaries.","text":"<p>This is because derivers like <code>python.pkgs.buildPythonApplication</code> or <code>qt5.mkDerivation</code> have setup-hooks automatically added that produce wrappers with makeWrapper. The simplest way to workaround that is to disable the <code>wrapGAppsHook</code> automatic wrapping with <code>dontWrapGApps = true;</code> and pass the arguments it intended to pass to makeWrapper to another.</p> <p>In the case of a Python application it could look like:</p> <pre><code>python3.pkgs.buildPythonApplication {\n  pname = \"gnome-music\";\n  version = \"3.32.2\";\n\n  nativeBuildInputs = [\n    wrapGAppsHook\n    gobject-introspection\n    ...\n  ];\n\n  dontWrapGApps = true;\n\n  # Arguments to be passed to `makeWrapper`, only used by buildPython*\n  preFixup = ''\n    makeWrapperArgs+=(\"''${gappsWrapperArgs[@]}\")\n  '';\n}\n</code></pre> <p>And for a QT app like:</p> <pre><code>mkDerivation {\n  pname = \"calibre\";\n  version = \"3.47.0\";\n\n  nativeBuildInputs = [\n    wrapGAppsHook\n    qmake\n    ...\n  ];\n\n  dontWrapGApps = true;\n\n  # Arguments to be passed to `makeWrapper`, only used by qt5\u2019s mkDerivation\n  preFixup = ''\n    qtWrapperArgs+=(\"''${gappsWrapperArgs[@]}\")\n  '';\n}\n</code></pre>"},{"location":"languages-frameworks/gnome.section.html#ssec-gnome-common-issues-unwrappable-package","title":"I am packaging a project that cannot be wrapped, like a library or GNOME Shell extension.","text":"<p>You can rely on applications depending on the library setting the necessary environment variables but that is often easy to miss. Instead we recommend to patch the paths in the source code whenever possible. Here are some examples:</p> <ul> <li> <p>[]{#ssec-gnome-common-issues-unwrappable-package-gnome-shell-ext} Replacing a <code>GI_TYPELIB_PATH</code> in GNOME Shell extension \u2013 we are using <code>substituteAll</code> to include the path to a typelib into a patch.</p> </li> <li> <p>[]{#ssec-gnome-common-issues-unwrappable-package-gsettings} The following examples are hardcoding GSettings schema paths. To get the schema paths we use the functions</p> </li> <li> <p><code>glib.getSchemaPath</code> Takes a nix package attribute as an argument.</p> </li> <li> <p><code>glib.makeSchemaPath</code> Takes a package output like <code>$out</code> and a derivation name. You should use this if the schemas you need to hardcode are in the same derivation.</p> </li> </ul> <p>[]{#ssec-gnome-common-issues-unwrappable-package-gsettings-vala} Hard-coding GSettings schema path in Vala plug-in (dynamically loaded library) \u2013 here, <code>substituteAll</code> cannot be used since the schema comes from the same package preventing us from pass its path to the function, probably due to a Nix bug.</p> <p>[]{#ssec-gnome-common-issues-unwrappable-package-gsettings-c} Hard-coding GSettings schema path in C library \u2013 nothing special other than using Coccinelle patch to generate the patch itself.</p>"},{"location":"languages-frameworks/gnome.section.html#ssec-gnome-common-issues-weird-location","title":"I need to wrap a binary outside <code>bin</code> and <code>libexec</code> directories.","text":"<p>You can manually trigger the wrapping with <code>wrapGApp</code> in <code>preFixup</code> phase. It takes a path to a program as a first argument; the remaining arguments are passed directly to <code>wrapProgram</code> function.</p>"},{"location":"languages-frameworks/go.section.html","title":"Go","text":""},{"location":"languages-frameworks/go.section.html#ssec-language-go","title":"Building Go modules with <code>buildGoModule</code>","text":"<p>The function <code>buildGoModule</code> builds Go programs managed with Go modules. It builds Go Modules through a two phase build:</p> <ul> <li>An intermediate fetcher derivation called <code>goModules</code>. This derivation will be used to fetch all the dependencies of the Go module.</li> <li>A final derivation will use the output of the intermediate derivation to build the binaries and produce the final output.</li> </ul>"},{"location":"languages-frameworks/go.section.html#buildgomodule-parameters","title":"Attributes of <code>buildGoModule</code>","text":"<p>The <code>buildGoModule</code> function accepts the following parameters in addition to the attributes accepted by both Go builders:</p> <ul> <li><code>vendorHash</code>: is the hash of the output of the intermediate fetcher derivation (the dependencies of the Go modules).</li> </ul> <p><code>vendorHash</code> can be set to <code>null</code>.   In that case, rather than fetching the dependencies, the dependencies already vendored in the <code>vendor</code> directory of the source repo will be used.</p> <p>To avoid updating this field when dependencies change, run <code>go mod vendor</code> in your source repo and set <code>vendorHash = null;</code>.   You can read more about vendoring in the Go documentation.</p> <p>To obtain the actual hash, set <code>vendorHash = lib.fakeHash;</code> and run the build (more details here). - <code>proxyVendor</code>: If <code>true</code>, the intermediate fetcher downloads dependencies from the   Go module proxy (using <code>go mod download</code>) instead of vendoring them. The resulting   module cache is then passed to the final derivation.</p> <p>This is useful if your code depends on C code and <code>go mod tidy</code> does not include the needed sources to build or   if any dependency has case-insensitive conflicts which will produce platform-dependent <code>vendorHash</code> checksums.</p> <p>Defaults to <code>false</code>. - <code>modPostBuild</code>: Shell commands to run after the build of the goModules executes <code>go mod vendor</code>, and before calculating fixed output derivation's <code>vendorHash</code>.   Note that if you change this attribute, you need to update <code>vendorHash</code> attribute. - <code>modRoot</code>: The root directory of the Go module that contains the <code>go.mod</code> file.   Defaults to <code>./</code>, which is the root of <code>src</code>.</p>"},{"location":"languages-frameworks/go.section.html#ex-buildGoModule","title":"Example for <code>buildGoModule</code>","text":"<p>The following is an example expression using <code>buildGoModule</code>:</p> <pre><code>pet = buildGoModule rec {\n  pname = \"pet\";\n  version = \"0.3.4\";\n\n  src = fetchFromGitHub {\n    owner = \"knqyf263\";\n    repo = \"pet\";\n    rev = \"v${version}\";\n    hash = \"sha256-Gjw1dRrgM8D3G7v6WIM2+50r4HmTXvx0Xxme2fH9TlQ=\";\n  };\n\n  vendorHash = \"sha256-ciBIR+a1oaYH+H1PcC8cD8ncfJczk1IiJ8iYNM+R6aA=\";\n\n  meta = with lib; {\n    description = \"Simple command-line snippet manager, written in Go\";\n    homepage = \"https://github.com/knqyf263/pet\";\n    license = licenses.mit;\n    maintainers = with maintainers; [ kalbasit ];\n  };\n}\n</code></pre>"},{"location":"languages-frameworks/go.section.html#ssec-go-legacy","title":"<code>buildGoPackage</code> (legacy)","text":"<p>The function <code>buildGoPackage</code> builds legacy Go programs, not supporting Go modules.</p>"},{"location":"languages-frameworks/go.section.html#example-for-buildgopackage","title":"Example for <code>buildGoPackage</code>","text":"<p>In the following is an example expression using <code>buildGoPackage</code>, the following arguments are of special significance to the function:</p> <ul> <li><code>goPackagePath</code> specifies the package's canonical Go import path.</li> <li><code>goDeps</code> is where the Go dependencies of a Go program are listed as a list of package source identified by Go import path. It could be imported as a separate <code>deps.nix</code> file for readability. The dependency data structure is described below.</li> </ul> <pre><code>deis = buildGoPackage rec {\n  pname = \"deis\";\n  version = \"1.13.0\";\n\n  goPackagePath = \"github.com/deis/deis\";\n\n  src = fetchFromGitHub {\n    owner = \"deis\";\n    repo = \"deis\";\n    rev = \"v${version}\";\n    hash = \"sha256-XCPD4LNWtAd8uz7zyCLRfT8rzxycIUmTACjU03GnaeM=\";\n  };\n\n  goDeps = ./deps.nix;\n}\n</code></pre> <p>The <code>goDeps</code> attribute can be imported from a separate <code>nix</code> file that defines which Go libraries are needed and should be included in <code>GOPATH</code> for <code>buildPhase</code>:</p> <pre><code># deps.nix\n[ # goDeps is a list of Go dependencies.\n  {\n    # goPackagePath specifies Go package import path.\n    goPackagePath = \"gopkg.in/yaml.v2\";\n    fetch = {\n      # `fetch type` that needs to be used to get package source.\n      # If `git` is used there should be `url`, `rev` and `hash` defined next to it.\n      type = \"git\";\n      url = \"https://gopkg.in/yaml.v2\";\n      rev = \"a83829b6f1293c91addabc89d0571c246397bbf4\";\n      hash = \"sha256-EMrdy0M0tNuOcITaTAmT5/dPSKPXwHDKCXFpkGbVjdQ=\";\n    };\n  }\n  {\n    goPackagePath = \"github.com/docopt/docopt-go\";\n    fetch = {\n      type = \"git\";\n      url = \"https://github.com/docopt/docopt-go\";\n      rev = \"784ddc588536785e7299f7272f39101f7faccc3f\";\n      hash = \"sha256-Uo89zjE+v3R7zzOq/gbQOHj3SMYt2W1nDHS7RCUin3M=\";\n    };\n  }\n]\n</code></pre> <p>To extract dependency information from a Go package in automated way use go2nix (deprecated). It can produce complete derivation and <code>goDeps</code> file for Go programs.</p> <p>You may use Go packages installed into the active Nix profiles by adding the following to your ~/.bashrc:</p> <pre><code>for p in $NIX_PROFILES; do\n    GOPATH=\"$p/share/go:$GOPATH\"\ndone\n</code></pre>"},{"location":"languages-frameworks/go.section.html#ssec-go-common-attributes","title":"Attributes used by both builders","text":"<p>Many attributes controlling the build phase are respected by both <code>buildGoModule</code> and <code>buildGoPackage</code>. Note that <code>buildGoModule</code> reads the following attributes also when building the <code>vendor/</code> goModules fixed output derivation as well:</p> <ul> <li><code>sourceRoot</code></li> <li><code>prePatch</code></li> <li><code>patches</code></li> <li><code>patchFlags</code></li> <li><code>postPatch</code></li> <li><code>preBuild</code></li> </ul> <p>To control test execution of the build derivation, the following attributes are of interest:</p> <ul> <li><code>checkInputs</code></li> <li><code>preCheck</code></li> <li><code>checkFlags</code></li> </ul> <p>In addition to the above attributes, and the many more variables respected also by <code>stdenv.mkDerivation</code>, both <code>buildGoModule</code> and <code>buildGoPackage</code> respect Go-specific attributes that tweak them to behave slightly differently:</p>"},{"location":"languages-frameworks/go.section.html#var-go-ldflags","title":"<code>ldflags</code>","text":"<p>A string list of flags to pass to the Go linker tool via the <code>-ldflags</code> argument of <code>go build</code>. Possible values can be retrieved by running <code>go tool link --help</code>. The most common use case for this argument is to make the resulting executable aware of its own version by injecting the value of string variable using the <code>-X</code> flag. For example:</p> <pre><code>  ldflags = [\n    \"-X main.Version=${version}\"\n    \"-X main.Commit=${version}\"\n  ];\n</code></pre>"},{"location":"languages-frameworks/go.section.html#var-go-tags","title":"<code>tags</code>","text":"<p>A string list of Go build tags (also called build constraints) that are passed via the <code>-tags</code> argument of <code>go build</code>.  These constraints control whether Go files from the source should be included in the build. For example:</p> <pre><code>  tags = [\n    \"production\"\n    \"sqlite\"\n  ];\n</code></pre> <p>Tags can also be set conditionally:</p> <pre><code>  tags = [ \"production\" ] ++ lib.optionals withSqlite [ \"sqlite\" ];\n</code></pre>"},{"location":"languages-frameworks/go.section.html#var-go-deleteVendor","title":"<code>deleteVendor</code>","text":"<p>If set to <code>true</code>, removes the pre-existing vendor directory. This should only be used if the dependencies included in the vendor folder are broken or incomplete.</p>"},{"location":"languages-frameworks/go.section.html#var-go-subPackages","title":"<code>subPackages</code>","text":"<p>Specified as a string or list of strings. Limits the builder from building child packages that have not been listed. If <code>subPackages</code> is not specified, all child packages will be built.</p> <p>Many Go projects keep the main package in a <code>cmd</code> directory. Following example could be used to only build the example-cli and example-server binaries:</p> <pre><code>subPackages = [\n  \"cmd/example-cli\"\n  \"cmd/example-server\"\n];\n</code></pre>"},{"location":"languages-frameworks/go.section.html#var-go-excludedPackages","title":"<code>excludedPackages</code>","text":"<p>Specified as a string or list of strings. Causes the builder to skip building child packages that match any of the provided values.</p>"},{"location":"languages-frameworks/go.section.html#var-go-CGO_ENABLED","title":"<code>CGO_ENABLED</code>","text":"<p>When set to <code>0</code>, the cgo command is disabled. As consequence, the build program can't link against C libraries anymore, and the resulting binary is statically linked.</p> <p>When building with CGO enabled, Go will likely link some packages from the Go standard library against C libraries, even when the target code does not explicitly call into C dependencies. With <code>CGO_ENABLED = 0;</code>, Go will always use the Go native implementation of these internal packages. For reference see net and os/user packages. Notice that the decision whether these packages should use native Go implementation or not can also be controlled on a per package level using build tags (<code>tags</code>). In case CGO is disabled, these tags have no additional effect.</p> <p>When a Go program depends on C libraries, place those dependencies in <code>buildInputs</code>:</p> <pre><code>  buildInputs = [\n    libvirt\n    libxml2\n  ];\n</code></pre> <p><code>CGO_ENABLED</code> defaults to <code>1</code>.</p>"},{"location":"languages-frameworks/go.section.html#var-go-enableParallelBuilding","title":"<code>enableParallelBuilding</code>","text":"<p>Whether builds and tests should run in parallel.</p> <p>Defaults to <code>true</code>.</p>"},{"location":"languages-frameworks/go.section.html#var-go-allowGoReference","title":"<code>allowGoReference</code>","text":"<p>Whether the build result should be allowed to contain references to the Go tool chain. This might be needed for programs that are coupled with the compiler, but shouldn't be set without a good reason.</p> <p>Defaults to <code>false</code></p>"},{"location":"languages-frameworks/go.section.html#ssec-go-environment","title":"Controlling the Go environment","text":"<p>The Go build can be further tweaked by setting environment variables. In most cases, this isn't needed. Possible values can be found in the Go documentation of accepted environment variables. Notice that some of these flags are set by the builder itself and should not be set explicitly. If in doubt, grep the implementation of the builder.</p>"},{"location":"languages-frameworks/go.section.html#ssec-skip-go-tests","title":"Skipping tests","text":"<p><code>buildGoModule</code> runs tests by default. Failing tests can be disabled using the <code>checkFlags</code> parameter. This is done with the <code>-skip</code> or <code>-run</code> flags of the <code>go test</code> command.</p> <p>For example, only a selection of tests could be run with:</p> <pre><code>  # -run and -skip accept regular expressions\n  checkFlags = [\n    \"-run=^Test(Simple|Fast)$\"\n  ];\n</code></pre> <p>If a larger amount of tests should be skipped, the following pattern can be used:</p> <pre><code>  checkFlags =\n    let\n      # Skip tests that require network access\n      skippedTests = [\n        \"TestNetwork\"\n        \"TestDatabase/with_mysql\" # exclude only the subtest\n        \"TestIntegration\"\n      ];\n    in\n    [ \"-skip=^${builtins.concatStringsSep \"$|^\" skippedTests}$\" ];\n</code></pre> <p>To disable tests altogether, set <code>doCheck = false;</code>. <code>buildGoPackage</code> does not execute tests by default.</p>"},{"location":"languages-frameworks/haskell.section.html","title":"Haskell","text":"<p>The Haskell infrastructure in Nixpkgs has two main purposes: The primary purpose is to provide a Haskell compiler and build tools as well as infrastructure for packaging Haskell-based packages.</p> <p>The secondary purpose is to provide support for Haskell development environments including prebuilt Haskell libraries. However, in this area sacrifices have been made due to self-imposed restrictions in Nixpkgs, to lessen the maintenance effort and to improve performance. (More details in the subsection Limitations.)</p>"},{"location":"languages-frameworks/haskell.section.html#haskell-available-packages","title":"Available packages","text":"<p>The compiler and most build tools are exposed at the top level:</p> <ul> <li><code>ghc</code> is the default version of GHC</li> <li>Language specific tools: <code>cabal-install</code>, <code>stack</code>, <code>hpack</code>, \u2026</li> </ul> <p>Many \u201cnormal\u201d user facing packages written in Haskell, like <code>niv</code> or <code>cachix</code>, are also exposed at the top level, and there is nothing Haskell specific to installing and using them.</p> <p>All of these packages are originally defined in the <code>haskellPackages</code> package set and are re-exposed with a reduced dependency closure for convenience. (see <code>justStaticExecutables</code> or <code>separateBinOutput</code> below)</p> <p>The <code>haskellPackages</code> set includes at least one version of every package from Hackage as well as some manually injected packages. This amounts to a lot of packages, so it is hidden from <code>nix-env -qa</code> by default for performance reasons. You can still list all packages in the set like this:</p> <p><pre><code>$ nix-env -f '&lt;nixpkgs&gt;' -qaP -A haskellPackages\nhaskellPackages.a50                                                         a50-0.5\nhaskellPackages.AAI                                                         AAI-0.2.0.1\nhaskellPackages.aasam                                                       aasam-0.2.0.0\nhaskellPackages.abacate                                                     abacate-0.0.0.0\nhaskellPackages.abc-puzzle                                                  abc-puzzle-0.2.1\n\u2026\n</code></pre> Also, the <code>haskellPackages</code> set is included on search.nixos.org.</p> <p>The attribute names in <code>haskellPackages</code> always correspond with their name on Hackage. Since Hackage allows names that are not valid Nix without escaping, you need to take care when handling attribute names like <code>3dmodels</code>.</p> <p>For packages that are part of Stackage (a curated set of known to be compatible packages), we use the version prescribed by a Stackage snapshot (usually the current LTS one) as the default version. For all other packages we use the latest version from Hackage (the repository of basically all open source Haskell packages). See below for a few more details on this.</p> <p>Roughly half of the 16K packages contained in <code>haskellPackages</code> don\u2019t actually build and are marked as broken semi-automatically. Most of those packages are deprecated or unmaintained, but sometimes packages that should build, do not build. Very often fixing them is not a lot of work.</p> <p><code>haskellPackages</code> is built with our default compiler, but we also provide other releases of GHC and package sets built with them. You can list all available compilers like this:</p> <pre><code>$ nix-env -f '&lt;nixpkgs&gt;' -qaP -A haskell.compiler\nhaskell.compiler.ghc810                  ghc-8.10.7\nhaskell.compiler.ghc90                   ghc-9.0.2\nhaskell.compiler.ghc925                  ghc-9.2.5\nhaskell.compiler.ghc926                  ghc-9.2.6\nhaskell.compiler.ghc927                  ghc-9.2.7\nhaskell.compiler.ghc92                   ghc-9.2.8\nhaskell.compiler.ghc945                  ghc-9.4.5\nhaskell.compiler.ghc946                  ghc-9.4.6\nhaskell.compiler.ghc947                  ghc-9.4.7\nhaskell.compiler.ghc94                   ghc-9.4.8\nhaskell.compiler.ghc963                  ghc-9.6.3\nhaskell.compiler.ghc96                   ghc-9.6.4\nhaskell.compiler.ghc98                   ghc-9.8.1\nhaskell.compiler.ghcHEAD                 ghc-9.9.20231121\nhaskell.compiler.ghc8107Binary           ghc-binary-8.10.7\nhaskell.compiler.ghc865Binary            ghc-binary-8.6.5\nhaskell.compiler.ghc924Binary            ghc-binary-9.2.4\nhaskell.compiler.integer-simple.ghc8107  ghc-integer-simple-8.10.7\nhaskell.compiler.integer-simple.ghc810   ghc-integer-simple-8.10.7\nhaskell.compiler.native-bignum.ghc90     ghc-native-bignum-9.0.2\nhaskell.compiler.native-bignum.ghc902    ghc-native-bignum-9.0.2\nhaskell.compiler.native-bignum.ghc925    ghc-native-bignum-9.2.5\nhaskell.compiler.native-bignum.ghc926    ghc-native-bignum-9.2.6\nhaskell.compiler.native-bignum.ghc927    ghc-native-bignum-9.2.7\nhaskell.compiler.native-bignum.ghc92     ghc-native-bignum-9.2.8\nhaskell.compiler.native-bignum.ghc928    ghc-native-bignum-9.2.8\nhaskell.compiler.native-bignum.ghc945    ghc-native-bignum-9.4.5\nhaskell.compiler.native-bignum.ghc946    ghc-native-bignum-9.4.6\nhaskell.compiler.native-bignum.ghc947    ghc-native-bignum-9.4.7\nhaskell.compiler.native-bignum.ghc94     ghc-native-bignum-9.4.8\nhaskell.compiler.native-bignum.ghc948    ghc-native-bignum-9.4.8\nhaskell.compiler.native-bignum.ghc963    ghc-native-bignum-9.6.3\nhaskell.compiler.native-bignum.ghc96     ghc-native-bignum-9.6.4\nhaskell.compiler.native-bignum.ghc964    ghc-native-bignum-9.6.4\nhaskell.compiler.native-bignum.ghc98     ghc-native-bignum-9.8.1\nhaskell.compiler.native-bignum.ghc981    ghc-native-bignum-9.8.1\nhaskell.compiler.native-bignum.ghcHEAD   ghc-native-bignum-9.9.20231121\nhaskell.compiler.ghcjs                   ghcjs-8.10.7\n</code></pre> <p>Each of those compiler versions has a corresponding attribute set built using it. However, the non-standard package sets are not tested regularly and, as a result, contain fewer working packages. The corresponding package set for GHC 9.4.5 is <code>haskell.packages.ghc945</code>. In fact <code>haskellPackages</code> is just an alias for <code>haskell.packages.ghc927</code>:</p> <pre><code>$ nix-env -f '&lt;nixpkgs&gt;' -qaP -A haskell.packages.ghc927\nhaskell.packages.ghc927.a50                                                         a50-0.5\nhaskell.packages.ghc927.AAI                                                         AAI-0.2.0.1\nhaskell.packages.ghc927.aasam                                                       aasam-0.2.0.0\nhaskell.packages.ghc927.abacate                                                     abacate-0.0.0.0\nhaskell.packages.ghc927.abc-puzzle                                                  abc-puzzle-0.2.1\n\u2026\n</code></pre> <p>Every package set also re-exposes the GHC used to build its packages as <code>haskell.packages.*.ghc</code>.</p>"},{"location":"languages-frameworks/haskell.section.html#haskell-available-versions","title":"Available package versions","text":"<p>We aim for a \u201cblessed\u201d package set which only contains one version of each package, like Stackage, which is a curated set of known to be compatible packages. We use the version information from Stackage snapshots and extend it with more packages. Normally in Nixpkgs the number of building Haskell packages is roughly two to three times the size of Stackage. For choosing the version to use for a certain package we use the following rules:</p> <ol> <li>By default, for <code>haskellPackages.foo</code> is the newest version of the package <code>foo</code> found on Hackage, which is the central registry of all open source Haskell packages. Nixpkgs contains a reference to a pinned Hackage snapshot, thus we use the state of Hackage as of the last time we updated this pin.</li> <li>If the Stackage snapshot that we use (usually the newest LTS snapshot) contains a package, we use instead the version in the Stackage snapshot as default version for that package.</li> <li>For some packages, which are not on Stackage, we have if necessary manual overrides to set the default version to a version older than the newest on Hackage.</li> <li>For all packages, for which the newest Hackage version is not the default version, there will also be a <code>haskellPackages.foo_x_y_z</code> package with the newest version. The <code>x_y_z</code> part encodes the version with dots replaced by underscores. When the newest version changes by a new release to Hackage the old package will disappear under that name and be replaced by a newer one under the name with the new version. The package name including the version will also disappear when the default version e.g. from Stackage catches up with the newest version from Hackage. E.g. if <code>haskellPackages.foo</code> gets updated from 1.0.0 to 1.1.0 the package <code>haskellPackages.foo_1_1_0</code> becomes obsolete and gets dropped.</li> <li>For some packages, we also manually add other <code>haskellPackages.foo_x_y_z</code> versions, if they are required for a certain build.</li> </ol> <p>Relying on <code>haskellPackages.foo_x_y_z</code> attributes in derivations outside nixpkgs is discouraged because they may change or disappear with every package set update.</p> <p>All <code>haskell.packages.*</code> package sets use the same package descriptions and the same sets of versions by default. There are however GHC version specific override <code>.nix</code> files to loosen this a bit.</p>"},{"location":"languages-frameworks/haskell.section.html#haskell-dependency-resolution","title":"Dependency resolution","text":"<p>Normally when you build Haskell packages with <code>cabal-install</code>, <code>cabal-install</code> does dependency resolution. It will look at all Haskell package versions known on Hackage and tries to pick for every (transitive) dependency of your build exactly one version. Those versions need to satisfy all the version constraints given in the <code>.cabal</code> file of your package and all its dependencies.</p> <p>The Haskell builder in nixpkgs does no such thing. It will take as input packages with names off the desired dependencies and just check whether they fulfill the version bounds and fail if they don\u2019t (by default, see <code>jailbreak</code> to circumvent this).</p> <p>The <code>haskellPackages.callPackage</code> function does the package resolution. It will, e.g., use <code>haskellPackages.aeson</code>which has the default version as described above for a package input of name <code>aeson</code>. (More general: <code>&lt;packages&gt;.callPackage f</code> will call <code>f</code> with named inputs provided from the package set <code>&lt;packages&gt;</code>.) While this is the default behavior, it is possible to override the dependencies for a specific package, see <code>override</code> and <code>overrideScope</code>.</p>"},{"location":"languages-frameworks/haskell.section.html#haskell-limitations","title":"Limitations","text":"<p>Our main objective with <code>haskellPackages</code> is to package Haskell software in nixpkgs. This entails some limitations, partially due to self-imposed restrictions of nixpkgs, partially in the name of maintainability:</p> <ul> <li>Only the packages built with the default compiler see extensive testing of the   whole package set. For other GHC versions only a few essential packages are   tested and cached.</li> <li>As described above we only build one version of most packages.</li> </ul> <p>The experience using an older or newer packaged compiler or using different versions may be worse, because builds will not be cached on <code>cache.nixos.org</code> or may fail.</p> <p>Thus, to get the best experience, make sure that your project can be compiled using the default compiler of nixpkgs and recent versions of its dependencies.</p> <p>A result of this setup is, that getting a valid build plan for a given package can sometimes be quite painful, and in fact this is where most of the maintenance work for <code>haskellPackages</code> is required. Besides that, it is not possible to get the dependencies of a legacy project from nixpkgs or to use a specific stack solver for compiling a project.</p> <p>Even though we couldn\u2019t use them directly in nixpkgs, it would be desirable to have tooling to generate working Nix package sets from build plans generated by <code>cabal-install</code> or a specific Stackage snapshot via import-from-derivation. Sadly we currently don\u2019t have tooling for this. For this you might be interested in the alternative haskell.nix framework, which, be warned, is completely incompatible with packages from <code>haskellPackages</code>.</p>"},{"location":"languages-frameworks/haskell.section.html#haskell-mkderivation","title":"<code>haskellPackages.mkDerivation</code>","text":"<p>Every haskell package set has its own haskell-aware <code>mkDerivation</code> which is used to build its packages. Generally you won't have to interact with this builder since cabal2nix can generate packages using it for an arbitrary cabal package definition. Still it is useful to know the parameters it takes when you need to override a generated Nix expression.</p> <p><code>haskellPackages.mkDerivation</code> is a wrapper around <code>stdenv.mkDerivation</code> which re-defines the default phases to be haskell aware and handles dependency specification, test suites, benchmarks etc. by compiling and invoking the package's <code>Setup.hs</code>. It does not use or invoke the <code>cabal-install</code> binary, but uses the underlying <code>Cabal</code> library instead.</p>"},{"location":"languages-frameworks/haskell.section.html#haskell-derivation-args","title":"General arguments","text":"<p><code>pname</code> : Package name, assumed to be the same as on Hackage (if applicable)</p> <p><code>version</code> : Packaged version, assumed to be the same as on Hackage (if applicable)</p> <p><code>src</code> : Source of the package. If omitted, fetch package corresponding to <code>pname</code> and <code>version</code> from Hackage.</p> <p><code>sha256</code> : Hash to use for the default case of <code>src</code>.</p> <p><code>revision</code> : Revision number of the updated cabal file to fetch from Hackage. If <code>null</code> (which is the default value), the one included in <code>src</code> is used.</p> <p><code>editedCabalFile</code> : <code>sha256</code> hash of the cabal file identified by <code>revision</code> or <code>null</code>.</p> <p><code>configureFlags</code> : Extra flags passed when executing the <code>configure</code> command of <code>Setup.hs</code>.</p> <p><code>buildFlags</code> : Extra flags passed when executing the <code>build</code> command of <code>Setup.hs</code>.</p> <p><code>haddockFlags</code> : Extra flags passed to <code>Setup.hs haddock</code> when building the documentation.</p> <p><code>doCheck</code> : Whether to execute the package's test suite if it has one. Defaults to <code>true</code> unless cross-compiling.</p> <p><code>doBenchmark</code> : Whether to execute the package's benchmark if it has one. Defaults to <code>false</code>.</p> <p><code>doHoogle</code> : Whether to generate an index file for hoogle as part of <code>haddockPhase</code> by passing the <code>--hoogle</code> option. Defaults to <code>true</code>.</p> <p><code>doHaddockQuickjump</code> : Whether to generate an index for interactive navigation of the HTML documentation. Defaults to <code>true</code> if supported.</p> <p><code>doInstallIntermediates</code> : Whether to install intermediate build products (files written to <code>dist/build</code> by GHC during the build process). With <code>enableSeparateIntermediatesOutput</code>, these files are instead installed to a separate <code>intermediates</code> output. The output can then be passed into a future build of the same package with the <code>previousIntermediates</code> argument to support incremental builds. See \u201cIncremental builds\u201d for more information. Defaults to <code>false</code>.</p> <p><code>enableLibraryProfiling</code> : Whether to enable profiling for libraries contained in the package. Enabled by default if supported.</p> <p><code>enableExecutableProfiling</code> : Whether to enable profiling for executables contained in the package. Disabled by default.</p> <p><code>profilingDetail</code> : Profiling detail level to set. Defaults to <code>exported-functions</code>.</p> <p><code>enableSharedExecutables</code> : Whether to link executables dynamically. By default, executables are linked statically.</p> <p><code>enableSharedLibraries</code> : Whether to build shared Haskell libraries. This is enabled by default unless we are using <code>pkgsStatic</code> or shared libraries have been disabled in GHC.</p> <p><code>enableStaticLibraries</code> : Whether to build static libraries. Enabled by default if supported.</p> <p><code>enableDeadCodeElimination</code> : Whether to enable linker based dead code elimination in GHC. Enabled by default if supported.</p> <p><code>enableHsc2hsViaAsm</code> : Whether to pass <code>--via-asm</code> to <code>hsc2hs</code>. Enabled by default only on Windows.</p> <p><code>hyperlinkSource</code> : Whether to render the source as well as part of the haddock documentation by passing the <code>--hyperlinked-source</code> flag. Defaults to <code>true</code>.</p> <p><code>isExecutable</code> : Whether the package contains an executable.</p> <p><code>isLibrary</code> : Whether the package contains a library.</p> <p><code>jailbreak</code> : Whether to execute jailbreak-cabal before <code>configurePhase</code> to lift any version constraints in the cabal file. Note that this can't lift version bounds if they are conditional, i.e. if a dependency is hidden behind a flag.</p> <p><code>enableParallelBuilding</code> : Whether to use the <code>-j</code> flag to make GHC/Cabal start multiple jobs in parallel.</p> <p><code>maxBuildCores</code> : Upper limit of jobs to use in parallel for compilation regardless of <code>$NIX_BUILD_CORES</code>. Defaults to 16 as Haskell compilation with GHC currently sees a performance regression if too many parallel jobs are used.</p> <p><code>doCoverage</code> : Whether to generate and install files needed for HPC. Defaults to <code>false</code>.</p> <p><code>doHaddock</code> : Whether to build (HTML) documentation using haddock. Defaults to <code>true</code> if supported.</p> <p><code>testTarget</code> : Name of the test suite to build and run. If unset, all test suites will be executed.</p> <p><code>preCompileBuildDriver</code> : Shell code to run before compiling <code>Setup.hs</code>.</p> <p><code>postCompileBuildDriver</code> : Shell code to run after compiling <code>Setup.hs</code>.</p> <p><code>preHaddock</code> : Shell code to run before building documentation using haddock.</p> <p><code>postHaddock</code> : Shell code to run after building documentation using haddock.</p> <p><code>coreSetup</code> : Whether to only allow core libraries to be used while building <code>Setup.hs</code>. Defaults to <code>false</code>.</p> <p><code>useCpphs</code> : Whether to enable the cpphs preprocessor. Defaults to <code>false</code>.</p> <p><code>enableSeparateBinOutput</code> : Whether to install executables to a separate <code>bin</code> output. Defaults to <code>false</code>.</p> <p><code>enableSeparateDataOutput</code> : Whether to install data files shipped with the package to a separate <code>data</code> output. Defaults to <code>false</code>.</p> <p><code>enableSeparateDocOutput</code> : Whether to install documentation to a separate <code>doc</code> output. Is automatically enabled if <code>doHaddock</code> is <code>true</code>.</p> <p><code>enableSeparateIntermediatesOutput</code> : When <code>doInstallIntermediates</code> is true, whether to install intermediate build products to a separate <code>intermediates</code> output. See \u201cIncremental builds\u201d for more information. Defaults to <code>false</code>.</p> <p><code>allowInconsistentDependencies</code> : If enabled, allow multiple versions of the same Haskell package in the dependency tree at configure time. Often in such a situation compilation would later fail because of type mismatches. Defaults to <code>false</code>.</p> <p><code>enableLibraryForGhci</code> : Build and install a special object file for GHCi. This improves performance when loading the library in the REPL, but requires extra build time and disk space. Defaults to <code>false</code>.</p> <p><code>previousIntermediates</code> : If non-null, intermediate build artifacts are copied from this input to <code>dist/build</code> before performing compiling. See \u201cIncremental builds\u201d for more information. Defaults to <code>null</code>.</p> <p><code>buildTarget</code> : Name of the executable or library to build and install. If unset, all available targets are built and installed.</p>"},{"location":"languages-frameworks/haskell.section.html#haskell-derivation-deps","title":"Specifying dependencies","text":"<p>Since <code>haskellPackages.mkDerivation</code> is intended to be generated from cabal files, it reflects cabal's way of specifying dependencies. For one, dependencies are grouped by what part of the package they belong to. This helps to reduce the dependency closure of a derivation, for example benchmark dependencies are not included if <code>doBenchmark == false</code>.</p> <p><code>setup*Depends</code> : dependencies necessary to compile <code>Setup.hs</code></p> <p><code>library*Depends</code> : dependencies of a library contained in the package</p> <p><code>executable*Depends</code> : dependencies of an executable contained in the package</p> <p><code>test*Depends</code> : dependencies of a test suite contained in the package</p> <p><code>benchmark*Depends</code> : dependencies of a benchmark contained in the package</p> <p>The other categorization relates to the way the package depends on the dependency:</p> <p><code>*ToolDepends</code> : Tools we need to run as part of the build process. They are added to the derivation's <code>nativeBuildInputs</code>.</p> <p><code>*HaskellDepends</code> : Haskell libraries the package depends on. They are added to <code>propagatedBuildInputs</code>.</p> <p><code>*SystemDepends</code> : Non-Haskell libraries the package depends on. They are added to <code>buildInputs</code></p> <p><code>*PkgconfigDepends</code> : <code>*SystemDepends</code> which are discovered using <code>pkg-config</code>. They are added to <code>buildInputs</code> and it is additionally ensured that <code>pkg-config</code> is available at build time.</p> <p><code>*FrameworkDepends</code> : Apple SDK Framework which the package depends on when compiling it on Darwin.</p> <p>Using these two distinctions, you should be able to categorize most of the dependency specifications that are available: <code>benchmarkFrameworkDepends</code>, <code>benchmarkHaskellDepends</code>, <code>benchmarkPkgconfigDepends</code>, <code>benchmarkSystemDepends</code>, <code>benchmarkToolDepends</code>, <code>executableFrameworkDepends</code>, <code>executableHaskellDepends</code>, <code>executablePkgconfigDepends</code>, <code>executableSystemDepends</code>, <code>executableToolDepends</code>, <code>libraryFrameworkDepends</code>, <code>libraryHaskellDepends</code>, <code>libraryPkgconfigDepends</code>, <code>librarySystemDepends</code>, <code>libraryToolDepends</code>, <code>setupHaskellDepends</code>, <code>testFrameworkDepends</code>, <code>testHaskellDepends</code>, <code>testPkgconfigDepends</code>, <code>testSystemDepends</code> and <code>testToolDepends</code>.</p> <p>That only leaves the following extra ways for specifying dependencies:</p> <p><code>buildDepends</code> : Allows specifying Haskell dependencies which are added to <code>propagatedBuildInputs</code> unconditionally.</p> <p><code>buildTools</code> : Like <code>*ToolDepends</code>, but are added to <code>nativeBuildInputs</code> unconditionally.</p> <p><code>extraLibraries</code> : Like <code>*SystemDepends</code>, but are added to <code>buildInputs</code> unconditionally.</p> <p><code>pkg-configDepends</code> : Like <code>*PkgconfigDepends</code>, but are added to <code>buildInputs</code> unconditionally.</p> <p><code>testDepends</code> : Deprecated, use either <code>testHaskellDepends</code> or <code>testSystemDepends</code>.</p> <p><code>benchmarkDepends</code> : Deprecated, use either <code>benchmarkHaskellDepends</code> or <code>benchmarkSystemDepends</code>.</p> <p>The dependency specification methods in this list which are unconditional are especially useful when writing overrides when you want to make sure that they are definitely included. However, it is recommended to use the more accurate ones listed above when possible.</p>"},{"location":"languages-frameworks/haskell.section.html#haskell-derivation-meta","title":"Meta attributes","text":"<p><code>haskellPackages.mkDerivation</code> accepts the following attributes as direct arguments which are transparently set in <code>meta</code> of the resulting derivation. See the Meta-attributes section for their documentation.</p> <ul> <li>These attributes are populated with a default value if omitted:<ul> <li><code>homepage</code>: defaults to the Hackage page for <code>pname</code>.</li> <li><code>platforms</code>: defaults to <code>lib.platforms.all</code> (since GHC can cross-compile)</li> </ul> </li> <li>These attributes are only set if given:<ul> <li><code>description</code></li> <li><code>license</code></li> <li><code>changelog</code></li> <li><code>maintainers</code></li> <li><code>broken</code></li> <li><code>hydraPlatforms</code></li> </ul> </li> </ul>"},{"location":"languages-frameworks/haskell.section.html#haskell-incremental-builds","title":"Incremental builds","text":"<p><code>haskellPackages.mkDerivation</code> supports incremental builds for GHC 9.4 and newer with the <code>doInstallIntermediates</code>, <code>enableSeparateIntermediatesOutput</code>, and <code>previousIntermediates</code> arguments.</p> <p>The basic idea is to first perform a full build of the package in question, save its intermediate build products for later, and then copy those build products into the build directory of an incremental build performed later. Then, GHC will use those build artifacts to avoid recompiling unchanged modules.</p> <p>For more detail on how to store and use incremental build products, see Gabriella Gonzalez\u2019 blog post \u201cNixpkgs support for incremental Haskell builds\u201d. motivation behind this feature.</p> <p>An incremental build for the <code>turtle</code> package can be performed like so:</p> <pre><code>let\n  pkgs = import &lt;nixpkgs&gt; {};\n  inherit (pkgs) haskell;\n  inherit (haskell.lib.compose) overrideCabal;\n\n  # Incremental builds work with GHC &gt;=9.4.\n  turtle = haskell.packages.ghc944.turtle;\n\n  # This will do a full build of `turtle`, while writing the intermediate build products\n  # (compiled modules, etc.) to the `intermediates` output.\n  turtle-full-build-with-incremental-output = overrideCabal (drv: {\n    doInstallIntermediates = true;\n    enableSeparateIntermediatesOutput = true;\n  }) turtle;\n\n  # This will do an incremental build of `turtle` by copying the previously\n  # compiled modules and intermediate build products into the source tree\n  # before running the build.\n  #\n  # GHC will then naturally pick up and reuse these products, making this build\n  # complete much more quickly than the previous one.\n  turtle-incremental-build = overrideCabal (drv: {\n    previousIntermediates = turtle-full-build-with-incremental-output.intermediates;\n  }) turtle;\nin\n  turtle-incremental-build\n</code></pre>"},{"location":"languages-frameworks/haskell.section.html#haskell-development-environments","title":"Development environments","text":"<p>In addition to building and installing Haskell software, nixpkgs can also provide development environments for Haskell projects. This has the obvious advantage that you benefit from <code>cache.nixos.org</code> and no longer need to compile all project dependencies yourself. While it is often very useful, this is not the primary use case of our package set. Have a look at the section available package versions to learn which versions of packages we provide and the section limitations, to judge whether a <code>haskellPackages</code> based development environment for your project is feasible.</p> <p>By default, every derivation built using <code>haskellPackages.mkDerivation</code> exposes an environment suitable for building it interactively as the <code>env</code> attribute. For example, if you have a local checkout of <code>random</code>, you can enter a development environment for it like this (if the dependencies in the development and packaged version match):</p> <pre><code>$ cd ~/src/random\n$ nix-shell -A haskellPackages.random.env '&lt;nixpkgs&gt;'\n[nix-shell:~/src/random]$ ghc-pkg list\n/nix/store/a8hhl54xlzfizrhcf03c1l3f6l9l8qwv-ghc-9.2.4-with-packages/lib/ghc-9.2.4/package.conf.d\n    Cabal-3.6.3.0\n    array-0.5.4.0\n    base-4.16.3.0\n    binary-0.8.9.0\n    \u2026\n    ghc-9.2.4\n    \u2026\n</code></pre> <p>As you can see, the environment contains a GHC which is set up so it finds all dependencies of <code>random</code>. Note that this environment does not mirror the environment used to build the package, but is intended as a convenient tool for development and simple debugging. <code>env</code> relies on the <code>ghcWithPackages</code> wrapper which automatically injects a pre-populated package-db into every GHC invocation. In contrast, using <code>nix-shell -A haskellPackages.random</code> will not result in an environment in which the dependencies are in GHCs package database. Instead, the Haskell builder will pass in all dependencies explicitly via configure flags.</p> <p><code>env</code> mirrors the normal derivation environment in one aspect: It does not include familiar development tools like <code>cabal-install</code>, since we rely on plain <code>Setup.hs</code> to build all packages. However, <code>cabal-install</code> will work as expected if in <code>PATH</code> (e.g. when installed globally and using a <code>nix-shell</code> without <code>--pure</code>). A declarative and pure way of adding arbitrary development tools is provided via <code>shellFor</code>.</p> <p>When using <code>cabal-install</code> for dependency resolution you need to be a bit careful to achieve build purity. <code>cabal-install</code> will find and use all dependencies installed from the packages <code>env</code> via Nix, but it will also consult Hackage to potentially download and compile dependencies if it can\u2019t find a valid build plan locally. To prevent this you can either never run <code>cabal update</code>, remove the cabal database from your <code>~/.cabal</code> folder or run <code>cabal</code> with <code>--offline</code>. Note though, that for some usecases <code>cabal2nix</code> needs the local Hackage db.</p> <p>Often you won't work on a package that is already part of <code>haskellPackages</code> or Hackage, so we first need to write a Nix expression to obtain the development environment from. Luckily, we can generate one very easily from an already existing cabal file using <code>cabal2nix</code>:</p> <pre><code>$ ls\nmy-project.cabal src \u2026\n$ cabal2nix ./. &gt; my-project.nix\n</code></pre> <p>The generated Nix expression evaluates to a function ready to be <code>callPackage</code>-ed. For now, we can add a minimal <code>default.nix</code> which does just that:</p> <pre><code># Retrieve nixpkgs impurely from NIX_PATH for now, you can pin it instead, of course.\n{ pkgs ? import &lt;nixpkgs&gt; {} }:\n\n# use the nixpkgs default haskell package set\npkgs.haskellPackages.callPackage ./my-project.nix { }\n</code></pre> <p>Using <code>nix-build default.nix</code> we can now build our project, but we can also enter a shell with all the package's dependencies available using <code>nix-shell -A env default.nix</code>. If you have <code>cabal-install</code> installed globally, it'll work inside the shell as expected.</p>"},{"location":"languages-frameworks/haskell.section.html#haskell-shellFor","title":"shellFor","text":"<p>Having to install tools globally is obviously not great, especially if you want to provide a batteries-included <code>shell.nix</code> with your project. Luckily there's a proper tool for making development environments out of packages' build environments: <code>shellFor</code>, a function exposed by every haskell package set. It takes the following arguments and returns a derivation which is suitable as a development environment inside <code>nix-shell</code>:</p> <p><code>packages</code> : This argument is used to select the packages for which to build the development environment. This should be a function which takes a haskell package set and returns a list of packages. <code>shellFor</code> will pass the used package set to this function and include all dependencies of the returned package in the build environment. This means you can reuse Nix expressions of packages included in nixpkgs, but also use local Nix expressions like this: <code>hpkgs: [ (hpkgs.callPackage ./my-project.nix { }) ]</code>.</p> <p><code>nativeBuildInputs</code> : Expects a list of derivations to add as build tools to the build environment. This is the place to add packages like <code>cabal-install</code>, <code>doctest</code> or <code>hlint</code>. Defaults to <code>[]</code>.</p> <p><code>buildInputs</code> : Expects a list of derivations to add as library dependencies, like <code>openssl</code>. This is rarely necessary as the haskell package expressions usually track system dependencies as well. Defaults to <code>[]</code>. (see also derivation dependencies)</p> <p><code>withHoogle</code> : If this is true, <code>hoogle</code> will be added to <code>nativeBuildInputs</code>. Additionally, its database will be populated with all included dependencies, so you'll be able search through the documentation of your dependencies. Defaults to <code>false</code>.</p> <p><code>genericBuilderArgsModifier</code> : This argument accepts a function allowing you to modify the arguments passed to <code>mkDerivation</code> in order to create the development environment. For example, <code>args: { doCheck = false; }</code> would cause the environment to not include any test dependencies. Defaults to <code>lib.id</code>.</p> <p><code>doBenchmark</code> : This is a shortcut for enabling <code>doBenchmark</code> via <code>genericBuilderArgsModifier</code>. Setting it to <code>true</code> will cause the development environment to include all benchmark dependencies which would be excluded by default. Defaults to <code>false</code>.</p> <p>One neat property of <code>shellFor</code> is that it allows you to work on multiple packages using the same environment in conjunction with cabal.project files. Say our example above depends on <code>distribution-nixpkgs</code> and we have a project file set up for both, we can add the following <code>shell.nix</code> expression:</p> <pre><code>{ pkgs ? import &lt;nixpkgs&gt; {} }:\n\npkgs.haskellPackages.shellFor {\n  packages = hpkgs: [\n    # reuse the nixpkgs for this package\n    hpkgs.distribution-nixpkgs\n    # call our generated Nix expression manually\n    (hpkgs.callPackage ./my-project/my-project.nix { })\n  ];\n\n  # development tools we use\n  nativeBuildInputs = [\n    pkgs.cabal-install\n    pkgs.haskellPackages.doctest\n    pkgs.cabal2nix\n  ];\n\n  # Extra arguments are added to mkDerivation's arguments as-is.\n  # Since it adds all passed arguments to the shell environment,\n  # we can use this to set the environment variable the `Paths_`\n  # module of distribution-nixpkgs uses to search for bundled\n  # files.\n  # See also: https://cabal.readthedocs.io/en/latest/cabal-package.html#accessing-data-files-from-package-code\n  distribution_nixpkgs_datadir = toString ./distribution-nixpkgs;\n}\n</code></pre>"},{"location":"languages-frameworks/haskell.section.html#haskell-language-server","title":"haskell-language-server","text":"<p>To use HLS in short: Install <code>pkgs.haskell-language-server</code> e.g. in <code>nativeBuildInputs</code> in <code>shellFor</code> and use the <code>haskell-language-server-wrapper</code> command to run it. See the HLS user guide on how to configure your text editor to use HLS and how to test your setup.</p> <p>HLS needs to be compiled with the GHC version of the project you use it on.</p> <p><code>pkgs.haskell-language-server</code> provides <code>haskell-language-server-wrapper</code>, <code>haskell-language-server</code> and <code>haskell-language-server-x.x.x</code> binaries, where <code>x.x.x</code> is the GHC version for which it is compiled. By default, it only includes binaries for the current GHC version, to reduce closure size. The closure size is large, because HLS needs to be dynamically linked to work reliably. You can override the list of supported GHC versions with e.g.</p> <p><pre><code>pkgs.haskell-language-server.override { supportedGhcVersions = [ \"90\" \"94\" ]; }\n</code></pre> Where all strings <code>version</code> are allowed such that <code>haskell.packages.ghc${version}</code> is an existing package set.</p> <p>When you run <code>haskell-language-server-wrapper</code> it will detect the GHC version used by the project you are working on (by asking e.g.\u00a0cabal or stack) and pick the appropriate versioned binary from your path.</p> <p>Be careful when installing HLS globally and using a pinned nixpkgs for a Haskell project in a <code>nix-shell</code>. If the nixpkgs versions deviate to much (e.g.,\u00a0use different <code>glibc</code> versions) the <code>haskell-language-server-?.?.?</code> executable will try to detect these situations and refuse to start. It is recommended to obtain HLS via <code>nix-shell</code> from the nixpkgs version pinned in there instead.</p> <p>The top level <code>pkgs.haskell-language-server</code> attribute is just a convenience wrapper to make it possible to install HLS for multiple GHC versions at the same time. If you know, that you only use one GHC version, e.g.,\u00a0in a project specific <code>nix-shell</code> you can use <code>pkgs.haskellPackages.haskell-language-server</code> or <code>pkgs.haskell.packages.*.haskell-language-server</code> from the package set you use.</p> <p>If you use <code>nix-shell</code> for your development environments remember to start your editor in that environment. You may want to use something like <code>direnv</code> and/or an editor plugin to achieve this.</p>"},{"location":"languages-frameworks/haskell.section.html#haskell-overriding-haskell-packages","title":"Overriding Haskell packages","text":""},{"location":"languages-frameworks/haskell.section.html#haskell-overriding-a-single-package","title":"Overriding a single package","text":"<p>Like many language specific subsystems in nixpkgs, the Haskell infrastructure also has its own quirks when it comes to overriding. Overriding of the inputs to a package at least follows the standard procedure. For example, imagine you need to build <code>nix-tree</code> with a more recent version of <code>brick</code> than the default one provided by <code>haskellPackages</code>:</p> <pre><code>haskellPackages.nix-tree.override {\n  brick = haskellPackages.brick_0_67;\n}\n</code></pre> <p>The custom interface comes into play when you want to override the arguments passed to <code>haskellPackages.mkDerivation</code>. For this, the function <code>overrideCabal</code> from <code>haskell.lib.compose</code> is used. E.g., if you want to install a man page that is distributed with the package, you can do something like this:</p> <pre><code>haskell.lib.compose.overrideCabal (drv: {\n  postInstall = ''\n    ${drv.postInstall or \"\"}\n    install -Dm644 man/pnbackup.1 -t $out/share/man/man1\n  '';\n}) haskellPackages.pnbackup\n</code></pre> <p><code>overrideCabal</code> takes two arguments:</p> <ol> <li>A function which receives all arguments passed to <code>haskellPackages.mkDerivation</code>    before and returns a set of arguments to replace (or add) with a new value.</li> <li>The Haskell derivation to override.</li> </ol> <p>The arguments are ordered so that you can easily create helper functions by making use of currying:</p> <pre><code>let\n  installManPage = haskell.lib.compose.overrideCabal (drv: {\n    postInstall = ''\n      ${drv.postInstall or \"\"}\n      install -Dm644 man/${drv.pname}.1 -t \"$out/share/man/man1\"\n    '';\n  });\nin\n\ninstallManPage haskellPackages.pnbackup\n</code></pre> <p>In fact, <code>haskell.lib.compose</code> already provides lots of useful helpers for common tasks, detailed in the next section. They are also structured in such a way that they can be combined using <code>lib.pipe</code>:</p> <pre><code>lib.pipe my-haskell-package [\n  # lift version bounds on dependencies\n  haskell.lib.compose.doJailbreak\n  # disable building the haddock documentation\n  haskell.lib.compose.dontHaddock\n  # pass extra package flag to Cabal's configure step\n  (haskell.lib.compose.enableCabalFlag \"myflag\")\n]\n</code></pre>"},{"location":"languages-frameworks/haskell.section.html#haskell-haskell.lib.compose","title":"<code>haskell.lib.compose</code>","text":"<p>The base interface for all overriding is the following function:</p> <p><code>overrideCabal f drv</code> : Takes the arguments passed to obtain <code>drv</code> to <code>f</code> and uses the resulting attribute set to update the argument set. Then a recomputed version of <code>drv</code> using the new argument set is returned.</p> <p>All other helper functions are implemented in terms of <code>overrideCabal</code> and make common overrides shorter and more complicate ones trivial. The simple overrides which only change a single argument are only described very briefly in the following overview. Refer to the documentation of <code>haskellPackages.mkDerivation</code> for a more detailed description of the effects of the respective arguments.</p>"},{"location":"languages-frameworks/haskell.section.html#haskell-packaging-helpers","title":"Packaging Helpers","text":"<p><code>overrideSrc { src, version } drv</code> : Replace the source used for building <code>drv</code> with the path or derivation given as <code>src</code>. The <code>version</code> attribute is optional. Prefer this function over overriding <code>src</code> via <code>overrideCabal</code>, since it also automatically takes care of removing any Hackage revisions.</p> <p><code>justStaticExecutables drv</code> : Only build and install the executables produced by <code>drv</code>, removing everything that may refer to other Haskell packages' store paths (like libraries and documentation). This dramatically reduces the closure size of the resulting derivation. Note that the executables are only statically linked against their Haskell dependencies, but will still link dynamically against libc, GMP and other system library dependencies. If dependencies use their Cabal-generated <code>Paths_*</code> module, this may not work as well if GHC's dead code elimination is unable to remove the references to the dependency's store path that module contains.</p> <p><code>enableSeparateBinOutput drv</code> : Install executables produced by <code>drv</code> to a separate <code>bin</code> output. This has a similar effect as <code>justStaticExecutables</code>, but preserves the libraries and documentation in the <code>out</code> output alongside the <code>bin</code> output with a much smaller closure size.</p> <p><code>markBroken drv</code> : Sets the <code>broken</code> flag to <code>true</code> for <code>drv</code>.</p> <p><code>markUnbroken drv</code>, <code>unmarkBroken drv</code> : Set the <code>broken</code> flag to <code>false</code> for <code>drv</code>.</p> <p><code>doDistribute drv</code> : Updates <code>hydraPlatforms</code> so that Hydra will build <code>drv</code>. This is sometimes necessary when working with versioned packages in <code>haskellPackages</code> which are not built by default.</p> <p><code>dontDistribute drv</code> : Sets <code>hydraPlatforms</code> to <code>[]</code>, causing Hydra to skip this package altogether. Useful if it fails to evaluate cleanly and is causing noise in the evaluation errors tab on Hydra.</p>"},{"location":"languages-frameworks/haskell.section.html#haskell-development-helpers","title":"Development Helpers","text":"<p><code>sdistTarball drv</code> : Create a source distribution tarball like those found on Hackage instead of building the package <code>drv</code>.</p> <p><code>documentationTarball drv</code> : Create a documentation tarball suitable for uploading to Hackage instead of building the package <code>drv</code>.</p> <p><code>buildFromSdist drv</code> : Uses <code>sdistTarball drv</code> as the source to compile <code>drv</code>. This helps to catch packaging bugs when building from a local directory, e.g. when required files are missing from <code>extra-source-files</code>.</p> <p><code>failOnAllWarnings drv</code> : Enables all warnings GHC supports and makes it fail the build if any of them are emitted.</p> <p><code>enableDWARFDebugging drv</code> : Compiles the package with additional debug symbols enabled, useful for debugging with e.g. <code>gdb</code>.</p> <p><code>doStrip drv</code> : Sets <code>doStrip</code> to <code>true</code> for <code>drv</code>.</p> <p><code>dontStrip drv</code> : Sets <code>doStrip</code> to <code>false</code> for <code>drv</code>.</p>"},{"location":"languages-frameworks/haskell.section.html#haskell-trivial-helpers","title":"Trivial Helpers","text":"<p><code>doJailbreak drv</code> : Sets the <code>jailbreak</code> argument to <code>true</code> for <code>drv</code>.</p> <p><code>dontJailbreak drv</code> : Sets the <code>jailbreak</code> argument to <code>false</code> for <code>drv</code>.</p> <p><code>doHaddock drv</code> : Sets <code>doHaddock</code> to <code>true</code> for <code>drv</code>.</p> <p><code>dontHaddock drv</code> : Sets <code>doHaddock</code> to <code>false</code> for <code>drv</code>. Useful if the build of a package is failing because of e.g. a syntax error in the Haddock documentation.</p> <p><code>doHyperlinkSource drv</code> : Sets <code>hyperlinkSource</code> to <code>true</code> for <code>drv</code>.</p> <p><code>dontHyperlinkSource drv</code> : Sets <code>hyperlinkSource</code> to <code>false</code> for <code>drv</code>.</p> <p><code>doCheck drv</code> : Sets <code>doCheck</code> to <code>true</code> for <code>drv</code>.</p> <p><code>dontCheck drv</code> : Sets <code>doCheck</code> to <code>false</code> for <code>drv</code>. Useful if a package has a broken, flaky or otherwise problematic test suite breaking the build.</p> <p><code>appendConfigureFlags list drv</code> : Adds the strings in <code>list</code> to the <code>configureFlags</code> argument for <code>drv</code>.</p> <p><code>enableCabalFlag flag drv</code> : Makes sure that the Cabal flag <code>flag</code> is enabled in Cabal's configure step.</p> <p><code>disableCabalFlag flag drv</code> : Makes sure that the Cabal flag <code>flag</code> is disabled in Cabal's configure step.</p> <p><code>appendBuildFlags list drv</code> : Adds the strings in <code>list</code> to the <code>buildFlags</code> argument for <code>drv</code>.</p> <p><code>appendPatches list drv</code> : Adds the <code>list</code> of derivations or paths to the <code>patches</code> argument for <code>drv</code>.</p> <p><code>addBuildTools list drv</code> : Adds the <code>list</code> of derivations to the <code>buildTools</code> argument for <code>drv</code>.</p> <p><code>addExtraLibraries list drv</code> : Adds the <code>list</code> of derivations to the <code>extraLibraries</code> argument for <code>drv</code>.</p> <p><code>addBuildDepends list drv</code> : Adds the <code>list</code> of derivations to the <code>buildDepends</code> argument for <code>drv</code>.</p> <p><code>addTestToolDepends list drv</code> : Adds the <code>list</code> of derivations to the <code>testToolDepends</code> argument for <code>drv</code>.</p> <p><code>addPkgconfigDepends list drv</code> : Adds the <code>list</code> of derivations to the <code>pkg-configDepends</code> argument for <code>drv</code>.</p> <p><code>addSetupDepends list drv</code> : Adds the <code>list</code> of derivations to the <code>setupHaskellDepends</code> argument for <code>drv</code>.</p> <p><code>doBenchmark drv</code> : Set <code>doBenchmark</code> to <code>true</code> for <code>drv</code>. Useful if your development environment is missing the dependencies necessary for compiling the benchmark component.</p> <p><code>dontBenchmark drv</code> : Set <code>doBenchmark</code> to <code>false</code> for <code>drv</code>.</p> <p><code>setBuildTargets drv list</code> : Sets the <code>buildTarget</code> argument for <code>drv</code> so that the targets specified in <code>list</code> are built.</p> <p><code>doCoverage drv</code> : Sets the <code>doCoverage</code> argument to <code>true</code> for <code>drv</code>.</p> <p><code>dontCoverage drv</code> : Sets the <code>doCoverage</code> argument to <code>false</code> for <code>drv</code>.</p> <p><code>enableExecutableProfiling drv</code> : Sets the <code>enableExecutableProfiling</code> argument to <code>true</code> for <code>drv</code>.</p> <p><code>disableExecutableProfiling drv</code> : Sets the <code>enableExecutableProfiling</code> argument to <code>false</code> for <code>drv</code>.</p> <p><code>enableLibraryProfiling drv</code> : Sets the <code>enableLibraryProfiling</code> argument to <code>true</code> for <code>drv</code>.</p> <p><code>disableLibraryProfiling drv</code> : Sets the <code>enableLibraryProfiling</code> argument to <code>false</code> for <code>drv</code>.</p>"},{"location":"languages-frameworks/haskell.section.html#haskell-package-set-lib-functions","title":"Library functions in the Haskell package sets","text":"<p>Some library functions depend on packages from the Haskell package sets. Thus they are exposed from those instead of from <code>haskell.lib.compose</code> which can only access what is passed directly to it. When using the functions below, make sure that you are obtaining them from the same package set (<code>haskellPackages</code>, <code>haskell.packages.ghc944</code> etc.) as the packages you are working with or \u2013 even better \u2013 from the <code>self</code>/<code>final</code> fix point of your overlay to <code>haskellPackages</code>.</p> <p>Note: Some functions like <code>shellFor</code> that are not intended for overriding per se, are omitted in this section. </p> <p><code>cabalSdist { src, name ? ... }</code> : Generates the Cabal sdist tarball for <code>src</code>, suitable for uploading to Hackage. Contrary to <code>haskell.lib.compose.sdistTarball</code>, it uses <code>cabal-install</code> over <code>Setup.hs</code>, so it is usually faster: No build dependencies need to be downloaded, and we can skip compiling <code>Setup.hs</code>.</p> <p><code>buildFromCabalSdist drv</code> : Build <code>drv</code>, but run its <code>src</code> attribute through <code>cabalSdist</code> first. Useful for catching files necessary for compilation that are missing from the sdist.</p> <p><code>generateOptparseApplicativeCompletions list drv</code> : Generate and install shell completion files for the installed executables whose names are given via <code>list</code>. The executables need to be using <code>optparse-applicative</code> for this to work. Note that this feature is automatically disabled when cross-compiling, since it requires executing the binaries in question.</p>"},{"location":"languages-frameworks/haskell.section.html#haskell-faq","title":"F.A.Q.","text":""},{"location":"languages-frameworks/haskell.section.html#haskell-why-not-covered","title":"Why is topic X not covered in this section? Why is section Y missing?","text":"<p>We have been working on moving the nixpkgs Haskell documentation back into the nixpkgs manual. Since this process has not been completed yet, you may find some topics missing here covered in the old haskell4nix docs.</p> <p>If you feel any important topic is not documented at all, feel free to comment on the issue linked above.</p>"},{"location":"languages-frameworks/haskell.section.html#haskell-faq-override-profiling","title":"How to enable or disable profiling builds globally?","text":"<p>By default, Nixpkgs builds a profiling version of each Haskell library. The exception to this rule are some platforms where it is disabled due to concerns over output size. You may want to\u2026</p> <ul> <li> <p>\u2026enable profiling globally so that you can build a project you are working on   with profiling ability giving you insight in the time spent across your code   and code you depend on using GHC's profiling feature.</p> </li> <li> <p>\u2026disable profiling (globally) to reduce the time spent building the profiling   versions of libraries which a significant amount of build time is spent on   (although they are not as expensive as the \u201cnormal\u201d build of a Haskell library).</p> </li> </ul> <p>::: {.note} The method described below affects the build of all libraries in the respective Haskell package set as well as GHC. If your choices differ from Nixpkgs' default for your (host) platform, you will lose the ability to substitute from the official binary cache.</p> <p>If you are concerned about build times and thus want to disable profiling, it probably makes sense to use <code>haskell.lib.compose.disableLibraryProfiling</code> (see ) on the packages you are building locally while continuing to substitute their dependencies and GHC. :::</p> <p>Since we need to change the profiling settings for the desired Haskell package set and GHC (as the core libraries like <code>base</code>, <code>filepath</code> etc. are bundled with GHC), it is recommended to use overlays for Nixpkgs to change them. Since the interrelated parts, i.e. the package set and GHC, are connected via the Nixpkgs fixpoint, we need to modify them both in a way that preserves their connection (or else we'd have to wire it up again manually). This is achieved by changing GHC and the package set in separate overlays to prevent the package set from pulling in GHC from <code>prev</code>.</p> <p>The result is two overlays like the ones shown below. Adjustable parts are annotated with comments, as are any optional or alternative ways to achieve the desired profiling settings without causing too many rebuilds.</p> <pre><code>let\n  # Name of the compiler and package set you want to change. If you are using\n  # the default package set `haskellPackages`, you need to look up what version\n  # of GHC it currently uses (note that this is subject to change).\n  ghcName = \"ghc92\";\n  # Desired new setting\n  enableProfiling = true;\nin\n\n[\n  # The first overlay modifies the GHC derivation so that it does or does not\n  # build profiling versions of the core libraries bundled with it. It is\n  # recommended to only use such an overlay if you are enabling profiling on a\n  # platform that doesn't by default, because compiling GHC from scratch is\n  # quite expensive.\n  (final: prev:\n  let\n    inherit (final) lib;\n  in\n\n  {\n    haskell = prev.haskell // {\n      compiler = prev.haskell.compiler // {\n        ${ghcName} = prev.haskell.compiler.${ghcName}.override {\n          # Unfortunately, the GHC setting is named differently for historical reasons\n          enableProfiledLibs = enableProfiling;\n        };\n      };\n    };\n  })\n\n  (final: prev:\n  let\n    inherit (final) lib;\n    haskellLib = final.haskell.lib.compose;\n  in\n\n  {\n    haskell = prev.haskell // {\n      packages = prev.haskell.packages // {\n        ${ghcName} = prev.haskell.packages.${ghcName}.override {\n          overrides = hfinal: hprev: {\n            mkDerivation = args: hprev.mkDerivation (args // {\n              # Since we are forcing our ideas upon mkDerivation, this change will\n              # affect every package in the package set.\n              enableLibraryProfiling = enableProfiling;\n\n              # To actually use profiling on an executable, executable profiling\n              # needs to be enabled for the executable you want to profile. You\n              # can either do this globally or\u2026\n              enableExecutableProfiling = enableProfiling;\n            });\n\n            # \u2026only for the package that contains an executable you want to profile.\n            # That saves on unnecessary rebuilds for packages that you only depend\n            # on for their library, but also contain executables (e.g. pandoc).\n            my-executable = haskellLib.enableExecutableProfiling hprev.my-executable;\n\n            # If you are disabling profiling to save on build time, but want to\n            # retain the ability to substitute from the binary cache. Drop the\n            # override for mkDerivation above and instead have an override like\n            # this for the specific packages you are building locally and want\n            # to make cheaper to build.\n            my-library = haskellLib.disableLibraryProfiling hprev.my-library;\n          };\n        };\n      };\n    };\n  })\n]\n</code></pre>"},{"location":"languages-frameworks/hy.section.html","title":"Hy","text":""},{"location":"languages-frameworks/hy.section.html#ssec-hy-installation","title":"Installation","text":""},{"location":"languages-frameworks/hy.section.html#installation-without-packages","title":"Installation without packages","text":"<p>You can install <code>hy</code> via nix-env or by adding it to <code>configuration.nix</code> by referring to it as a <code>hy</code> attribute. This kind of installation adds <code>hy</code> to your environment and it successfully works with <code>python3</code>.</p> <p>::: {.caution} Packages that are installed with your python derivation, are not accessible by <code>hy</code> this way. :::</p>"},{"location":"languages-frameworks/hy.section.html#installation-with-packages","title":"Installation with packages","text":"<p>Creating <code>hy</code> derivation with custom <code>python</code> packages is really simple and similar to the way that python does it. Attribute <code>hy</code> provides function <code>withPackages</code> that creates custom <code>hy</code> derivation with specified packages.</p> <p>For example if you want to create shell with <code>matplotlib</code> and <code>numpy</code>, you can do it like so:</p> <pre><code>$ nix-shell -p \"hy.withPackages (ps: with ps; [ numpy matplotlib ])\"\n</code></pre> <p>Or if you want to extend your <code>configuration.nix</code>: <pre><code>{ # ...\n\n  environment.systemPackages = with pkgs; [\n    (hy.withPackages (py-packages: with py-packages; [ numpy matplotlib ]))\n  ];\n}\n</code></pre></p>"},{"location":"languages-frameworks/idris.section.html","title":"Idris","text":""},{"location":"languages-frameworks/idris.section.html#installing-idris","title":"Installing Idris","text":"<p>The easiest way to get a working idris version is to install the <code>idris</code> attribute:</p> <pre><code>$ nix-env -f \"&lt;nixpkgs&gt;\" -iA idris\n</code></pre> <p>This however only provides the <code>prelude</code> and <code>base</code> libraries. To install idris with additional libraries, you can use the <code>idrisPackages.with-packages</code> function, e.g. in an overlay in <code>~/.config/nixpkgs/overlays/my-idris.nix</code>:</p> <pre><code>self: super: {\n  myIdris = with self.idrisPackages; with-packages [ contrib pruviloj ];\n}\n</code></pre> <p>And then:</p> <pre><code>$ # On NixOS\n$ nix-env -iA nixos.myIdris\n$ # On non-NixOS\n$ nix-env -iA nixpkgs.myIdris\n</code></pre> <p>To see all available Idris packages:</p> <pre><code>$ # On NixOS\n$ nix-env -qaPA nixos.idrisPackages\n$ # On non-NixOS\n$ nix-env -qaPA nixpkgs.idrisPackages\n</code></pre> <p>Similarly, entering a <code>nix-shell</code>:</p> <pre><code>$ nix-shell -p 'idrisPackages.with-packages (with idrisPackages; [ contrib pruviloj ])'\n</code></pre>"},{"location":"languages-frameworks/idris.section.html#starting-idris-with-library-support","title":"Starting Idris with library support","text":"<p>To have access to these libraries in idris, call it with an argument <code>-p &lt;library name&gt;</code> for each library:</p> <pre><code>$ nix-shell -p 'idrisPackages.with-packages (with idrisPackages; [ contrib pruviloj ])'\n[nix-shell:~]$ idris -p contrib -p pruviloj\n</code></pre> <p>A listing of all available packages the Idris binary has access to is available via <code>--listlibs</code>:</p> <pre><code>$ idris --listlibs\n00prelude-idx.ibc\npruviloj\nbase\ncontrib\nprelude\n00pruviloj-idx.ibc\n00base-idx.ibc\n00contrib-idx.ibc\n</code></pre>"},{"location":"languages-frameworks/idris.section.html#building-an-idris-project-with-nix","title":"Building an Idris project with Nix","text":"<p>As an example of how a Nix expression for an Idris package can be created, here is the one for <code>idrisPackages.yaml</code>:</p> <pre><code>{ lib\n, build-idris-package\n, fetchFromGitHub\n, contrib\n, lightyear\n}:\nbuild-idris-package  {\n  name = \"yaml\";\n  version = \"2018-01-25\";\n\n  # This is the .ipkg file that should be built, defaults to the package name\n  # In this case it should build `Yaml.ipkg` instead of `yaml.ipkg`\n  # This is only necessary because the yaml packages ipkg file is\n  # different from its package name here.\n  ipkgName = \"Yaml\";\n  # Idris dependencies to provide for the build\n  idrisDeps = [ contrib lightyear ];\n\n  src = fetchFromGitHub {\n    owner = \"Heather\";\n    repo = \"Idris.Yaml\";\n    rev = \"5afa51ffc839844862b8316faba3bafa15656db4\";\n    hash = \"sha256-h28F9EEPuvab6zrfeE+0k1XGQJGwINnsJEG8yjWIl7w=\";\n  };\n\n  meta = with lib; {\n    description = \"Idris YAML lib\";\n    homepage = \"https://github.com/Heather/Idris.Yaml\";\n    license = licenses.mit;\n    maintainers = [ maintainers.brainrape ];\n  };\n}\n</code></pre> <p>Assuming this file is saved as <code>yaml.nix</code>, it's buildable using</p> <pre><code>$ nix-build -E '(import &lt;nixpkgs&gt; {}).idrisPackages.callPackage ./yaml.nix {}'\n</code></pre> <p>Or it's possible to use</p> <pre><code>with import &lt;nixpkgs&gt; {};\n\n{\n  yaml = idrisPackages.callPackage ./yaml.nix {};\n}\n</code></pre> <p>in another file (say <code>default.nix</code>) to be able to build it with</p> <pre><code>$ nix-build -A yaml\n</code></pre>"},{"location":"languages-frameworks/idris.section.html#passing-options-to-idris-commands","title":"Passing options to <code>idris</code> commands","text":"<p>The <code>build-idris-package</code> function provides also optional input values to set additional options for the used <code>idris</code> commands.</p> <p>Specifically, you can set <code>idrisBuildOptions</code>, <code>idrisTestOptions</code>, <code>idrisInstallOptions</code> and <code>idrisDocOptions</code> to provide additional options to the <code>idris</code> command respectively when building, testing, installing and generating docs for your package.</p> <p>For example you could set</p> <pre><code>build-idris-package {\n  idrisBuildOptions = [ \"--log\" \"1\" \"--verbose\" ]\n\n  ...\n}\n</code></pre> <p>to require verbose output during <code>idris</code> build phase.</p>"},{"location":"languages-frameworks/idris2.section.html","title":"Idris2","text":"<p>In addition to exposing the Idris2 compiler itself, Nixpkgs exposes an <code>idris2Packages.buildIdris</code> helper to make it a bit more ergonomic to build Idris2 executables or libraries.</p> <p>The <code>buildIdris</code> function takes an attribute set that defines at a minimum the <code>src</code> and <code>ipkgName</code> of the package to be built and any <code>idrisLibraries</code> required to build it. The <code>src</code> is the same source you're familiar with and the <code>ipkgName</code> must be the name of the <code>ipkg</code> file for the project (omitting the <code>.ipkg</code> extension). The <code>idrisLibraries</code> is a list of other library derivations created with <code>buildIdris</code>. You can optionally specify other derivation properties as needed but sensible defaults for <code>configurePhase</code>, <code>buildPhase</code>, and <code>installPhase</code> are provided.</p> <p>Importantly, <code>buildIdris</code> does not create a single derivation but rather an attribute set with two properties: <code>executable</code> and <code>library</code>. The <code>executable</code> property is a derivation and the <code>library</code> property is a function that will return a derivation for the library with or without source code included. Source code need not be included unless you are aiming to use IDE or LSP features that are able to jump to definitions within an editor.</p> <p>A simple example of a fully packaged library would be the <code>LSP-lib</code> found in the <code>idris-community</code> GitHub organization. <pre><code>{ fetchFromGitHub, idris2Packages }:\nlet lspLibPkg = idris2Packages.buildIdris {\n  ipkgName = \"lsp-lib\";\n  src = fetchFromGitHub {\n   owner = \"idris-community\";\n   repo = \"LSP-lib\";\n   rev = \"main\";\n   hash = \"sha256-EvSyMCVyiy9jDZMkXQmtwwMoLaem1GsKVFqSGNNHHmY=\";\n  };\n  idrisLibraries = [ ];\n};\nin lspLibPkg.library\n</code></pre></p> <p>The above results in a derivation with the installed library results (with sourcecode).</p> <p>A slightly more involved example of a fully packaged executable would be the <code>idris2-lsp</code> which is an Idris2 language server that uses the <code>LSP-lib</code> found above. <pre><code>{ callPackage, fetchFromGitHub, idris2Packages }:\n\n# Assuming the previous example lives in `lsp-lib.nix`:\nlet lspLib = callPackage ./lsp-lib.nix { };\n    lspPkg = idris2Packages.buildIdris {\n      ipkgName = \"idris2-lsp\";\n      src = fetchFromGitHub {\n         owner = \"idris-community\";\n         repo = \"idris2-lsp\";\n         rev = \"main\";\n         hash = \"sha256-vQTzEltkx7uelDtXOHc6QRWZ4cSlhhm5ziOqWA+aujk=\";\n      };\n      idrisLibraries = [(idris2Packages.idris2Api { }) (lspLib { })];\n    };\nin lspPkg.executable\n</code></pre></p> <p>The above uses the default value of <code>withSource = false</code> for both of the two required Idris libraries that the <code>idris2-lsp</code> executable depends on. <code>idris2Api</code> in the above derivation comes built in with <code>idris2Packages</code>. This library exposes many of the otherwise internal APIs of the Idris2 compiler.</p>"},{"location":"languages-frameworks/ios.section.html","title":"iOS","text":"<p>This component is basically a wrapper/workaround that makes it possible to expose an Xcode installation as a Nix package by means of symlinking to the relevant executables on the host system.</p> <p>Since Xcode can't be packaged with Nix, nor we can publish it as a Nix package (because of its license) this is basically the only integration strategy making it possible to do iOS application builds that integrate with other components of the Nix ecosystem</p> <p>The primary objective of this project is to use the Nix expression language to specify how iOS apps can be built from source code, and to automatically spawn iOS simulator instances for testing.</p> <p>This component also makes it possible to use Hydra, the Nix-based continuous integration server to regularly build iOS apps and to do wireless ad-hoc installations of enterprise IPAs on iOS devices through Hydra.</p> <p>The Xcode build environment implements a number of features.</p>"},{"location":"languages-frameworks/ios.section.html#deploying-a-proxy-component-wrapper-exposing-xcode","title":"Deploying a proxy component wrapper exposing Xcode","text":"<p>The first use case is deploying a Nix package that provides symlinks to the Xcode installation on the host system. This package can be used as a build input to any build function implemented in the Nix expression language that requires Xcode.</p> <pre><code>let\n  pkgs = import &lt;nixpkgs&gt; {};\n\n  xcodeenv = import ./xcodeenv {\n    inherit (pkgs) stdenv;\n  };\nin\nxcodeenv.composeXcodeWrapper {\n  version = \"9.2\";\n  xcodeBaseDir = \"/Applications/Xcode.app\";\n}\n</code></pre> <p>By deploying the above expression with <code>nix-build</code> and inspecting its content you will notice that several Xcode-related executables are exposed as a Nix package:</p> <pre><code>$ ls result/bin\nlrwxr-xr-x  1 sander  staff  94  1 jan  1970 Simulator -&gt; /Applications/Xcode.app/Contents/Developer/Applications/Simulator.app/Contents/MacOS/Simulator\nlrwxr-xr-x  1 sander  staff  17  1 jan  1970 codesign -&gt; /usr/bin/codesign\nlrwxr-xr-x  1 sander  staff  17  1 jan  1970 security -&gt; /usr/bin/security\nlrwxr-xr-x  1 sander  staff  21  1 jan  1970 xcode-select -&gt; /usr/bin/xcode-select\nlrwxr-xr-x  1 sander  staff  61  1 jan  1970 xcodebuild -&gt; /Applications/Xcode.app/Contents/Developer/usr/bin/xcodebuild\nlrwxr-xr-x  1 sander  staff  14  1 jan  1970 xcrun -&gt; /usr/bin/xcrun\n</code></pre>"},{"location":"languages-frameworks/ios.section.html#building-an-ios-application","title":"Building an iOS application","text":"<p>We can build an iOS app executable for the simulator, or an IPA/xcarchive file for release purposes, e.g. ad-hoc, enterprise or store installations, by executing the <code>xcodeenv.buildApp {}</code> function:</p> <pre><code>let\n  pkgs = import &lt;nixpkgs&gt; {};\n\n  xcodeenv = import ./xcodeenv {\n    inherit (pkgs) stdenv;\n  };\nin\nxcodeenv.buildApp {\n  name = \"MyApp\";\n  src = ./myappsources;\n  sdkVersion = \"11.2\";\n\n  target = null; # Corresponds to the name of the app by default\n  configuration = null; # Release for release builds, Debug for debug builds\n  scheme = null; # -scheme will correspond to the app name by default\n  sdk = null; # null will set it to 'iphonesimulator` for simulator builds or `iphoneos` to real builds\n  xcodeFlags = \"\";\n\n  release = true;\n  certificateFile = ./mycertificate.p12;\n  certificatePassword = \"secret\";\n  provisioningProfile = ./myprovisioning.profile;\n  signMethod = \"ad-hoc\"; # 'enterprise' or 'store'\n  generateIPA = true;\n  generateXCArchive = false;\n\n  enableWirelessDistribution = true;\n  installURL = \"/installipa.php\";\n  bundleId = \"mycompany.myapp\";\n  appVersion = \"1.0\";\n\n  # Supports all xcodewrapper parameters as well\n  xcodeBaseDir = \"/Applications/Xcode.app\";\n}\n</code></pre> <p>The above function takes a variety of parameters:</p> <ul> <li>The <code>name</code> and <code>src</code> parameters are mandatory and specify the name of the app   and the location where the source code resides</li> <li><code>sdkVersion</code> specifies which version of the iOS SDK to use.</li> </ul> <p>It also possible to adjust the <code>xcodebuild</code> parameters. This is only needed in rare circumstances. In most cases the default values should suffice:</p> <ul> <li>Specifies which <code>xcodebuild</code> target to build. By default it takes the target   that has the same name as the app.</li> <li>The <code>configuration</code> parameter can be overridden if desired. By default, it   will do a debug build for the simulator and a release build for real devices.</li> <li>The <code>scheme</code> parameter specifies which <code>-scheme</code> parameter to propagate to   <code>xcodebuild</code>. By default, it corresponds to the app name.</li> <li>The <code>sdk</code> parameter specifies which SDK to use. By default, it picks   <code>iphonesimulator</code> for simulator builds and <code>iphoneos</code> for release builds.</li> <li>The <code>xcodeFlags</code> parameter specifies arbitrary command line parameters that   should be propagated to <code>xcodebuild</code>.</li> </ul> <p>By default, builds are carried out for the iOS simulator. To do release builds (builds for real iOS devices), you must set the <code>release</code> parameter to <code>true</code>. In addition, you need to set the following parameters:</p> <ul> <li><code>certificateFile</code> refers to a P12 certificate file.</li> <li><code>certificatePassword</code> specifies the password of the P12 certificate.</li> <li><code>provisioningProfile</code> refers to the provision profile needed to sign the app</li> <li><code>signMethod</code> should refer to <code>ad-hoc</code> for signing the app with an ad-hoc   certificate, <code>enterprise</code> for enterprise certificates and <code>app-store</code> for App   store certificates.</li> <li><code>generateIPA</code> specifies that we want to produce an IPA file (this is probably   what you want)</li> <li><code>generateXCArchive</code> specifies that we want to produce an xcarchive file.</li> </ul> <p>When building IPA files on Hydra and when it is desired to allow iOS devices to install IPAs by browsing to the Hydra build products page, you can enable the <code>enableWirelessDistribution</code> parameter.</p> <p>When enabled, you need to configure the following options:</p> <ul> <li>The <code>installURL</code> parameter refers to the URL of a PHP script that composes the   <code>itms-services://</code> URL allowing iOS devices to install the IPA file.</li> <li><code>bundleId</code> refers to the bundle ID value of the app</li> <li><code>appVersion</code> refers to the app's version number</li> </ul> <p>To use wireless adhoc distributions, you must also install the corresponding PHP script on a web server (see section: 'Installing the PHP script for wireless ad hoc installations from Hydra' for more information).</p> <p>In addition to the build parameters, you can also specify any parameters that the <code>xcodeenv.composeXcodeWrapper {}</code> function takes. For example, the <code>xcodeBaseDir</code> parameter can be overridden to refer to a different Xcode version.</p>"},{"location":"languages-frameworks/ios.section.html#spawning-simulator-instances","title":"Spawning simulator instances","text":"<p>In addition to building iOS apps, we can also automatically spawn simulator instances:</p> <pre><code>let\n  pkgs = import &lt;nixpkgs&gt; {};\n\n  xcodeenv = import ./xcodeenv {\n    inherit (pkgs) stdenv;\n  };\nin\nxcode.simulateApp {\n  name = \"simulate\";\n\n  # Supports all xcodewrapper parameters as well\n  xcodeBaseDir = \"/Applications/Xcode.app\";\n}\n</code></pre> <p>The above expression produces a script that starts the simulator from the provided Xcode installation. The script can be started as follows:</p> <pre><code>./result/bin/run-test-simulator\n</code></pre> <p>By default, the script will show an overview of UDID for all available simulator instances and asks you to pick one. You can also provide a UDID as a command-line parameter to launch an instance automatically:</p> <pre><code>./result/bin/run-test-simulator 5C93129D-CF39-4B1A-955F-15180C3BD4B8\n</code></pre> <p>You can also extend the simulator script to automatically deploy and launch an app in the requested simulator instance:</p> <pre><code>let\n  pkgs = import &lt;nixpkgs&gt; {};\n\n  xcodeenv = import ./xcodeenv {\n    inherit (pkgs) stdenv;\n  };\nin\nxcode.simulateApp {\n  name = \"simulate\";\n  bundleId = \"mycompany.myapp\";\n  app = xcode.buildApp {\n    # ...\n  };\n\n  # Supports all xcodewrapper parameters as well\n  xcodeBaseDir = \"/Applications/Xcode.app\";\n}\n</code></pre> <p>By providing the result of an <code>xcode.buildApp {}</code> function and configuring the app bundle id, the app gets deployed automatically and started.</p>"},{"location":"languages-frameworks/ios.section.html#troubleshooting","title":"Troubleshooting","text":"<p>In some rare cases, it may happen that after a failure, changes are not picked up. Most likely, this is caused by a derived data cache that Xcode maintains. To wipe it you can run:</p> <pre><code>$ rm -rf ~/Library/Developer/Xcode/DerivedData\n</code></pre>"},{"location":"languages-frameworks/java.section.html","title":"Java","text":"<p>Ant-based Java packages are typically built from source as follows:</p> <pre><code>stdenv.mkDerivation {\n  name = \"...\";\n  src = fetchurl { ... };\n\n  nativeBuildInputs = [ jdk ant ];\n\n  buildPhase = \"ant\";\n}\n</code></pre> <p>Note that <code>jdk</code> is an alias for the OpenJDK (self-built where available, or pre-built via Zulu). Platforms with OpenJDK not (yet) in Nixpkgs (<code>Aarch32</code>, <code>Aarch64</code>) point to the (unfree) <code>oraclejdk</code>.</p> <p>JAR files that are intended to be used by other packages should be installed in <code>$out/share/java</code>. JDKs have a stdenv setup hook that add any JARs in the <code>share/java</code> directories of the build inputs to the <code>CLASSPATH</code> environment variable. For instance, if the package <code>libfoo</code> installs a JAR named <code>foo.jar</code> in its <code>share/java</code> directory, and another package declares the attribute</p> <pre><code>buildInputs = [ libfoo ];\nnativeBuildInputs = [ jdk ];\n</code></pre> <p>then <code>CLASSPATH</code> will be set to <code>/nix/store/...-libfoo/share/java/foo.jar</code>.</p> <p>Private JARs should be installed in a location like <code>$out/share/package-name</code>.</p> <p>If your Java package provides a program, you need to generate a wrapper script to run it using a JRE. You can use <code>makeWrapper</code> for this:</p> <pre><code>nativeBuildInputs = [ makeWrapper ];\n\ninstallPhase = ''\n  mkdir -p $out/bin\n  makeWrapper ${jre}/bin/java $out/bin/foo \\\n    --add-flags \"-cp $out/share/java/foo.jar org.foo.Main\"\n'';\n</code></pre> <p>Since the introduction of the Java Platform Module System in Java 9, Java distributions typically no longer ship with a general-purpose JRE: instead, they allow generating a JRE with only the modules required for your application(s). Because we can't predict what modules will be needed on a general-purpose system, the default jre package is the full JDK. When building a minimal system/image, you can override the <code>modules</code> parameter on <code>jre_minimal</code> to build a JRE with only the modules relevant for you:</p> <pre><code>let\n  my_jre = pkgs.jre_minimal.override {\n    modules = [\n      # The modules used by 'something' and 'other' combined:\n      \"java.base\"\n      \"java.logging\"\n    ];\n  };\n  something = (pkgs.something.override { jre = my_jre; });\n  other = (pkgs.other.override { jre = my_jre; });\nin\n  ...\n</code></pre> <p>You can also specify what JDK your JRE should be based on, for example selecting a 'headless' build to avoid including a link to GTK+:</p> <pre><code>my_jre = pkgs.jre_minimal.override {\n  jdk = jdk11_headless;\n};\n</code></pre> <p>Note all JDKs passthru <code>home</code>, so if your application requires environment variables like <code>JAVA_HOME</code> being set, that can be done in a generic fashion with the <code>--set</code> argument of <code>makeWrapper</code>:</p> <pre><code>--set JAVA_HOME ${jdk.home}\n</code></pre> <p>It is possible to use a different Java compiler than <code>javac</code> from the OpenJDK. For instance, to use the GNU Java Compiler:</p> <pre><code>nativeBuildInputs = [ gcj ant ];\n</code></pre> <p>Here, Ant will automatically use <code>gij</code> (the GNU Java Runtime) instead of the OpenJRE.</p>"},{"location":"languages-frameworks/javascript.section.html","title":"Javascript","text":""},{"location":"languages-frameworks/javascript.section.html#javascript-introduction","title":"Introduction","text":"<p>This contains instructions on how to package javascript applications.</p> <p>The various tools available will be listed in the tools-overview. Some general principles for packaging will follow. Finally some tool specific instructions will be given.</p>"},{"location":"languages-frameworks/javascript.section.html#javascript-finding-examples","title":"Getting unstuck / finding code examples","text":"<p>If you find you are lacking inspiration for packing javascript applications, the links below might prove useful. Searching online for prior art can be helpful if you are running into solved problems.</p>"},{"location":"languages-frameworks/javascript.section.html#javascript-finding-examples-github","title":"Github","text":"<ul> <li>Searching Nix files for <code>mkYarnPackage</code>: https://github.com/search?q=mkYarnPackage+language%3ANix&amp;type=code</li> <li>Searching just <code>flake.nix</code> files for <code>mkYarnPackage</code>: https://github.com/search?q=mkYarnPackage+path%3A**%2Fflake.nix&amp;type=code</li> </ul>"},{"location":"languages-frameworks/javascript.section.html#javascript-finding-examples-gitlab","title":"Gitlab","text":"<ul> <li>Searching Nix files for <code>mkYarnPackage</code>: https://gitlab.com/search?scope=blobs&amp;search=mkYarnPackage+extension%3Anix</li> <li>Searching just <code>flake.nix</code> files for <code>mkYarnPackage</code>: https://gitlab.com/search?scope=blobs&amp;search=mkYarnPackage+filename%3Aflake.nix</li> </ul>"},{"location":"languages-frameworks/javascript.section.html#javascript-tools-overview","title":"Tools overview","text":""},{"location":"languages-frameworks/javascript.section.html#javascript-general-principles","title":"General principles","text":"<p>The following principles are given in order of importance with potential exceptions.</p>"},{"location":"languages-frameworks/javascript.section.html#javascript-upstream-node-version","title":"Try to use the same node version used upstream","text":"<p>It is often not documented which node version is used upstream, but if it is, try to use the same version when packaging.</p> <p>This can be a problem if upstream is using the latest and greatest and you are trying to use an earlier version of node. Some cryptic errors regarding V8 may appear.</p>"},{"location":"languages-frameworks/javascript.section.html#javascript-upstream-package-manager","title":"Try to respect the package manager originally used by upstream (and use the upstream lock file)","text":"<p>A lock file (package-lock.json, yarn.lock...) is supposed to make reproducible installations of node_modules for each tool.</p> <p>Guidelines of package managers, recommend to commit those lock files to the repos. If a particular lock file is present, it is a strong indication of which package manager is used upstream.</p> <p>It's better to try to use a Nix tool that understand the lock file. Using a different tool might give you hard to understand error because different packages have been installed. An example of problems that could arise can be found here. Upstream use NPM, but this is an attempt to package it with <code>yarn2nix</code> (that uses yarn.lock).</p> <p>Using a different tool forces to commit a lock file to the repository. Those files are fairly large, so when packaging for nixpkgs, this approach does not scale well.</p> <p>Exceptions to this rule are:</p> <ul> <li>When you encounter one of the bugs from a Nix tool. In each of the tool specific instructions, known problems will be detailed. If you have a problem with a particular tool, then it's best to try another tool, even if this means you will have to recreate a lock file and commit it to nixpkgs. In general <code>yarn2nix</code> has less known problems and so a simple search in nixpkgs will reveal many yarn.lock files committed.</li> <li>Some lock files contain particular version of a package that has been pulled off NPM for some reason. In that case, you can recreate upstream lock (by removing the original and <code>npm install</code>, <code>yarn</code>, ...) and commit this to nixpkgs.</li> <li>The only tool that supports workspaces (a feature of NPM that helps manage sub-directories with different package.json from a single top level package.json) is <code>yarn2nix</code>. If upstream has workspaces you should try <code>yarn2nix</code>.</li> </ul>"},{"location":"languages-frameworks/javascript.section.html#javascript-upstream-package-json","title":"Try to use upstream package.json","text":"<p>Exceptions to this rule are:</p> <ul> <li>Sometimes the upstream repo assumes some dependencies be installed globally. In that case you can add them manually to the upstream package.json (<code>yarn add xxx</code> or <code>npm install xxx</code>, ...). Dependencies that are installed locally can be executed with <code>npx</code> for CLI tools. (e.g. <code>npx postcss ...</code>, this is how you can call those dependencies in the phases).</li> <li>Sometimes there is a version conflict between some dependency requirements. In that case you can fix a version by removing the <code>^</code>.</li> <li>Sometimes the script defined in the package.json does not work as is. Some scripts for example use CLI tools that might not be available, or cd in directory with a different package.json (for workspaces notably). In that case, it's perfectly fine to look at what the particular script is doing and break this down in the phases. In the build script you can see <code>build:*</code> calling in turns several other build scripts like <code>build:ui</code> or <code>build:server</code>. If one of those fails, you can try to separate those into,</li> </ul> <pre><code>yarn build:ui\nyarn build:server\n# OR\nnpm run build:ui\nnpm run build:server\n</code></pre> <p>when you need to override a package.json. It's nice to use the one from the upstream source and do some explicit override. Here is an example:</p> <pre><code>patchedPackageJSON = final.runCommand \"package.json\" { } ''\n  ${jq}/bin/jq '.version = \"0.4.0\" |\n    .devDependencies.\"@jsdoc/cli\" = \"^0.2.5\"\n    ${sonar-src}/package.json &gt; $out\n'';\n</code></pre> <p>You will still need to commit the modified version of the lock files, but at least the overrides are explicit for everyone to see.</p>"},{"location":"languages-frameworks/javascript.section.html#javascript-using-node_modules","title":"Using node_modules directly","text":"<p>Each tool has an abstraction to just build the node_modules (dependencies) directory. You can always use the <code>stdenv.mkDerivation</code> with the node_modules to build the package (symlink the node_modules directory and then use the package build command). The node_modules abstraction can be also used to build some web framework frontends. For an example of this see how plausible is built. <code>mkYarnModules</code> to make the derivation containing node_modules. Then when building the frontend you can just symlink the node_modules directory.</p>"},{"location":"languages-frameworks/javascript.section.html#javascript-packages-nixpkgs","title":"Javascript packages inside nixpkgs","text":"<p>The pkgs/development/node-packages folder contains a generated collection of NPM packages that can be installed with the Nix package manager.</p> <p>As a rule of thumb, the package set should only provide end user software packages, such as command-line utilities. Libraries should only be added to the package set if there is a non-NPM package that requires it.</p> <p>When it is desired to use NPM libraries in a development project, use the <code>node2nix</code> generator directly on the <code>package.json</code> configuration file of the project.</p> <p>The package set provides support for the official stable Node.js versions. The latest stable LTS release in <code>nodePackages</code>, as well as the latest stable current release in <code>nodePackages_latest</code>.</p> <p>If your package uses native addons, you need to examine what kind of native build system it uses. Here are some examples:</p> <ul> <li><code>node-gyp</code></li> <li><code>node-gyp-builder</code></li> <li><code>node-pre-gyp</code></li> </ul> <p>After you have identified the correct system, you need to override your package expression while adding in build system as a build input. For example, <code>dat</code> requires <code>node-gyp-build</code>, so we override its expression in pkgs/development/node-packages/overrides.nix:</p> <pre><code>    dat = prev.dat.override (oldAttrs: {\n      buildInputs = [ final.node-gyp-build pkgs.libtool pkgs.autoconf pkgs.automake ];\n      meta = oldAttrs.meta // { broken = since \"12\"; };\n    });\n</code></pre>"},{"location":"languages-frameworks/javascript.section.html#javascript-adding-or-updating-packages","title":"Adding and Updating Javascript packages in nixpkgs","text":"<p>To add a package from NPM to nixpkgs:</p> <ol> <li>Modify pkgs/development/node-packages/node-packages.json to add, update or remove package entries to have it included in <code>nodePackages</code> and <code>nodePackages_latest</code>.</li> <li>Run the script:</li> </ol> <pre><code>./pkgs/development/node-packages/generate.sh\n</code></pre> <ol> <li>Build your new package to test your changes:</li> </ol> <pre><code>nix-build -A nodePackages.&lt;new-or-updated-package&gt;\n</code></pre> <pre><code>To build against the latest stable Current Node.js version (e.g. 18.x):\n\n```sh\nnix-build -A nodePackages_latest.&lt;new-or-updated-package&gt;\n```\n\nIf the package doesn't build, you may need to add an override as explained above.\n</code></pre> <ol> <li>If the package's name doesn't match any of the executables it provides, add an entry in pkgs/development/node-packages/main-programs.nix. This will be the case for all scoped packages, e.g., <code>@angular/cli</code>.</li> <li>Add and commit all modified and generated files.</li> </ol> <p>For more information about the generation process, consult the README.md file of the <code>node2nix</code> tool.</p> <p>To update NPM packages in nixpkgs, run the same <code>generate.sh</code> script:</p> <pre><code>./pkgs/development/node-packages/generate.sh\n</code></pre>"},{"location":"languages-frameworks/javascript.section.html#javascript-git-error","title":"Git protocol error","text":"<p>Some packages may have Git dependencies from GitHub specified with <code>git://</code>. GitHub has disabled unencrypted Git connections, so you may see the following error when running the generate script:</p> <pre><code>The unauthenticated git protocol on port 9418 is no longer supported\n</code></pre> <p>Use the following Git configuration to resolve the issue:</p> <pre><code>git config --global url.\"https://github.com/\".insteadOf git://github.com/\n</code></pre>"},{"location":"languages-frameworks/javascript.section.html#javascript-tool-specific","title":"Tool specific instructions","text":""},{"location":"languages-frameworks/javascript.section.html#javascript-buildNpmPackage","title":"buildNpmPackage","text":"<p><code>buildNpmPackage</code> allows you to package npm-based projects in Nixpkgs without the use of an auto-generated dependencies file (as used in node2nix). It works by utilizing npm's cache functionality -- creating a reproducible cache that contains the dependencies of a project, and pointing npm to it.</p> <p>Here's an example:</p> <pre><code>{ lib, buildNpmPackage, fetchFromGitHub }:\n\nbuildNpmPackage rec {\n  pname = \"flood\";\n  version = \"4.7.0\";\n\n  src = fetchFromGitHub {\n    owner = \"jesec\";\n    repo = pname;\n    rev = \"v${version}\";\n    hash = \"sha256-BR+ZGkBBfd0dSQqAvujsbgsEPFYw/ThrylxUbOksYxM=\";\n  };\n\n  npmDepsHash = \"sha256-tuEfyePwlOy2/mOPdXbqJskO6IowvAP4DWg8xSZwbJw=\";\n\n  # The prepack script runs the build script, which we'd rather do in the build phase.\n  npmPackFlags = [ \"--ignore-scripts\" ];\n\n  NODE_OPTIONS = \"--openssl-legacy-provider\";\n\n  meta = with lib; {\n    description = \"A modern web UI for various torrent clients with a Node.js backend and React frontend\";\n    homepage = \"https://flood.js.org\";\n    license = licenses.gpl3Only;\n    maintainers = with maintainers; [ winter ];\n  };\n}\n</code></pre> <p>In the default <code>installPhase</code> set by <code>buildNpmPackage</code>, it uses <code>npm pack --json --dry-run</code> to decide what files to install in <code>$out/lib/node_modules/$name/</code>, where <code>$name</code> is the <code>name</code> string defined in the package's <code>package.json</code>. Additionally, the <code>bin</code> and <code>man</code> keys in the source's <code>package.json</code> are used to decide what binaries and manpages are supposed to be installed. If these are not defined, <code>npm pack</code> may miss some files, and no binaries will be produced.</p>"},{"location":"languages-frameworks/javascript.section.html#javascript-buildNpmPackage-arguments","title":"Arguments","text":"<ul> <li><code>npmDepsHash</code>: The output hash of the dependencies for this project. Can be calculated in advance with <code>prefetch-npm-deps</code>.</li> <li><code>makeCacheWritable</code>: Whether to make the cache writable prior to installing dependencies. Don't set this unless npm tries to write to the cache directory, as it can slow down the build.</li> <li><code>npmBuildScript</code>: The script to run to build the project. Defaults to <code>\"build\"</code>.</li> <li><code>npmWorkspace</code>: The workspace directory within the project to build and install.</li> <li><code>dontNpmBuild</code>: Option to disable running the build script. Set to <code>true</code> if the package does not have a build script. Defaults to <code>false</code>. Alternatively, setting <code>buildPhase</code> explicitly also disables this.</li> <li><code>dontNpmInstall</code>: Option to disable running <code>npm install</code>. Defaults to <code>false</code>. Alternatively, setting <code>installPhase</code> explicitly also disables this.</li> <li><code>npmFlags</code>: Flags to pass to all npm commands.</li> <li><code>npmInstallFlags</code>: Flags to pass to <code>npm ci</code>.</li> <li><code>npmBuildFlags</code>: Flags to pass to <code>npm run ${npmBuildScript}</code>.</li> <li><code>npmPackFlags</code>: Flags to pass to <code>npm pack</code>.</li> <li><code>npmPruneFlags</code>: Flags to pass to <code>npm prune</code>. Defaults to the value of <code>npmInstallFlags</code>.</li> <li><code>makeWrapperArgs</code>: Flags to pass to <code>makeWrapper</code>, added to executable calling the generated <code>.js</code> with <code>node</code> as an interpreter. These scripts are defined in <code>package.json</code>.</li> <li><code>nodejs</code>: The <code>nodejs</code> package to build against, using the corresponding <code>npm</code> shipped with that version of <code>node</code>. Defaults to <code>pkgs.nodejs</code>.</li> <li><code>npmDeps</code>: The dependencies used to build the npm package. Especially useful to not have to recompute workspace depedencies.</li> </ul>"},{"location":"languages-frameworks/javascript.section.html#javascript-buildNpmPackage-prefetch-npm-deps","title":"prefetch-npm-deps","text":"<p><code>prefetch-npm-deps</code> is a Nixpkgs package that calculates the hash of the dependencies of an npm project ahead of time.</p> <pre><code>$ ls\npackage.json package-lock.json index.js\n$ prefetch-npm-deps package-lock.json\n...\nsha256-AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=\n</code></pre>"},{"location":"languages-frameworks/javascript.section.html#javascript-buildNpmPackage-fetchNpmDeps","title":"fetchNpmDeps","text":"<p><code>fetchNpmDeps</code> is a Nix function that requires the following mandatory arguments:</p> <ul> <li><code>src</code>: A directory / tarball with <code>package-lock.json</code> file</li> <li><code>hash</code>: The output hash of the node dependencies defined in <code>package-lock.json</code>.</li> </ul> <p>It returns a derivation with all <code>package-lock.json</code> dependencies downloaded into <code>$out/</code>, usable as an npm cache.</p>"},{"location":"languages-frameworks/javascript.section.html#javascript-corepack","title":"corepack","text":"<p>This package puts the corepack wrappers for pnpm and yarn in your PATH, and they will honor the <code>packageManager</code> setting in the <code>package.json</code>.</p>"},{"location":"languages-frameworks/javascript.section.html#javascript-node2nix","title":"node2nix","text":""},{"location":"languages-frameworks/javascript.section.html#javascript-node2nix-preparation","title":"Preparation","text":"<p>You will need to generate a Nix expression for the dependencies. Don't forget the <code>-l package-lock.json</code> if there is a lock file. Most probably you will need the <code>--development</code> to include the <code>devDependencies</code></p> <p>So the command will most likely be: <pre><code>node2nix --development -l package-lock.json\n</code></pre></p> <p>See <code>node2nix</code> docs for more info.</p>"},{"location":"languages-frameworks/javascript.section.html#javascript-node2nix-pitfalls","title":"Pitfalls","text":"<ul> <li>If upstream package.json does not have a \"version\" attribute, <code>node2nix</code> will crash. You will need to add it like shown in the package.json section.</li> <li><code>node2nix</code> has some bugs related to working with lock files from NPM distributed with <code>nodejs_16</code>.</li> <li><code>node2nix</code> does not like missing packages from NPM. If you see something like <code>Cannot resolve version: vue-loader-v16@undefined</code> then you might want to try another tool. The package might have been pulled off of NPM.</li> </ul>"},{"location":"languages-frameworks/javascript.section.html#javascript-yarn2nix","title":"yarn2nix","text":""},{"location":"languages-frameworks/javascript.section.html#javascript-yarn2nix-preparation","title":"Preparation","text":"<p>You will need at least a <code>yarn.lock</code> file. If upstream does not have one you need to generate it and reference it in your package definition.</p> <p>If the downloaded files contain the <code>package.json</code> and <code>yarn.lock</code> files they can be used like this:</p> <pre><code>offlineCache = fetchYarnDeps {\n  yarnLock = src + \"/yarn.lock\";\n  hash = \"....\";\n};\n</code></pre>"},{"location":"languages-frameworks/javascript.section.html#javascript-yarn2nix-mkYarnPackage","title":"mkYarnPackage","text":"<p><code>mkYarnPackage</code> will by default try to generate a binary. For package only generating static assets (Svelte, Vue, React, WebPack, ...), you will need to explicitly override the build step with your instructions.</p> <p>It's important to use the <code>--offline</code> flag. For example if you script is <code>\"build\": \"something\"</code> in <code>package.json</code> use:</p> <pre><code>buildPhase = ''\n  export HOME=$(mktemp -d)\n  yarn --offline build\n'';\n</code></pre> <p>The dist phase is also trying to build a binary, the only way to override it is with:</p> <pre><code>distPhase = \"true\";\n</code></pre> <p>The configure phase can sometimes fail because it makes many assumptions which may not always apply. One common override is:</p> <pre><code>configurePhase = ''\n  ln -s $node_modules node_modules\n'';\n</code></pre> <p>or if you need a writeable node_modules directory:</p> <pre><code>configurePhase = ''\n  cp -r $node_modules node_modules\n  chmod +w node_modules\n'';\n</code></pre>"},{"location":"languages-frameworks/javascript.section.html#javascript-yarn2nix-mkYarnModules","title":"mkYarnModules","text":"<p>This will generate a derivation including the <code>node_modules</code> directory. If you have to build a derivation for an integrated web framework (rails, phoenix..), this is probably the easiest way.</p>"},{"location":"languages-frameworks/javascript.section.html#javascript-mkYarnPackage-overriding-dependencies","title":"Overriding dependency behavior","text":"<p>In the <code>mkYarnPackage</code> record the property <code>pkgConfig</code> can be used to override packages when you encounter problems building.</p> <p>For instance, say your package is throwing errors when trying to invoke node-sass:</p> <pre><code>ENOENT: no such file or directory, scandir '/build/source/node_modules/node-sass/vendor'\n</code></pre> <p>To fix this we will specify different versions of build inputs to use, as well as some post install steps to get the software built the way we want:</p> <pre><code>mkYarnPackage rec {\n  pkgConfig = {\n    node-sass = {\n      buildInputs = with final;[ python libsass pkg-config ];\n      postInstall = ''\n        LIBSASS_EXT=auto yarn --offline run build\n        rm build/config.gypi\n      '';\n    };\n  };\n}\n</code></pre>"},{"location":"languages-frameworks/javascript.section.html#javascript-yarn2nix-pitfalls","title":"Pitfalls","text":"<ul> <li>If version is missing from upstream package.json, yarn will silently install nothing. In that case, you will need to override package.json as shown in the package.json section</li> <li>Having trouble with <code>node-gyp</code>? Try adding these lines to the <code>yarnPreBuild</code> steps:</li> </ul> <pre><code>yarnPreBuild = ''\n  mkdir -p $HOME/.node-gyp/${nodejs.version}\n  echo 9 &gt; $HOME/.node-gyp/${nodejs.version}/installVersion\n  ln -sfv ${nodejs}/include $HOME/.node-gyp/${nodejs.version}\n  export npm_config_nodedir=${nodejs}\n'';\n</code></pre> <ul> <li>The <code>echo 9</code> steps comes from this answer: https://stackoverflow.com/a/49139496</li> <li>Exporting the headers in <code>npm_config_nodedir</code> comes from this issue: https://github.com/nodejs/node-gyp/issues/1191#issuecomment-301243919</li> <li><code>offlineCache</code> (described above) must be specified to avoid Import From Derivation (IFD) when used inside Nixpkgs.</li> </ul>"},{"location":"languages-frameworks/javascript.section.html#javascript-outside-nixpkgs","title":"Outside Nixpkgs","text":"<p>There are some other tools available, which are written in the Nix language. These that can't be used inside Nixpkgs because they require Import From Derivation, which is not allowed in Nixpkgs.</p> <p>If you are packaging something outside Nixpkgs, consider the following:</p>"},{"location":"languages-frameworks/javascript.section.html#javascript-npmlock2nix","title":"npmlock2nix","text":"<p>npmlock2nix aims at building <code>node_modules</code> without code generation. It hasn't reached v1 yet, the API might be subject to change.</p>"},{"location":"languages-frameworks/javascript.section.html#javascript-npmlock2nix-pitfalls","title":"Pitfalls","text":"<p>There are some problems with npm v7.</p>"},{"location":"languages-frameworks/javascript.section.html#javascript-nix-npm-buildpackage","title":"nix-npm-buildpackage","text":"<p>nix-npm-buildpackage aims at building <code>node_modules</code> without code generation. It hasn't reached v1 yet, the API might change. It supports both <code>package-lock.json</code> and yarn.lock.</p>"},{"location":"languages-frameworks/javascript.section.html#javascript-nix-npm-buildpackage-pitfalls","title":"Pitfalls","text":"<p>There are some problems with npm v7.</p>"},{"location":"languages-frameworks/julia.section.html","title":"Julia","text":""},{"location":"languages-frameworks/julia.section.html#julia-introduction","title":"Introduction","text":"<p>Nixpkgs includes Julia as the <code>julia</code> derivation. You can get specific versions by looking at the other <code>julia*</code> top-level derivations available. For example, <code>julia_19</code> corresponds to Julia 1.9. We also provide the current stable version as <code>julia-stable</code>, and an LTS version as <code>julia-lts</code>.</p> <p>Occasionally, a Julia version has been too difficult to build from source in Nixpkgs and has been fetched prebuilt instead. These Julia versions are differentiated with the <code>*-bin</code> suffix; for example, <code>julia-stable-bin</code>.</p>"},{"location":"languages-frameworks/julia.section.html#julia-withpackage","title":"julia.withPackages","text":"<p>The basic Julia derivations only provide the built-in packages that come with the distribution.</p> <p>You can build Julia environments with additional packages using the <code>julia.withPackages</code> command. This function accepts a list of strings representing Julia package names. For example, you can build a Julia environment with the <code>Plots</code> package as follows.</p> <pre><code>julia.withPackages [\"Plots\"]\n</code></pre> <p>Arguments can be passed using <code>.override</code>. For example:</p> <pre><code>(julia.withPackages.override {\n  precompile = false; # Turn off precompilation\n}) [\"Plots\"]\n</code></pre> <p>Here's a nice way to run a Julia environment with a shell one-liner:</p> <pre><code>nix-shell -p 'julia.withPackages [\"Plots\"]' --run julia\n</code></pre>"},{"location":"languages-frameworks/julia.section.html#julia-withpackage-arguments","title":"Arguments","text":"<ul> <li><code>precompile</code>: Whether to run <code>Pkg.precompile()</code> on the generated environment.</li> </ul> <p>This will make package imports faster, but may fail in some cases.   For example, there is an upstream issue with <code>Gtk.jl</code> that prevents precompilation from working in the Nix build sandbox, because the precompiled code tries to access a display.   Packages like this will work fine if you build with <code>precompile=false</code>, and then precompile as needed once your environment starts.</p> <p>Defaults: <code>true</code></p> <ul> <li><code>extraLibs</code>: Extra library dependencies that will be placed on the <code>LD_LIBRARY_PATH</code> for Julia.</li> </ul> <p>Should not be needed as we try to obtain library dependencies automatically using Julia's artifacts system.</p> <ul> <li><code>makeWrapperArgs</code>: Extra arguments to pass to the <code>makeWrapper</code> call which we use to wrap the Julia binary.</li> <li><code>setDefaultDepot</code>: Whether to automatically prepend <code>$HOME/.julia</code> to the <code>JULIA_DEPOT_PATH</code>.</li> </ul> <p>This is useful because Julia expects a writable depot path as the first entry, which the one we build in Nixpkgs is not.   If there's no writable depot, then Julia will show a warning and be unable to save command history logs etc.</p> <p>Default: <code>true</code></p> <ul> <li><code>packageOverrides</code>: Allows you to override packages by name by passing an alternative source.</li> </ul> <p>For example, you can use a custom version of the <code>LanguageServer</code> package by passing <code>packageOverrides = { \"LanguageServer\" = fetchFromGitHub {...}; }</code>.</p> <ul> <li><code>augmentedRegistry</code>: Allows you to change the registry from which Julia packages are drawn.</li> </ul> <p>This normally points at a special augmented version of the Julia General packages registry.   If you want to use a bleeding-edge version to pick up the latest package updates, you can plug in a later revision than the one in Nixpkgs.</p>"},{"location":"languages-frameworks/lisp.section.html","title":"lisp-modules","text":"<p>This document describes the Nixpkgs infrastructure for building Common Lisp systems that use ASDF (Another System Definition Facility). It lives in <code>pkgs/development/lisp-modules</code>.</p>"},{"location":"languages-frameworks/lisp.section.html#lisp-overview","title":"Overview","text":"<p>The main entry point of the API are the Common Lisp implementation packages themselves (e.g. <code>abcl</code>, <code>ccl</code>, <code>clasp-common-lisp</code>, <code>clisp</code>, <code>ecl</code>, <code>sbcl</code>). They have the <code>pkgs</code> and <code>withPackages</code> attributes, which can be used to discover available packages and to build wrappers, respectively.</p> <p>The <code>pkgs</code> attribute set contains packages that were automatically imported from Quicklisp, and any other manually defined ones. Not every package works for all the CL implementations (e.g. <code>nyxt</code> only makes sense for <code>sbcl</code>).</p> <p>The <code>withPackages</code> function is of primary utility. It is used to build runnable wrappers, with a pinned and pre-built ASDF FASL available in the <code>ASDF</code> environment variable, and <code>CL_SOURCE_REGISTRY</code>/<code>ASDF_OUTPUT_TRANSLATIONS</code> configured to find the desired systems on runtime.</p> <p>In addition, Lisps have the <code>withOverrides</code> function, which can be used to substitute any package in the scope of their <code>pkgs</code>. This will also be useful together with <code>overrideLispAttrs</code> when dealing with slashy systems, because they should stay in the main package and be built by specifying the <code>systems</code> argument to <code>build-asdf-system</code>.</p>"},{"location":"languages-frameworks/lisp.section.html#lisp-use-case-example","title":"The 90% use case example","text":"<p>The most common way to use the library is to run ad-hoc wrappers like this:</p> <p><code>nix-shell -p 'sbcl.withPackages (ps: with ps; [ alexandria ])'</code></p> <p>Then, in a shell:</p> <pre><code>$ sbcl\n* (load (sb-ext:posix-getenv \"ASDF\"))\n* (asdf:load-system 'alexandria)\n</code></pre> <p>Also one can create a <code>pkgs.mkShell</code> environment in <code>shell.nix</code>/<code>flake.nix</code>:</p> <pre><code>let\n  sbcl' = sbcl.withPackages (ps: [ ps.alexandria ]);\nin mkShell {\n  packages = [ sbcl' ];\n}\n</code></pre> <p>Such a Lisp can be now used e.g. to compile your sources:</p> <pre><code>buildPhase = ''\n  ${sbcl'}/bin/sbcl --load my-build-file.lisp\n''\n</code></pre>"},{"location":"languages-frameworks/lisp.section.html#lisp-importing-packages-from-quicklisp","title":"Importing packages from Quicklisp","text":"<p>To save some work of writing Nix expressions, there is a script that imports all the packages distributed by Quicklisp into <code>imported.nix</code>. This works by parsing its <code>releases.txt</code> and <code>systems.txt</code> files, which are published every couple of months on quicklisp.org.</p> <p>The import process is implemented in the <code>import</code> directory as Common Lisp code in the <code>org.lispbuilds.nix</code> ASDF system. To run the script, one can execute <code>ql-import.lisp</code>:</p> <pre><code>cd pkgs/development/lisp-modules\nnix-shell --run 'sbcl --script ql-import.lisp'\n</code></pre> <p>The script will:</p> <ol> <li>Download the latest Quicklisp <code>systems.txt</code> and <code>releases.txt</code> files</li> <li>Generate a temporary SQLite database of all QL systems in <code>packages.sqlite</code></li> <li>Generate an <code>imported.nix</code> file from the database</li> </ol> <p>(The <code>packages.sqlite</code> file can be deleted at will, because it is regenerated each time the script runs.)</p> <p>The maintainer's job is to:</p> <ol> <li>Re-run the <code>ql-import.lisp</code> script when there is a new Quicklisp release</li> <li>Add any missing native dependencies in <code>ql.nix</code></li> <li>For packages that still don't build, package them manually in <code>packages.nix</code></li> </ol> <p>Also, the <code>imported.nix</code> file must not be edited manually! It should only be generated as described in this section (by running <code>ql-import.lisp</code>).</p>"},{"location":"languages-frameworks/lisp.section.html#lisp-quicklisp-adding-native-dependencies","title":"Adding native dependencies","text":"<p>The Quicklisp files contain ASDF dependency data, but don't include native library (CFFI) dependencies, and, in the case of ABCL, Java dependencies.</p> <p>The <code>ql.nix</code> file contains a long list of overrides, where these dependencies can be added.</p> <p>Packages defined in <code>packages.nix</code> contain these dependencies naturally.</p>"},{"location":"languages-frameworks/lisp.section.html#lisp-quicklisp-trusting","title":"Trusting <code>systems.txt</code> and <code>releases.txt</code>","text":"<p>The previous implementation of <code>lisp-modules</code> didn't fully trust the Quicklisp data, because there were times where the dependencies specified were not complete and caused broken builds. It instead used a <code>nix-shell</code> environment to discover real dependencies by using the ASDF APIs.</p> <p>The current implementation has chosen to trust this data, because it's faster to parse a text file than to build each system to generate its Nix file, and because that way packages can be mass-imported. Because of that, there may come a day where some packages will break, due to bugs in Quicklisp. In that case, the fix could be a manual override in <code>packages.nix</code> and <code>ql.nix</code>.</p> <p>A known fact is that Quicklisp doesn't include dependencies on slashy systems in its data. This is an example of a situation where such fixes were used, e.g. to replace the <code>systems</code> attribute of the affected packages. (See the definition of <code>iolib</code>).</p>"},{"location":"languages-frameworks/lisp.section.html#lisp-quicklisp-quirks","title":"Quirks","text":"<p>During Quicklisp import:</p> <ul> <li><code>+</code> in names is converted to <code>_plus{_,}</code>: <code>cl+ssl</code>-&gt;<code>cl_plus_ssl</code>, <code>alexandria+</code>-&gt;<code>alexandria_plus</code></li> <li><code>.</code> in names is converted to <code>_dot_</code>: <code>iolib.base</code>-&gt;<code>iolib_dot_base</code></li> <li>names starting with a number have a <code>_</code> prepended (<code>3d-vectors</code>-&gt;<code>_3d-vectors</code>)</li> <li><code>_</code> in names is converted to <code>__</code> for reversibility</li> </ul>"},{"location":"languages-frameworks/lisp.section.html#lisp-defining-packages-inside","title":"Defining packages manually inside Nixpkgs","text":"<p>Packages that for some reason are not in Quicklisp, and so cannot be auto-imported, or don't work straight from the import, are defined in the <code>packages.nix</code> file.</p> <p>In that file, use the <code>build-asdf-system</code> function, which is a wrapper around <code>mkDerivation</code> for building ASDF systems. Various other hacks are present, such as <code>build-with-compile-into-pwd</code> for systems which create files during compilation (such as cl-unicode).</p> <p>The <code>build-asdf-system</code> function is documented here. Also, <code>packages.nix</code> is full of examples of how to use it.</p>"},{"location":"languages-frameworks/lisp.section.html#lisp-defining-packages-outside","title":"Defining packages manually outside Nixpkgs","text":"<p>Lisp derivations (<code>abcl</code>, <code>sbcl</code> etc.) also export the <code>buildASDFSystem</code> function, which is similar to <code>build-asdf-system</code> from <code>packages.nix</code>, but is part of the public API.</p> <p>It takes the following arguments:</p> <ul> <li><code>pname</code>: the package name</li> <li><code>version</code>: the package version</li> <li><code>src</code>: the package source</li> <li><code>patches</code>: patches to apply to the source before build</li> <li><code>nativeLibs</code>: native libraries used by CFFI and grovelling</li> <li><code>javaLibs</code>: Java libraries for ABCL</li> <li><code>lispLibs</code>: dependencies on other packages build with <code>buildASDFSystem</code></li> <li><code>systems</code>: list of systems to build</li> </ul> <p>It can be used to define packages outside Nixpkgs, and, for example, add them into the package scope with <code>withOverrides</code>.</p>"},{"location":"languages-frameworks/lisp.section.html#lisp-including-external-pkg-in-scope","title":"Including an external package in scope","text":"<p>A package defined outside Nixpkgs using <code>buildASDFSystem</code> can be woven into the Nixpkgs-provided scope like this:</p> <pre><code>let\n  alexandria = sbcl.buildASDFSystem rec {\n    pname = \"alexandria\";\n    version = \"1.4\";\n    src = fetchFromGitLab {\n      domain = \"gitlab.common-lisp.net\";\n      owner = \"alexandria\";\n      repo = \"alexandria\";\n      rev = \"v${version}\";\n      hash = \"sha256-1Hzxt65dZvgOFIljjjlSGgKYkj+YBLwJCACi5DZsKmQ=\";\n    };\n  };\n  sbcl' = sbcl.withOverrides (self: super: {\n    inherit alexandria;\n  });\nin sbcl'.pkgs.alexandria\n</code></pre>"},{"location":"languages-frameworks/lisp.section.html#lisp-overriding-package-attributes","title":"Overriding package attributes","text":"<p>Packages export the <code>overrideLispAttrs</code> function, which can be used to build a new package with different parameters.</p> <p>Example of overriding <code>alexandria</code>:</p> <pre><code>sbcl.pkgs.alexandria.overrideLispAttrs (oldAttrs: rec {\n  version = \"1.4\";\n  src = fetchFromGitLab {\n    domain = \"gitlab.common-lisp.net\";\n    owner = \"alexandria\";\n    repo = \"alexandria\";\n    rev = \"v${version}\";\n    hash = \"sha256-1Hzxt65dZvgOFIljjjlSGgKYkj+YBLwJCACi5DZsKmQ=\";\n  };\n})\n</code></pre>"},{"location":"languages-frameworks/lisp.section.html#lisp-dealing-with-slashy-systems","title":"Dealing with slashy systems","text":"<p>Slashy (secondary) systems should not exist in their own packages! Instead, they should be included in the parent package as an extra entry in the <code>systems</code> argument to the <code>build-asdf-system</code>/<code>buildASDFSystem</code> functions.</p> <p>The reason is that ASDF searches for a secondary system in the <code>.asd</code> of the parent package. Thus, having them separate would cause either one of them not to load cleanly, because one will contains FASLs of itself but not the other, and vice versa.</p> <p>To package slashy systems, use <code>overrideLispAttrs</code>, like so:</p> <pre><code>ecl.pkgs.alexandria.overrideLispAttrs (oldAttrs: {\n  systems = oldAttrs.systems ++ [ \"alexandria/tests\" ];\n  lispLibs = oldAttrs.lispLibs ++ [ ecl.pkgs.rt ];\n})\n</code></pre> <p>See the respective section on using <code>withOverrides</code> for how to weave it back into <code>ecl.pkgs</code>.</p> <p>Note that sometimes the slashy systems might not only have more dependencies than the main one, but create a circular dependency between <code>.asd</code> files. Unfortunately, in this case an adhoc solution becomes necessary.</p>"},{"location":"languages-frameworks/lisp.section.html#lisp-building-wrappers","title":"Building Wrappers","text":"<p>Wrappers can be built using the <code>withPackages</code> function of Common Lisp implementations (<code>abcl</code>, <code>ecl</code>, <code>sbcl</code> etc.):</p> <pre><code>nix-shell -p 'sbcl.withPackages (ps: [ ps.alexandria ps.bordeaux-threads ])'\n</code></pre> <p>Such a wrapper can then be used like this:</p> <pre><code>$ sbcl\n* (load (sb-ext:posix-getenv \"ASDF\"))\n* (asdf:load-system 'alexandria)\n* (asdf:load-system 'bordeaux-threads)\n</code></pre>"},{"location":"languages-frameworks/lisp.section.html#lisp-loading-asdf","title":"Loading ASDF","text":"<p>For best results, avoid calling <code>(require 'asdf)</code> When using the library-generated wrappers.</p> <p>Use <code>(load (ext:getenv \"ASDF\"))</code> instead, supplying your implementation's way of getting an environment variable for <code>ext:getenv</code>. This will load the (pre-compiled to FASL) Nixpkgs-provided version of ASDF.</p>"},{"location":"languages-frameworks/lisp.section.html#lisp-loading-systems","title":"Loading systems","text":"<p>There, you can use <code>asdf:load-system</code>. This works by setting the right values for the <code>CL_SOURCE_REGISTRY</code>/<code>ASDF_OUTPUT_TRANSLATIONS</code> environment variables, so that systems are found in the Nix store and pre-compiled FASLs are loaded.</p>"},{"location":"languages-frameworks/lisp.section.html#lisp-adding-a-new-lisp","title":"Adding a new Lisp","text":"<p>The function <code>wrapLisp</code> is used to wrap Common Lisp implementations. It adds the <code>pkgs</code>, <code>withPackages</code>, <code>withOverrides</code> and <code>buildASDFSystem</code> attributes to the derivation.</p> <p><code>wrapLisp</code> takes these arguments:</p> <ul> <li><code>pkg</code>: the Lisp package</li> <li><code>faslExt</code>: Implementation-specific extension for FASL files</li> <li><code>program</code>: The name of executable file in <code>${pkg}/bin/</code> (Default: <code>pkg.pname</code>)</li> <li><code>flags</code>: A list of flags to always pass to <code>program</code> (Default: <code>[]</code>)</li> <li><code>asdf</code>: The ASDF version to use (Default: <code>pkgs.asdf_3_3</code>)</li> <li><code>packageOverrides</code>: Package overrides config (Default: <code>(self: super: {})</code>)</li> </ul> <p>This example wraps CLISP:</p> <pre><code>wrapLisp {\n  pkg = clisp;\n  faslExt = \"fas\";\n  flags = [\"-E\" \"UTF8\"];\n}\n</code></pre>"},{"location":"languages-frameworks/lua.section.html","title":"User\u2019s Guide to Lua Infrastructure","text":""},{"location":"languages-frameworks/lua.section.html#using-lua","title":"Using Lua","text":""},{"location":"languages-frameworks/lua.section.html#overview-of-lua","title":"Overview of Lua","text":"<p>Several versions of the Lua interpreter are available: luajit, lua 5.1, 5.2, 5.3. The attribute <code>lua</code> refers to the default interpreter, it is also possible to refer to specific versions, e.g. <code>lua5_2</code> refers to Lua 5.2.</p> <p>Lua libraries are in separate sets, with one set per interpreter version.</p> <p>The interpreters have several common attributes. One of these attributes is <code>pkgs</code>, which is a package set of Lua libraries for this specific interpreter. E.g., the <code>busted</code> package corresponding to the default interpreter is <code>lua.pkgs.busted</code>, and the lua 5.2 version is <code>lua5_2.pkgs.busted</code>. The main package set contains aliases to these package sets, e.g. <code>luaPackages</code> refers to <code>lua5_1.pkgs</code> and <code>lua52Packages</code> to <code>lua5_2.pkgs</code>.</p>"},{"location":"languages-frameworks/lua.section.html#installing-lua-and-packages","title":"Installing Lua and packages","text":""},{"location":"languages-frameworks/lua.section.html#lua-environment-defined-in-separate-.nix-file","title":"Lua environment defined in separate <code>.nix</code> file","text":"<p>Create a file, e.g. <code>build.nix</code>, with the following expression</p> <pre><code>with import &lt;nixpkgs&gt; {};\n\nlua5_2.withPackages (ps: with ps; [ busted luafilesystem ])\n</code></pre> <p>and install it in your profile with</p> <p><pre><code>nix-env -if build.nix\n</code></pre> Now you can use the Lua interpreter, as well as the extra packages (<code>busted</code>, <code>luafilesystem</code>) that you added to the environment.</p>"},{"location":"languages-frameworks/lua.section.html#lua-environment-defined-in-.confignixpkgsconfig.nix","title":"Lua environment defined in <code>~/.config/nixpkgs/config.nix</code>","text":"<p>If you prefer to, you could also add the environment as a package override to the Nixpkgs set, e.g. using <code>config.nix</code>,</p> <pre><code>{ # ...\n\n  packageOverrides = pkgs: with pkgs; {\n    myLuaEnv = lua5_2.withPackages (ps: with ps; [ busted luafilesystem ]);\n  };\n}\n</code></pre> <p>and install it in your profile with</p> <p><pre><code>nix-env -iA nixpkgs.myLuaEnv\n</code></pre> The environment is installed by referring to the attribute, and considering the <code>nixpkgs</code> channel was used.</p>"},{"location":"languages-frameworks/lua.section.html#lua-environment-defined-in-etcnixosconfiguration.nix","title":"Lua environment defined in <code>/etc/nixos/configuration.nix</code>","text":"<p>For the sake of completeness, here's another example how to install the environment system-wide.</p> <pre><code>{ # ...\n\n  environment.systemPackages = with pkgs; [\n    (lua.withPackages(ps: with ps; [ busted luafilesystem ]))\n  ];\n}\n</code></pre>"},{"location":"languages-frameworks/lua.section.html#how-to-override-a-lua-package-using-overlays","title":"How to override a Lua package using overlays?","text":"<p>Use the following overlay template:</p> <pre><code>final: prev:\n{\n\n  lua = prev.lua.override {\n    packageOverrides = luaself: luaprev: {\n\n      luarocks-nix = luaprev.luarocks-nix.overrideAttrs(oa: {\n        pname = \"luarocks-nix\";\n        src = /home/my_luarocks/repository;\n      });\n  };\n\n  luaPackages = lua.pkgs;\n}\n</code></pre>"},{"location":"languages-frameworks/lua.section.html#temporary-lua-environment-with-nix-shell","title":"Temporary Lua environment with <code>nix-shell</code>","text":"<p>There are two methods for loading a shell with Lua packages. The first and recommended method is to create an environment with <code>lua.buildEnv</code> or <code>lua.withPackages</code> and load that. E.g.</p> <pre><code>$ nix-shell -p 'lua.withPackages(ps: with ps; [ busted luafilesystem ])'\n</code></pre> <p>opens a shell from which you can launch the interpreter</p> <pre><code>[nix-shell:~] lua\n</code></pre> <p>The other method, which is not recommended, does not create an environment and requires you to list the packages directly,</p> <p><pre><code>$ nix-shell -p lua.pkgs.busted lua.pkgs.luafilesystem\n</code></pre> Again, it is possible to launch the interpreter from the shell. The Lua interpreter has the attribute <code>pkgs</code> which contains all Lua libraries for that specific interpreter.</p>"},{"location":"languages-frameworks/lua.section.html#developing-with-lua","title":"Developing with Lua","text":"<p>Now that you know how to get a working Lua environment with Nix, it is time to go forward and start actually developing with Lua. There are two ways to package lua software, either it is on luarocks and most of it can be taken care of by the luarocks2nix converter or the packaging has to be done manually. Let's present the luarocks way first and the manual one in a second time.</p>"},{"location":"languages-frameworks/lua.section.html#packaging-a-library-on-luarocks","title":"Packaging a library on luarocks","text":"<p>Luarocks.org is the main repository of lua packages. The site proposes two types of packages, the <code>rockspec</code> and the <code>src.rock</code> (equivalent of a rockspec but with the source).</p> <p>Luarocks-based packages are generated in pkgs/development/lua-modules/generated-packages.nix from the whitelist maintainers/scripts/luarocks-packages.csv and updated by running the package <code>luarocks-packages-updater</code>:</p> <pre><code>nix-shell -p luarocks-packages-updater --run luarocks-packages-updater\n</code></pre> <p>luarocks2nix is a tool capable of generating nix derivations from both rockspec and src.rock (and favors the src.rock). The automation only goes so far though and some packages need to be customized. These customizations go in pkgs/development/lua-modules/overrides.nix. For instance if the rockspec defines <code>external_dependencies</code>, these need to be manually added to the overrides.nix.</p> <p>You can try converting luarocks packages to nix packages with the command <code>nix-shell -p luarocks-nix</code> and then <code>luarocks nix PKG_NAME</code>.</p>"},{"location":"languages-frameworks/lua.section.html#packaging-a-library-manually","title":"Packaging a library manually","text":"<p>You can develop your package as you usually would, just don't forget to wrap it within a <code>toLuaModule</code> call, for instance</p> <pre><code>mynewlib = toLuaModule ( stdenv.mkDerivation { ... });\n</code></pre> <p>There is also the <code>buildLuaPackage</code> function that can be used when lua modules are not packaged for luarocks. You can see a few examples at <code>pkgs/top-level/lua-packages.nix</code>.</p>"},{"location":"languages-frameworks/lua.section.html#lua-reference","title":"Lua Reference","text":""},{"location":"languages-frameworks/lua.section.html#lua-interpreters","title":"Lua interpreters","text":"<p>Versions 5.1, 5.2, 5.3 and 5.4 of the lua interpreter are available as respectively <code>lua5_1</code>, <code>lua5_2</code>, <code>lua5_3</code> and <code>lua5_4</code>. Luajit is available too. The Nix expressions for the interpreters can be found in <code>pkgs/development/interpreters/lua-5</code>.</p>"},{"location":"languages-frameworks/lua.section.html#attributes-on-lua-interpreters-packages","title":"Attributes on lua interpreters packages","text":"<p>Each interpreter has the following attributes:</p> <ul> <li><code>interpreter</code>. Alias for <code>${pkgs.lua}/bin/lua</code>.</li> <li><code>buildEnv</code>. Function to build lua interpreter environments with extra packages bundled together. See section lua.buildEnv function for usage and documentation.</li> <li><code>withPackages</code>. Simpler interface to <code>buildEnv</code>.</li> <li><code>pkgs</code>. Set of Lua packages for that specific interpreter. The package set can be modified by overriding the interpreter and passing <code>packageOverrides</code>.</li> </ul>"},{"location":"languages-frameworks/lua.section.html#buildluarockspackage-function","title":"<code>buildLuarocksPackage</code> function","text":"<p>The <code>buildLuarocksPackage</code> function is implemented in <code>pkgs/development/interpreters/lua-5/build-luarocks-package.nix</code> The following is an example: <pre><code>luaposix = buildLuarocksPackage {\n  pname = \"luaposix\";\n  version = \"34.0.4-1\";\n\n  src = fetchurl {\n    url    = \"https://raw.githubusercontent.com/rocks-moonscript-org/moonrocks-mirror/master/luaposix-34.0.4-1.src.rock\";\n    hash = \"sha256-4mLJG8n4m6y4Fqd0meUDfsOb9RHSR0qa/KD5KCwrNXs=\";\n  };\n  disabled = (luaOlder \"5.1\") || (luaAtLeast \"5.4\");\n  propagatedBuildInputs = [ bit32 lua std_normalize ];\n\n  meta = with lib; {\n    homepage = \"https://github.com/luaposix/luaposix/\";\n    description = \"Lua bindings for POSIX\";\n    maintainers = with maintainers; [ vyp lblasc ];\n    license.fullName = \"MIT/X11\";\n  };\n};\n</code></pre></p> <p>The <code>buildLuarocksPackage</code> delegates most tasks to luarocks:</p> <ul> <li>it adds <code>luarocks</code> as an unpacker for <code>src.rock</code> files (zip files really).</li> <li><code>configurePhase</code> writes a temporary luarocks configuration file which location is exported via the environment variable <code>LUAROCKS_CONFIG</code>.</li> <li>the <code>buildPhase</code> does nothing.</li> <li><code>installPhase</code> calls <code>luarocks make --deps-mode=none --tree $out</code> to build and install the package</li> <li>In the <code>postFixup</code> phase, the <code>wrapLuaPrograms</code> bash function is called to   wrap all programs in the <code>$out/bin/*</code> directory to include <code>$PATH</code>   environment variable and add dependent libraries to script's <code>LUA_PATH</code> and   <code>LUA_CPATH</code>.</li> </ul> <p>By default <code>meta.platforms</code> is set to the same value as the interpreter unless overridden otherwise.</p>"},{"location":"languages-frameworks/lua.section.html#buildluaapplication-function","title":"<code>buildLuaApplication</code> function","text":"<p>The <code>buildLuaApplication</code> function is practically the same as <code>buildLuaPackage</code>. The difference is that <code>buildLuaPackage</code> by default prefixes the names of the packages with the version of the interpreter. Because with an application we're not interested in multiple version the prefix is dropped.</p>"},{"location":"languages-frameworks/lua.section.html#lua.withpackages-function","title":"lua.withPackages function","text":"<p>The <code>lua.withPackages</code> takes a function as an argument that is passed the set of lua packages and returns the list of packages to be included in the environment. Using the <code>withPackages</code> function, the previous example for the luafilesystem environment can be written like this:</p> <pre><code>with import &lt;nixpkgs&gt; {};\n\nlua.withPackages (ps: [ps.luafilesystem])\n</code></pre> <p><code>withPackages</code> passes the correct package set for the specific interpreter version as an argument to the function. In the above example, <code>ps</code> equals <code>luaPackages</code>. But you can also easily switch to using <code>lua5_2</code>:</p> <pre><code>with import &lt;nixpkgs&gt; {};\n\nlua5_2.withPackages (ps: [ps.lua])\n</code></pre> <p>Now, <code>ps</code> is set to <code>lua52Packages</code>, matching the version of the interpreter.</p>"},{"location":"languages-frameworks/lua.section.html#possible-todos","title":"Possible Todos","text":"<ul> <li>export/use version specific variables such as <code>LUA_PATH_5_2</code>/<code>LUAROCKS_CONFIG_5_2</code></li> <li>let luarocks check for dependencies via exporting the different rocktrees in temporary config</li> </ul>"},{"location":"languages-frameworks/lua.section.html#lua-contributing-guidelines","title":"Lua Contributing guidelines","text":"<p>Following rules should be respected:</p> <ul> <li>Make sure libraries build for all Lua interpreters.</li> <li>Commit names of Lua libraries should reflect that they are Lua libraries, so write for example <code>luaPackages.luafilesystem: 1.11 -&gt; 1.12</code>.</li> </ul>"},{"location":"languages-frameworks/maven.section.html","title":"Maven","text":"<p>Maven is a well-known build tool for the Java ecosystem however it has some challenges when integrating into the Nix build system.</p> <p>The following provides a list of common patterns with how to package a Maven project (or any JVM language that can export to Maven) as a Nix package.</p>"},{"location":"languages-frameworks/maven.section.html#maven-buildmavenpackage","title":"Building a package using <code>maven.buildMavenPackage</code>","text":"<p>Consider the following package:</p> <pre><code>{ lib, fetchFromGitHub, jre, makeWrapper, maven }:\n\nmaven.buildMavenPackage rec {\n  pname = \"jd-cli\";\n  version = \"1.2.1\";\n\n  src = fetchFromGitHub {\n    owner = \"intoolswetrust\";\n    repo = pname;\n    rev = \"${pname}-${version}\";\n    hash = \"sha256-rRttA5H0A0c44loBzbKH7Waoted3IsOgxGCD2VM0U/Q=\";\n  };\n\n  mvnHash = \"sha256-kLpjMj05uC94/5vGMwMlFzLKNFOKeyNvq/vmB6pHTAo=\";\n\n  nativeBuildInputs = [ makeWrapper ];\n\n  installPhase = ''\n    mkdir -p $out/bin $out/share/jd-cli\n    install -Dm644 jd-cli/target/jd-cli.jar $out/share/jd-cli\n\n    makeWrapper ${jre}/bin/java $out/bin/jd-cli \\\n      --add-flags \"-jar $out/share/jd-cli/jd-cli.jar\"\n  '';\n\n  meta = with lib; {\n    description = \"Simple command line wrapper around JD Core Java Decompiler project\";\n    homepage = \"https://github.com/intoolswetrust/jd-cli\";\n    license = licenses.gpl3Plus;\n    maintainers = with maintainers; [ majiir ];\n  };\n}:\n</code></pre> <p>This package calls <code>maven.buildMavenPackage</code> to do its work. The primary difference from <code>stdenv.mkDerivation</code> is the <code>mvnHash</code> variable, which is a hash of all of the Maven dependencies.</p> <p>::: {.tip} After setting <code>maven.buildMavenPackage</code>, we then do standard Java <code>.jar</code> installation by saving the <code>.jar</code> to <code>$out/share/java</code> and then making a wrapper which allows executing that file; see  for additional generic information about packaging Java applications. :::</p>"},{"location":"languages-frameworks/maven.section.html#stable-maven-plugins","title":"Stable Maven plugins","text":"<p>Maven defines default versions for its core plugins, e.g. <code>maven-compiler-plugin</code>. If your project does not override these versions, an upgrade of Maven will change the version of the used plugins, and therefore the derivation and hash.</p> <p>When <code>maven</code> is upgraded, <code>mvnHash</code> for the derivation must be updated as well: otherwise, the project will be built on the derivation of old plugins, and fail because the requested plugins are missing.</p> <p>This clearly prevents automatic upgrades of Maven: a manual effort must be made throughout nixpkgs by any maintainer wishing to push the upgrades.</p> <p>To make sure that your package does not add extra manual effort when upgrading Maven, explicitly define versions for all plugins. You can check if this is the case by adding the following plugin to your (parent) POM:</p> <pre><code>&lt;plugin&gt;\n  &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\n  &lt;artifactId&gt;maven-enforcer-plugin&lt;/artifactId&gt;\n  &lt;version&gt;3.3.0&lt;/version&gt;\n  &lt;executions&gt;\n    &lt;execution&gt;\n      &lt;id&gt;enforce-plugin-versions&lt;/id&gt;\n      &lt;goals&gt;\n        &lt;goal&gt;enforce&lt;/goal&gt;\n      &lt;/goals&gt;\n      &lt;configuration&gt;\n        &lt;rules&gt;\n          &lt;requirePluginVersions /&gt;\n        &lt;/rules&gt;\n      &lt;/configuration&gt;\n    &lt;/execution&gt;\n  &lt;/executions&gt;\n&lt;/plugin&gt;\n</code></pre>"},{"location":"languages-frameworks/maven.section.html#maven-mvn2nix","title":"Manually using <code>mvn2nix</code>","text":"<p>::: {.warning} This way is no longer recommended; see  for the simpler and preferred way. :::</p> <p>For the purposes of this example let's consider a very basic Maven project with the following <code>pom.xml</code> with a single dependency on emoji-java.</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n        xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt;\n  &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;\n  &lt;groupId&gt;io.github.fzakaria&lt;/groupId&gt;\n  &lt;artifactId&gt;maven-demo&lt;/artifactId&gt;\n  &lt;version&gt;1.0&lt;/version&gt;\n  &lt;packaging&gt;jar&lt;/packaging&gt;\n  &lt;name&gt;NixOS Maven Demo&lt;/name&gt;\n\n  &lt;dependencies&gt;\n    &lt;dependency&gt;\n        &lt;groupId&gt;com.vdurmont&lt;/groupId&gt;\n        &lt;artifactId&gt;emoji-java&lt;/artifactId&gt;\n        &lt;version&gt;5.1.1&lt;/version&gt;\n      &lt;/dependency&gt;\n  &lt;/dependencies&gt;\n&lt;/project&gt;\n</code></pre> <p>Our main class file will be very simple:</p> <pre><code>import com.vdurmont.emoji.EmojiParser;\n\npublic class Main {\n  public static void main(String[] args) {\n    String str = \"NixOS :grinning: is super cool :smiley:!\";\n    String result = EmojiParser.parseToUnicode(str);\n    System.out.println(result);\n  }\n}\n</code></pre> <p>You find this demo project at https://github.com/fzakaria/nixos-maven-example.</p>"},{"location":"languages-frameworks/maven.section.html#solving-for-dependencies","title":"Solving for dependencies","text":""},{"location":"languages-frameworks/maven.section.html#buildmaven-with-nixosmvn2nix-maven-plugin","title":"buildMaven with NixOS/mvn2nix-maven-plugin","text":"<p><code>buildMaven</code> is an alternative method that tries to follow similar patterns of other programming languages by generating a lock file. It relies on the maven plugin mvn2nix-maven-plugin.</p> <p>First you generate a <code>project-info.json</code> file using the maven plugin.</p> <p>This should be executed in the project's source repository or be told which <code>pom.xml</code> to execute with.</p> <pre><code># run this step within the project's source repository\n\u276f mvn org.nixos.mvn2nix:mvn2nix-maven-plugin:mvn2nix\n\n\u276f cat project-info.json | jq | head\n{\n  \"project\": {\n    \"artifactId\": \"maven-demo\",\n    \"groupId\": \"org.nixos\",\n    \"version\": \"1.0\",\n    \"classifier\": \"\",\n    \"extension\": \"jar\",\n    \"dependencies\": [\n      {\n        \"artifactId\": \"maven-resources-plugin\",\n</code></pre> <p>This file is then given to the <code>buildMaven</code> function, and it returns 2 attributes.</p> <p><code>repo</code>:     A Maven repository that is a symlink farm of all the dependencies found in the <code>project-info.json</code></p> <p><code>build</code>:     A simple derivation that runs through <code>mvn compile</code> &amp; <code>mvn package</code> to build the JAR. You may use this as inspiration for more complicated derivations.</p> <p>Here is an example of building the Maven repository</p> <pre><code>{ pkgs ? import &lt;nixpkgs&gt; { } }:\nwith pkgs;\n(buildMaven ./project-info.json).repo\n</code></pre> <p>The benefit over the double invocation as we will see below, is that the /nix/store entry is a linkFarm of every package, so that changes to your dependency set doesn't involve downloading everything from scratch.</p> <pre><code>\u276f tree $(nix-build --no-out-link build-maven-repository.nix) | head\n/nix/store/g87va52nkc8jzbmi1aqdcf2f109r4dvn-maven-repository\n\u251c\u2500\u2500 antlr\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 antlr\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 2.7.2\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 antlr-2.7.2.jar -&gt; /nix/store/d027c8f2cnmj5yrynpbq2s6wmc9cb559-antlr-2.7.2.jar\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 antlr-2.7.2.pom -&gt; /nix/store/mv42fc5gizl8h5g5vpywz1nfiynmzgp2-antlr-2.7.2.pom\n\u251c\u2500\u2500 avalon-framework\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 avalon-framework\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 4.1.3\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 avalon-framework-4.1.3.jar -&gt; /nix/store/iv5fp3955w3nq28ff9xfz86wvxbiw6n9-avalon-framework-4.1.3.jar\n</code></pre>"},{"location":"languages-frameworks/maven.section.html#double-invocation","title":"Double Invocation","text":"<p>::: {.note} This pattern is the simplest but may cause unnecessary rebuilds due to the output hash changing. :::</p> <p>The double invocation is a simple way to get around the problem that <code>nix-build</code> may be sandboxed and have no Internet connectivity.</p> <p>It treats the entire Maven repository as a single source to be downloaded, relying on Maven's dependency resolution to satisfy the output hash. This is similar to fetchers like <code>fetchgit</code>, except it has to run a Maven build to determine what to download.</p> <p>The first step will be to build the Maven project as a fixed-output derivation in order to collect the Maven repository -- below is an example.</p> <p>::: {.note} Traditionally the Maven repository is at <code>~/.m2/repository</code>. We will override this to be the <code>$out</code> directory. :::</p> <pre><code>{ lib, stdenv, maven }:\nstdenv.mkDerivation {\n  name = \"maven-repository\";\n  buildInputs = [ maven ];\n  src = ./.; # or fetchFromGitHub, cleanSourceWith, etc\n  buildPhase = ''\n    mvn package -Dmaven.repo.local=$out\n  '';\n\n  # keep only *.{pom,jar,sha1,nbm} and delete all ephemeral files with lastModified timestamps inside\n  installPhase = ''\n    find $out -type f \\\n      -name \\*.lastUpdated -or \\\n      -name resolver-status.properties -or \\\n      -name _remote.repositories \\\n      -delete\n  '';\n\n  # don't do any fixup\n  dontFixup = true;\n  outputHashAlgo = \"sha256\";\n  outputHashMode = \"recursive\";\n  # replace this with the correct SHA256\n  outputHash = lib.fakeSha256;\n}\n</code></pre> <p>The build will fail, and tell you the expected <code>outputHash</code> to place. When you've set the hash, the build will return with a <code>/nix/store</code> entry whose contents are the full Maven repository.</p> <p>::: {.warning} Some additional files are deleted that would cause the output hash to change potentially on subsequent runs. :::</p> <pre><code>\u276f tree $(nix-build --no-out-link double-invocation-repository.nix) | head\n/nix/store/8kicxzp98j68xyi9gl6jda67hp3c54fq-maven-repository\n\u251c\u2500\u2500 backport-util-concurrent\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 backport-util-concurrent\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 3.1\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 backport-util-concurrent-3.1.pom\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 backport-util-concurrent-3.1.pom.sha1\n\u251c\u2500\u2500 classworlds\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 classworlds\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 1.1\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 classworlds-1.1.jar\n</code></pre> <p>If your package uses SNAPSHOT dependencies or version ranges; there is a strong likelihood that over-time your output hash will change since the resolved dependencies may change. Hence this method is less recommended then using <code>buildMaven</code>.</p>"},{"location":"languages-frameworks/maven.section.html#building-a-jar","title":"Building a JAR","text":"<p>Regardless of which strategy is chosen above, the step to build the derivation is the same.</p> <pre><code>{ stdenv, maven, callPackage }:\n# pick a repository derivation, here we will use buildMaven\nlet repository = callPackage ./build-maven-repository.nix { };\nin stdenv.mkDerivation rec {\n  pname = \"maven-demo\";\n  version = \"1.0\";\n\n  src = builtins.fetchTarball \"https://github.com/fzakaria/nixos-maven-example/archive/main.tar.gz\";\n  buildInputs = [ maven ];\n\n  buildPhase = ''\n    echo \"Using repository ${repository}\"\n    mvn --offline -Dmaven.repo.local=${repository} package;\n  '';\n\n  installPhase = ''\n    install -Dm644 target/${pname}-${version}.jar $out/share/java\n  '';\n}\n</code></pre> <p>::: {.tip} We place the library in <code>$out/share/java</code> since JDK package has a stdenv setup hook that adds any JARs in the <code>share/java</code> directories of the build inputs to the CLASSPATH environment. :::</p> <pre><code>\u276f tree $(nix-build --no-out-link build-jar.nix)\n/nix/store/7jw3xdfagkc2vw8wrsdv68qpsnrxgvky-maven-demo-1.0\n\u2514\u2500\u2500 share\n    \u2514\u2500\u2500 java\n        \u2514\u2500\u2500 maven-demo-1.0.jar\n\n2 directories, 1 file\n</code></pre>"},{"location":"languages-frameworks/maven.section.html#runnable-jar","title":"Runnable JAR","text":"<p>The previous example builds a <code>jar</code> file but that's not a file one can run.</p> <p>You need to use it with <code>java -jar $out/share/java/output.jar</code> and make sure to provide the required dependencies on the classpath.</p> <p>The following explains how to use <code>makeWrapper</code> in order to make the derivation produce an executable that will run the JAR file you created.</p> <p>We will use the same repository we built above (either double invocation or buildMaven) to setup a CLASSPATH for our JAR.</p> <p>The following two methods are more suited to Nix then building an UberJar which may be the more traditional approach.</p>"},{"location":"languages-frameworks/maven.section.html#classpath","title":"CLASSPATH","text":"<p>This method is ideal if you are providing a derivation for nixpkgs and don't want to patch the project's <code>pom.xml</code>.</p> <p>We will read the Maven repository and flatten it to a single list. This list will then be concatenated with the CLASSPATH separator to create the full classpath.</p> <p>We make sure to provide this classpath to the <code>makeWrapper</code>.</p> <pre><code>{ stdenv, maven, callPackage, makeWrapper, jre }:\nlet\n  repository = callPackage ./build-maven-repository.nix { };\nin stdenv.mkDerivation rec {\n  pname = \"maven-demo\";\n  version = \"1.0\";\n\n  src = builtins.fetchTarball\n    \"https://github.com/fzakaria/nixos-maven-example/archive/main.tar.gz\";\n  nativeBuildInputs = [ makeWrapper ];\n  buildInputs = [ maven ];\n\n  buildPhase = ''\n    echo \"Using repository ${repository}\"\n    mvn --offline -Dmaven.repo.local=${repository} package;\n  '';\n\n  installPhase = ''\n    mkdir -p $out/bin\n\n    classpath=$(find ${repository} -name \"*.jar\" -printf ':%h/%f');\n    install -Dm644 target/${pname}-${version}.jar $out/share/java\n    # create a wrapper that will automatically set the classpath\n    # this should be the paths from the dependency derivation\n    makeWrapper ${jre}/bin/java $out/bin/${pname} \\\n          --add-flags \"-classpath $out/share/java/${pname}-${version}.jar:''${classpath#:}\" \\\n          --add-flags \"Main\"\n  '';\n}\n</code></pre>"},{"location":"languages-frameworks/maven.section.html#manifest-file-via-maven-plugin","title":"MANIFEST file via Maven Plugin","text":"<p>This method is ideal if you are the project owner and want to change your <code>pom.xml</code> to set the CLASSPATH within it.</p> <p>Augment the <code>pom.xml</code> to create a JAR with the following manifest:</p> <pre><code>&lt;build&gt;\n  &lt;plugins&gt;\n    &lt;plugin&gt;\n        &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt;\n        &lt;configuration&gt;\n            &lt;archive&gt;\n                &lt;manifest&gt;\n                    &lt;addClasspath&gt;true&lt;/addClasspath&gt;\n                    &lt;classpathPrefix&gt;../../repository/&lt;/classpathPrefix&gt;\n                    &lt;classpathLayoutType&gt;repository&lt;/classpathLayoutType&gt;\n                    &lt;mainClass&gt;Main&lt;/mainClass&gt;\n                &lt;/manifest&gt;\n                &lt;manifestEntries&gt;\n                    &lt;Class-Path&gt;.&lt;/Class-Path&gt;\n                &lt;/manifestEntries&gt;\n            &lt;/archive&gt;\n        &lt;/configuration&gt;\n    &lt;/plugin&gt;\n  &lt;/plugins&gt;\n&lt;/build&gt;\n</code></pre> <p>The above plugin instructs the JAR to look for the necessary dependencies in the <code>lib/</code> relative folder. The layout of the folder is also in the maven repository style.</p> <pre><code>\u276f unzip -q -c $(nix-build --no-out-link runnable-jar.nix)/share/java/maven-demo-1.0.jar META-INF/MANIFEST.MF\n\nManifest-Version: 1.0\nArchiver-Version: Plexus Archiver\nBuilt-By: nixbld\nClass-Path: . ../../repository/com/vdurmont/emoji-java/5.1.1/emoji-jav\n a-5.1.1.jar ../../repository/org/json/json/20170516/json-20170516.jar\nCreated-By: Apache Maven 3.6.3\nBuild-Jdk: 1.8.0_265\nMain-Class: Main\n</code></pre> <p>We will modify the derivation above to add a symlink to our repository so that it's accessible to our JAR during the <code>installPhase</code>.</p> <p><pre><code>{ stdenv, maven, callPackage, makeWrapper, jre }:\n# pick a repository derivation, here we will use buildMaven\nlet repository = callPackage ./build-maven-repository.nix { };\nin stdenv.mkDerivation rec {\n  pname = \"maven-demo\";\n  version = \"1.0\";\n\n  src = builtins.fetchTarball\n    \"https://github.com/fzakaria/nixos-maven-example/archive/main.tar.gz\";\n  nativeBuildInputs = [ makeWrapper ];\n  buildInputs = [ maven ];\n\n  buildPhase = ''\n    echo \"Using repository ${repository}\"\n    mvn --offline -Dmaven.repo.local=${repository} package;\n  '';\n\n  installPhase = ''\n    mkdir -p $out/bin\n\n    # create a symbolic link for the repository directory\n    ln -s ${repository} $out/repository\n\n    install -Dm644 target/${pname}-${version}.jar $out/share/java\n    # create a wrapper that will automatically set the classpath\n    # this should be the paths from the dependency derivation\n    makeWrapper ${jre}/bin/java $out/bin/${pname} \\\n          --add-flags \"-jar $out/share/java/${pname}-${version}.jar\"\n  '';\n}\n</code></pre> ::: {.note} Our script produces a dependency on <code>jre</code> rather than <code>jdk</code> to restrict the runtime closure necessary to run the application. :::</p> <p>This will give you an executable shell-script that launches your JAR with all the dependencies available.</p> <pre><code>\u276f tree $(nix-build --no-out-link runnable-jar.nix)\n/nix/store/8d4c3ibw8ynsn01ibhyqmc1zhzz75s26-maven-demo-1.0\n\u251c\u2500\u2500 bin\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 maven-demo\n\u251c\u2500\u2500 repository -&gt; /nix/store/g87va52nkc8jzbmi1aqdcf2f109r4dvn-maven-repository\n\u2514\u2500\u2500 share\n    \u2514\u2500\u2500 java\n        \u2514\u2500\u2500 maven-demo-1.0.jar\n\n\u276f $(nix-build --no-out-link --option tarball-ttl 1 runnable-jar.nix)/bin/maven-demo\nNixOS \ud83d\ude00 is super cool \ud83d\ude03!\n</code></pre>"},{"location":"languages-frameworks/nim.section.html","title":"Nim","text":"<p>The Nim compiler and a builder function is available. Nim programs are built using <code>buildNimPackage</code> and a lockfile containing Nim dependencies.</p> <p>The following example shows a Nim program that depends only on Nim libraries: <pre><code>{ lib, buildNimPackage, fetchFromGitHub }:\n\nbuildNimPackage { } (finalAttrs: {\n  pname = \"ttop\";\n  version = \"1.2.7\";\n\n  src = fetchFromGitHub {\n    owner = \"inv2004\";\n    repo = \"ttop\";\n    rev = \"v${finalAttrs.version}\";\n    hash = \"sha256-oPdaUqh6eN1X5kAYVvevOndkB/xnQng9QVLX9bu5P5E=\";\n  };\n\n  lockFile = ./lock.json;\n\n  nimFlags = [\n    \"-d:NimblePkgVersion=${finalAttrs.version}\"\n  ];\n})\n</code></pre></p>"},{"location":"languages-frameworks/nim.section.html#buildnimpackage-parameters","title":"<code>buildNimPackage</code> parameters","text":"<p>The <code>buildNimPackage</code> function takes an attrset of parameters that are passed on to <code>stdenv.mkDerivation</code>.</p> <p>The following parameters are specific to <code>buildNimPackage</code>:</p> <ul> <li><code>lockFile</code>: JSON formatted lockfile.</li> <li><code>nimbleFile</code>: Specify the Nimble file location of the package being built   rather than discover the file at build-time.</li> <li><code>nimRelease ? true</code>: Build the package in release mode.</li> <li><code>nimDefines ? []</code>: A list of Nim defines. Key-value tuples are not supported.</li> <li><code>nimFlags ? []</code>: A list of command line arguments to pass to the Nim compiler.   Use this to specify defines with arguments in the form of <code>-d:${name}=${value}</code>.</li> <li><code>nimDoc</code> ? false`: Build and install HTML documentation.</li> </ul>"},{"location":"languages-frameworks/nim.section.html#nim-lockfiles","title":"Lockfiles","text":"<p>Nim lockfiles are created with the <code>nim_lk</code> utility. Run <code>nim_lk</code> with the source directory as an argument and it will print a lockfile to stdout. <pre><code>$ cd nixpkgs\n$ nix build -f . ttop.src\n$ nix run -f . nim_lk ./result | jq --sort-keys &gt; pkgs/by-name/tt/ttop/lock.json\n</code></pre></p>"},{"location":"languages-frameworks/nim.section.html#nim-overrides","title":"Overriding Nim packages","text":"<p>The <code>buildNimPackage</code> function generates flags and additional build dependencies from the <code>lockFile</code> parameter passed to <code>buildNimPackage</code>. Using <code>overrideAttrs</code> on the final package will apply after this has already been generated, so this can't be used to override the <code>lockFile</code> in a package built with <code>buildNimPackage</code>. To be able to override parameters before flags and build dependencies are generated from the <code>lockFile</code>, use <code>overrideNimAttrs</code> instead with the same syntax as <code>overrideAttrs</code>:</p> <pre><code>pkgs.nitter.overrideNimAttrs {\n  # using a different source which has different dependencies from the standard package\n  src = pkgs.fetchFromGithub { /* \u2026 */ };\n  # new lock file generated from the source\n  lockFile = ./custom-lock.json;\n}\n</code></pre>"},{"location":"languages-frameworks/nim.section.html#nim-lock-overrides","title":"Lockfile dependency overrides","text":"<p>The <code>buildNimPackage</code> function matches the libraries specified by <code>lockFile</code> to attrset of override functions that are then applied to the package derivation. The default overrides are maintained as the top-level <code>nimOverrides</code> attrset at <code>pkgs/top-level/nim-overrides.nix</code>.</p> <p>For example, to propagate a dependency on SDL2 for lockfiles that select the Nim <code>sdl2</code> library, an overlay is added to the set in the <code>nim-overrides.nix</code> file: <pre><code>{ lib\n/* \u2026 */\n, SDL2\n/* \u2026 */\n}:\n\n{\n  /* \u2026 */\n  sdl2 =\n    lockAttrs:\n    finalAttrs:\n    { buildInputs ? [ ], ... }:\n    {\n      buildInputs = buildInputs ++ [ SDL2 ];\n    };\n  /* \u2026 */\n}\n</code></pre></p> <p>The annotations in the <code>nim-overrides.nix</code> set are functions that take three arguments and return a new attrset to be overlayed on the package being built. - lockAttrs: the attrset for this library from within a lockfile. This can be used to implement library version constraints, such as marking libraries as broken or insecure. - finalAttrs: the final attrset passed by <code>buildNimPackage</code> to <code>stdenv.mkDerivation</code>. - prevAttrs: the attrset produced by initial arguments to <code>buildNimPackage</code> and any preceding lockfile overlays.</p>"},{"location":"languages-frameworks/nim.section.html#nim-lock-overrides-overrides","title":"Overriding an Nim library override","text":"<p>The <code>nimOverrides</code> attrset makes it possible to modify overrides in a few different ways.</p> <p>Override a package internal to its definition: <pre><code>{ lib, buildNimPackage, nimOverrides, libressl }:\n\nlet\n  buildNimPackage' = buildNimPackage.override {\n    nimOverrides = nimOverrides.override { openssl = libressl; };\n  };\nin buildNimPackage' (finalAttrs: {\n  pname = \"foo\";\n  # \u2026\n})\n</code></pre></p> <p>Override a package externally: <pre><code>{ pkgs }: {\n  foo = pkgs.foo.override {\n    buildNimPackage = pkgs.buildNimPackage.override {\n      nimOverrides = pkgs.nimOverrides.override { openssl = libressl; };\n    };\n  };\n}\n</code></pre></p>"},{"location":"languages-frameworks/ocaml.section.html","title":"OCaml","text":""},{"location":"languages-frameworks/ocaml.section.html#sec-language-ocaml-user-guide","title":"User guide","text":"<p>OCaml libraries are available in attribute sets of the form <code>ocaml-ng.ocamlPackages_X_XX</code> where X is to be replaced with the desired compiler version. For example, ocamlgraph compiled with OCaml 4.12 can be found in <code>ocaml-ng.ocamlPackages_4_12.ocamlgraph</code>. The compiler itself is also located in this set, under the name <code>ocaml</code>.</p> <p>If you don't care about the exact compiler version, <code>ocamlPackages</code> is a top-level alias pointing to a recent version of OCaml.</p> <p>OCaml applications are usually available top-level, and not inside <code>ocamlPackages</code>. Notable exceptions are build tools that must be built with the same compiler version as the compiler you intend to use like <code>dune</code> or <code>ocaml-lsp</code>.</p> <p>To open a shell able to build a typical OCaml project, put the dependencies in <code>buildInputs</code> and add <code>ocamlPackages.ocaml</code> and <code>ocamlPackages.findlib</code> to <code>nativeBuildInputs</code> at least. For example: <pre><code>let\n pkgs = import &lt;nixpkgs&gt; {};\n # choose the ocaml version you want to use\n ocamlPackages = pkgs.ocaml-ng.ocamlPackages_4_12;\nin\npkgs.mkShell {\n  # build tools\n  nativeBuildInputs = with ocamlPackages; [ ocaml findlib dune_2 ocaml-lsp ];\n  # dependencies\n  buildInputs = with ocamlPackages; [ ocamlgraph ];\n}\n</code></pre></p>"},{"location":"languages-frameworks/ocaml.section.html#sec-language-ocaml-packaging","title":"Packaging guide","text":"<p>OCaml libraries should be installed in <code>$(out)/lib/ocaml/${ocaml.version}/site-lib/</code>. Such directories are automatically added to the <code>$OCAMLPATH</code> environment variable when building another package that depends on them or when opening a <code>nix-shell</code>.</p> <p>Given that most of the OCaml ecosystem is now built with dune, nixpkgs includes a convenience build support function called <code>buildDunePackage</code> that will build an OCaml package using dune, OCaml and findlib and any additional dependencies provided as <code>buildInputs</code> or <code>propagatedBuildInputs</code>.</p> <p>Here is a simple package example.</p> <ul> <li> <p>It defines an (optional) attribute <code>minimalOCamlVersion</code> (see note below)   that will be used to throw a descriptive evaluation error if building with   an older OCaml is attempted.</p> </li> <li> <p>It uses the <code>fetchFromGitHub</code> fetcher to get its source.</p> </li> <li> <p>It also accept <code>duneVersion</code> parameter (valid value are <code>\"1\"</code>, <code>\"2\"</code>, and   <code>\"3\"</code>). The recommended practice it to set only if you don't want the default   value and/or it depends on something else like package version. You might see   a not-supported argument <code>useDune2</code>. The behavior was <code>useDune2 = true;</code> =&gt;   <code>duneVersion = \"2\";</code> and <code>useDune2 = false;</code> =&gt; <code>duneVersion = \"1\";</code>. It was   used at the time when dune3 didn't existed.</p> </li> <li> <p>It sets the optional <code>doCheck</code> attribute such that tests will be run with   <code>dune runtest -p angstrom</code> after the build (<code>dune build -p angstrom</code>) is   complete, but only if the Ocaml version is at at least <code>\"4.05\"</code>.</p> </li> <li> <p>It uses the package <code>ocaml-syntax-shims</code> as a build input, <code>alcotest</code> and   <code>ppx_let</code> as check inputs (because they are needed to run the tests), and   <code>bigstringaf</code> and <code>result</code> as propagated build inputs (thus they will also be   available to libraries depending on this library).</p> </li> <li> <p>The library will be installed using the <code>angstrom.install</code> file that dune   generates.</p> </li> </ul> <pre><code>{ lib,\n  fetchFromGitHub,\n  buildDunePackage,\n  ocaml,\n  ocaml-syntax-shims,\n  alcotest,\n  result,\n  bigstringaf,\n  ppx_let }:\n\nbuildDunePackage rec {\n  pname = \"angstrom\";\n  version = \"0.15.0\";\n\n  minimalOCamlVersion = \"4.04\";\n\n  src = fetchFromGitHub {\n    owner  = \"inhabitedtype\";\n    repo   = pname;\n    rev    = version;\n    hash   = \"sha256-MK8o+iPGANEhrrTc1Kz9LBilx2bDPQt7Pp5P2libucI=\";\n  };\n\n  checkInputs = [ alcotest ppx_let ];\n  buildInputs = [ ocaml-syntax-shims ];\n  propagatedBuildInputs = [ bigstringaf result ];\n  doCheck = lib.versionAtLeast ocaml.version \"4.05\";\n\n  meta = {\n    homepage = \"https://github.com/inhabitedtype/angstrom\";\n    description = \"OCaml parser combinators built for speed and memory efficiency\";\n    license = lib.licenses.bsd3;\n    maintainers = with lib.maintainers; [ sternenseemann ];\n  };\n</code></pre> <p>Here is a second example, this time using a source archive generated with <code>dune-release</code>. It is a good idea to use this archive when it is available as it will usually contain substituted variables such as a <code>%%VERSION%%</code> field. This library does not depend on any other OCaml library and no tests are run after building it.</p> <pre><code>{ lib, fetchurl, buildDunePackage }:\n\nbuildDunePackage rec {\n  pname = \"wtf8\";\n  version = \"1.0.2\";\n\n  minimalOCamlVersion = \"4.02\";\n\n  src = fetchurl {\n    url = \"https://github.com/flowtype/ocaml-${pname}/releases/download/v${version}/${pname}-v${version}.tbz\";\n    hash = \"sha256-d5/3KUBAWRj8tntr4RkJ74KWW7wvn/B/m1nx0npnzyc=\";\n  };\n\n  meta = with lib; {\n    homepage = \"https://github.com/flowtype/ocaml-wtf8\";\n    description = \"WTF-8 is a superset of UTF-8 that allows unpaired surrogates.\";\n    license = licenses.mit;\n    maintainers = [ maintainers.eqyiel ];\n  };\n}\n</code></pre> <p>Note about <code>minimalOCamlVersion</code>.  A deprecated version of this argument was spelled <code>minimumOCamlVersion</code>; setting the old attribute wrongly modifies the derivation hash and is therefore inappropriate. As a technical dept, currently packaged libraries may still use the old spelling: maintainers are invited to fix this when updating packages. Massive renaming is strongly discouraged as it would be challenging to review, difficult to test, and will cause unnecessary rebuild.</p> <p>The build will automatically fail if two distinct versions of the same library are added to <code>buildInputs</code> (which usually happens transitively because of <code>propagatedBuildInputs</code>). Set <code>dontDetectOcamlConflicts</code> to true to disable this behavior.</p>"},{"location":"languages-frameworks/octave.section.html","title":"Octave","text":""},{"location":"languages-frameworks/octave.section.html#ssec-octave-introduction","title":"Introduction","text":"<p>Octave is a modular scientific programming language and environment. A majority of the packages supported by Octave from their website are packaged in nixpkgs.</p>"},{"location":"languages-frameworks/octave.section.html#ssec-octave-structure","title":"Structure","text":"<p>All Octave add-on packages are available in two ways: 1. Under the top-level <code>Octave</code> attribute, <code>octave.pkgs</code>. 2. As a top-level attribute, <code>octavePackages</code>.</p>"},{"location":"languages-frameworks/octave.section.html#ssec-octave-packaging","title":"Packaging Octave Packages","text":"<p>Nixpkgs provides a function <code>buildOctavePackage</code>, a generic package builder function for any Octave package that complies with the Octave's current packaging format.</p> <p>All Octave packages are defined in pkgs/top-level/octave-packages.nix rather than <code>pkgs/all-packages.nix</code>. Each package is defined in their own file in the pkgs/development/octave-modules directory. Octave packages are made available through <code>all-packages.nix</code> through both the attribute <code>octavePackages</code> and <code>octave.pkgs</code>. You can test building an Octave package as follows:</p> <pre><code>$ nix-build -A octavePackages.symbolic\n</code></pre> <p>To install it into your user profile, run this command from the root of the repository:</p> <pre><code>$ nix-env -f. -iA octavePackages.symbolic\n</code></pre> <p>You can build Octave with packages by using the <code>withPackages</code> passed-through function.</p> <pre><code>$ nix-shell -p 'octave.withPackages (ps: with ps; [ symbolic ])'\n</code></pre> <p>This will also work in a <code>shell.nix</code> file.</p> <pre><code>{ pkgs ? import &lt;nixpkgs&gt; { }}:\n\npkgs.mkShell {\n  nativeBuildInputs = with pkgs; [\n    (octave.withPackages (opkgs: with opkgs; [ symbolic ]))\n  ];\n}\n</code></pre>"},{"location":"languages-frameworks/octave.section.html#sssec-buildOctavePackage-steps","title":"<code>buildOctavePackage</code> Steps","text":"<p>The <code>buildOctavePackage</code> does several things to make sure things work properly.</p> <ol> <li>Sets the environment variable <code>OCTAVE_HISTFILE</code> to <code>/dev/null</code> during package compilation so that the commands run through the Octave interpreter directly are not logged.</li> <li>Skips the configuration step, because the packages are stored as gzipped tarballs, which Octave itself handles directly.</li> <li>Change the hierarchy of the tarball so that only a single directory is at the top-most level of the tarball.</li> <li>Use Octave itself to run the <code>pkg build</code> command, which unzips the tarball, extracts the necessary files written in Octave, and compiles any code written in C++ or Fortran, and places the fully compiled artifact in <code>$out</code>.</li> </ol> <p><code>buildOctavePackage</code> is built on top of <code>stdenv</code> in a standard way, allowing most things to be customized.</p>"},{"location":"languages-frameworks/octave.section.html#sssec-octave-handling-dependencies","title":"Handling Dependencies","text":"<p>In Octave packages, there are four sets of dependencies that can be specified:</p> <p><code>nativeBuildInputs</code> : Just like other packages, <code>nativeBuildInputs</code> is intended for architecture-dependent build-time-only dependencies.</p> <p><code>buildInputs</code> : Like other packages, <code>buildInputs</code> is intended for architecture-independent build-time-only dependencies.</p> <p><code>propagatedBuildInputs</code> : Similar to other packages, <code>propagatedBuildInputs</code> is intended for packages that are required for both building and running of the package. See Symbolic for how this works and why it is needed.</p> <p><code>requiredOctavePackages</code> : This is a special dependency that ensures the specified Octave packages are dependent on others, and are made available simultaneously when loading them in Octave.</p>"},{"location":"languages-frameworks/octave.section.html#sssec-installing-octave-packages","title":"Installing Octave Packages","text":"<p>By default, the <code>buildOctavePackage</code> function does not install the requested package into Octave for use. The function will only build the requested package. This is due to Octave maintaining an text-based database about which packages are installed where. To this end, when all the requested packages have been built, the Octave package and all its add-on packages are put together into an environment, similar to Python.</p> <ol> <li>First, all the Octave binaries are wrapped with the environment variable <code>OCTAVE_SITE_INITFILE</code> set to a file in <code>$out</code>, which is required for Octave to be able to find the non-standard package database location.</li> <li>Because of the way <code>buildEnv</code> works, all tarballs that are present (which should be all Octave packages to install) should be removed.</li> <li>The path down to the default install location of Octave packages is recreated so that Nix-operated Octave can install the packages.</li> <li>Install the packages into the <code>$out</code> environment while writing package entries to the database file. This database file is unique for each different (according to Nix) environment invocation.</li> <li>Rewrite the Octave-wide startup file to read from the list of packages installed in that particular environment.</li> <li>Wrap any programs that are required by the Octave packages so that they work with all the paths defined within the environment.</li> </ol>"},{"location":"languages-frameworks/perl.section.html","title":"Perl","text":""},{"location":"languages-frameworks/perl.section.html#ssec-perl-running","title":"Running Perl programs on the shell","text":"<p>When executing a Perl script, it is possible you get an error such as <code>./myscript.pl: bad interpreter: /usr/bin/perl: no such file or directory</code>. This happens when the script expects Perl to be installed at <code>/usr/bin/perl</code>, which is not the case when using Perl from nixpkgs. You can fix the script by changing the first line to:</p> <pre><code>#!/usr/bin/env perl\n</code></pre> <p>to take the Perl installation from the <code>PATH</code> environment variable, or invoke Perl directly with:</p> <pre><code>$ perl ./myscript.pl\n</code></pre> <p>When the script is using a Perl library that is not installed globally, you might get an error such as <code>Can't locate DB_File.pm in @INC (you may need to install the DB_File module)</code>. In that case, you can use <code>nix-shell</code> to start an ad-hoc shell with that library installed, for instance:</p> <pre><code>$ nix-shell -p perl perlPackages.DBFile --run ./myscript.pl\n</code></pre> <p>If you are always using the script in places where <code>nix-shell</code> is available, you can embed the <code>nix-shell</code> invocation in the shebang like this:</p> <pre><code>#!/usr/bin/env nix-shell\n#! nix-shell -i perl -p perl perlPackages.DBFile\n</code></pre>"},{"location":"languages-frameworks/perl.section.html#ssec-perl-packaging","title":"Packaging Perl programs","text":"<p>Nixpkgs provides a function <code>buildPerlPackage</code>, a generic package builder function for any Perl package that has a standard <code>Makefile.PL</code>. It\u2019s implemented in pkgs/development/perl-modules/generic.</p> <p>Perl packages from CPAN are defined in pkgs/top-level/perl-packages.nix rather than <code>pkgs/all-packages.nix</code>. Most Perl packages are so straight-forward to build that they are defined here directly, rather than having a separate function for each package called from <code>perl-packages.nix</code>. However, more complicated packages should be put in a separate file, typically in <code>pkgs/development/perl-modules</code>. Here is an example of the former:</p> <pre><code>ClassC3 = buildPerlPackage rec {\n  pname = \"Class-C3\";\n  version = \"0.21\";\n  src = fetchurl {\n    url = \"mirror://cpan/authors/id/F/FL/FLORA/${pname}-${version}.tar.gz\";\n    hash = \"sha256-/5GE5xHT0uYGOQxroqj6LMU7CtKn2s6vMVoSXxL4iK4=\";\n  };\n};\n</code></pre> <p>Note the use of <code>mirror://cpan/</code>, and the <code>pname</code> and <code>version</code> in the URL definition to ensure that the <code>pname</code> attribute is consistent with the source that we\u2019re actually downloading. Perl packages are made available in <code>all-packages.nix</code> through the variable <code>perlPackages</code>. For instance, if you have a package that needs <code>ClassC3</code>, you would typically write</p> <pre><code>foo = import ../path/to/foo.nix {\n  inherit stdenv fetchurl ...;\n  inherit (perlPackages) ClassC3;\n};\n</code></pre> <p>in <code>all-packages.nix</code>. You can test building a Perl package as follows:</p> <pre><code>$ nix-build -A perlPackages.ClassC3\n</code></pre> <p>To install it with <code>nix-env</code> instead: <code>nix-env -f. -iA perlPackages.ClassC3</code>.</p> <p>So what does <code>buildPerlPackage</code> do? It does the following:</p> <ol> <li>In the configure phase, it calls <code>perl Makefile.PL</code> to generate a Makefile. You can set the variable <code>makeMakerFlags</code> to pass flags to <code>Makefile.PL</code></li> <li>It adds the contents of the <code>PERL5LIB</code> environment variable to <code>.../bin/perl</code> line of Perl scripts as <code>-Idir</code> flags. This ensures that a script can find its dependencies. (This can cause this shebang line to become too long for Darwin to handle; see the note below.)</li> <li>In the fixup phase, it writes the propagated build inputs (<code>propagatedBuildInputs</code>) to the file <code>$out/nix-support/propagated-user-env-packages</code>. <code>nix-env</code> recursively installs all packages listed in this file when you install a package that has it. This ensures that a Perl package can find its dependencies.</li> </ol> <p><code>buildPerlPackage</code> is built on top of <code>stdenv</code>, so everything can be customised in the usual way. For instance, the <code>BerkeleyDB</code> module has a <code>preConfigure</code> hook to generate a configuration file used by <code>Makefile.PL</code>:</p> <pre><code>{ buildPerlPackage, fetchurl, db }:\n\nbuildPerlPackage rec {\n  pname = \"BerkeleyDB\";\n  version = \"0.36\";\n\n  src = fetchurl {\n    url = \"mirror://cpan/authors/id/P/PM/PMQS/${pname}-${version}.tar.gz\";\n    hash = \"sha256-4Y+HGgGQqcOfdiKcFIyMrWBEccVNVAMDBWZlFTMorh8=\";\n  };\n\n  preConfigure = ''\n    echo \"LIB = ${db.out}/lib\" &gt; config.in\n    echo \"INCLUDE = ${db.dev}/include\" &gt;&gt; config.in\n  '';\n}\n</code></pre> <p>Dependencies on other Perl packages can be specified in the <code>buildInputs</code> and <code>propagatedBuildInputs</code> attributes. If something is exclusively a build-time dependency, use <code>buildInputs</code>; if it\u2019s (also) a runtime dependency, use <code>propagatedBuildInputs</code>. For instance, this builds a Perl module that has runtime dependencies on a bunch of other modules:</p> <pre><code>ClassC3Componentised = buildPerlPackage rec {\n  pname = \"Class-C3-Componentised\";\n  version = \"1.0004\";\n  src = fetchurl {\n    url = \"mirror://cpan/authors/id/A/AS/ASH/${pname}-${version}.tar.gz\";\n    hash = \"sha256-ASO9rV/FzJYZ0BH572Fxm2ZrFLMZLFATJng1NuU4FHc=\";\n  };\n  propagatedBuildInputs = [\n    ClassC3 ClassInspector TestException MROCompat\n  ];\n};\n</code></pre> <p>On Darwin, if a script has too many <code>-Idir</code> flags in its first line (its \u201cshebang line\u201d), it will not run. This can be worked around by calling the <code>shortenPerlShebang</code> function from the <code>postInstall</code> phase:</p> <pre><code>{ lib, stdenv, buildPerlPackage, fetchurl, shortenPerlShebang }:\n\nImageExifTool = buildPerlPackage {\n  pname = \"Image-ExifTool\";\n  version = \"12.50\";\n\n  src = fetchurl {\n    url = \"https://exiftool.org/${pname}-${version}.tar.gz\";\n    hash = \"sha256-vOhB/FwQMC8PPvdnjDvxRpU6jAZcC6GMQfc0AH4uwKg=\";\n  };\n\n  nativeBuildInputs = lib.optional stdenv.isDarwin shortenPerlShebang;\n  postInstall = lib.optionalString stdenv.isDarwin ''\n    shortenPerlShebang $out/bin/exiftool\n  '';\n};\n</code></pre> <p>This will remove the <code>-I</code> flags from the shebang line, rewrite them in the <code>use lib</code> form, and put them on the next line instead. This function can be given any number of Perl scripts as arguments; it will modify them in-place.</p>"},{"location":"languages-frameworks/perl.section.html#ssec-generation-from-CPAN","title":"Generation from CPAN","text":"<p>Nix expressions for Perl packages can be generated (almost) automatically from CPAN. This is done by the program <code>nix-generate-from-cpan</code>, which can be installed as follows:</p> <pre><code>$ nix-env -f \"&lt;nixpkgs&gt;\" -iA nix-generate-from-cpan\n</code></pre> <p>Substitute <code>&lt;nixpkgs&gt;</code> by the path of a nixpkgs clone to use the latest version.</p> <p>This program takes a Perl module name, looks it up on CPAN, fetches and unpacks the corresponding package, and prints a Nix expression on standard output. For example:</p> <pre><code>$ nix-generate-from-cpan XML::Simple\n  XMLSimple = buildPerlPackage rec {\n    pname = \"XML-Simple\";\n    version = \"2.22\";\n    src = fetchurl {\n      url = \"mirror://cpan/authors/id/G/GR/GRANTM/XML-Simple-2.22.tar.gz\";\n      hash = \"sha256-uUUO8i6pZErl1q2ghtxDAPoQW+BQogMOvU79KMGY60k=\";\n    };\n    propagatedBuildInputs = [ XMLNamespaceSupport XMLSAX XMLSAXExpat ];\n    meta = {\n      description = \"An API for simple XML files\";\n      license = with lib.licenses; [ artistic1 gpl1Plus ];\n    };\n  };\n</code></pre> <p>The output can be pasted into <code>pkgs/top-level/perl-packages.nix</code> or wherever else you need it.</p>"},{"location":"languages-frameworks/perl.section.html#ssec-perl-cross-compilation","title":"Cross-compiling modules","text":"<p>Nixpkgs has experimental support for cross-compiling Perl modules. In many cases, it will just work out of the box, even for modules with native extensions. Sometimes, however, the Makefile.PL for a module may (indirectly) import a native module. In that case, you will need to make a stub for that module that will satisfy the Makefile.PL and install it into <code>lib/perl5/site_perl/cross_perl/${perl.version}</code>. See the <code>postInstall</code> for <code>DBI</code> for an example.</p>"},{"location":"languages-frameworks/php.section.html","title":"PHP","text":""},{"location":"languages-frameworks/php.section.html#ssec-php-user-guide","title":"User Guide","text":""},{"location":"languages-frameworks/php.section.html#ssec-php-user-guide-overview","title":"Overview","text":"<p>Several versions of PHP are available on Nix, each of which having a wide variety of extensions and libraries available.</p> <p>The different versions of PHP that nixpkgs provides are located under attributes named based on major and minor version number; e.g., <code>php81</code> is PHP 8.1.</p> <p>Only versions of PHP that are supported by upstream for the entirety of a given NixOS release will be included in that release of NixOS. See PHP Supported Versions.</p> <p>The attribute <code>php</code> refers to the version of PHP considered most stable and thoroughly tested in nixpkgs for any given release of NixOS - not necessarily the latest major release from upstream.</p> <p>All available PHP attributes are wrappers around their respective binary PHP package and provide commonly used extensions this way. The real PHP 8.1 package, i.e. the unwrapped one, is available as <code>php81.unwrapped</code>; see the next section for more details.</p> <p>Interactive tools built on PHP are put in <code>php.packages</code>; composer is for example available at <code>php.packages.composer</code>.</p> <p>Most extensions that come with PHP, as well as some popular third-party ones, are available in <code>php.extensions</code>; for example, the opcache extension shipped with PHP is available at <code>php.extensions.opcache</code> and the third-party ImageMagick extension at <code>php.extensions.imagick</code>.</p>"},{"location":"languages-frameworks/php.section.html#ssec-php-user-guide-installing-with-extensions","title":"Installing PHP with extensions","text":"<p>A PHP package with specific extensions enabled can be built using <code>php.withExtensions</code>. This is a function which accepts an anonymous function as its only argument; the function should accept two named parameters: <code>enabled</code> - a list of currently enabled extensions and <code>all</code> - the set of all extensions, and return a list of wanted extensions. For example, a PHP package with all default extensions and ImageMagick enabled:</p> <pre><code>php.withExtensions ({ enabled, all }:\n  enabled ++ [ all.imagick ])\n</code></pre> <p>To exclude some, but not all, of the default extensions, you can filter the <code>enabled</code> list like this:</p> <pre><code>php.withExtensions ({ enabled, all }:\n  (lib.filter (e: e != php.extensions.opcache) enabled)\n  ++ [ all.imagick ])\n</code></pre> <p>To build your list of extensions from the ground up, you can ignore <code>enabled</code>:</p> <pre><code>php.withExtensions ({ all, ... }: with all; [ imagick opcache ])\n</code></pre> <p><code>php.withExtensions</code> provides extensions by wrapping a minimal php base package, providing a <code>php.ini</code> file listing all extensions to be loaded. You can access this package through the <code>php.unwrapped</code> attribute; useful if you, for example, need access to the <code>dev</code> output. The generated <code>php.ini</code> file can be accessed through the <code>php.phpIni</code> attribute.</p> <p>If you want a PHP build with extra configuration in the <code>php.ini</code> file, you can use <code>php.buildEnv</code>. This function takes two named and optional parameters: <code>extensions</code> and <code>extraConfig</code>. <code>extensions</code> takes an extension specification equivalent to that of <code>php.withExtensions</code>, <code>extraConfig</code> a string of additional <code>php.ini</code> configuration parameters. For example, a PHP package with the opcache and ImageMagick extensions enabled, and <code>memory_limit</code> set to <code>256M</code>:</p> <pre><code>php.buildEnv {\n  extensions = { all, ... }: with all; [ imagick opcache ];\n  extraConfig = \"memory_limit=256M\";\n}\n</code></pre>"},{"location":"languages-frameworks/php.section.html#ssec-php-user-guide-installing-with-extensions-phpfpm","title":"Example setup for <code>phpfpm</code>","text":"<p>You can use the previous examples in a <code>phpfpm</code> pool called <code>foo</code> as follows:</p> <pre><code>let\n  myPhp = php.withExtensions ({ all, ... }: with all; [ imagick opcache ]);\nin {\n  services.phpfpm.pools.\"foo\".phpPackage = myPhp;\n};\n</code></pre> <pre><code>let\n  myPhp = php.buildEnv {\n    extensions = { all, ... }: with all; [ imagick opcache ];\n    extraConfig = \"memory_limit=256M\";\n  };\nin {\n  services.phpfpm.pools.\"foo\".phpPackage = myPhp;\n};\n</code></pre>"},{"location":"languages-frameworks/php.section.html#ssec-php-user-guide-installing-with-extensions-nix-shell","title":"Example usage with <code>nix-shell</code>","text":"<p>This brings up a temporary environment that contains a PHP interpreter with the extensions <code>imagick</code> and <code>opcache</code> enabled:</p> <pre><code>nix-shell -p 'php.withExtensions ({ all, ... }: with all; [ imagick opcache ])'\n</code></pre>"},{"location":"languages-frameworks/php.section.html#ssec-php-user-guide-installing-packages-with-extensions","title":"Installing PHP packages with extensions","text":"<p>All interactive tools use the PHP package you get them from, so all packages at <code>php.packages.*</code> use the <code>php</code> package with its default extensions. Sometimes this default set of extensions isn't enough and you may want to extend it. A common case of this is the <code>composer</code> package: a project may depend on certain extensions and <code>composer</code> won't work with that project unless those extensions are loaded.</p> <p>Example of building <code>composer</code> with additional extensions:</p> <pre><code>(php.withExtensions ({ all, enabled }:\n  enabled ++ (with all; [ imagick redis ]))\n).packages.composer\n</code></pre>"},{"location":"languages-frameworks/php.section.html#ssec-php-user-guide-overriding-packages","title":"Overriding PHP packages","text":"<p><code>php-packages.nix</code> form a scope, allowing us to override the packages defined within. For example, to apply a patch to a <code>mysqlnd</code> extension, you can pass an overlay-style function to <code>php</code>\u2019s <code>packageOverrides</code> argument:</p> <pre><code>php.override {\n  packageOverrides = final: prev: {\n    extensions = prev.extensions // {\n      mysqlnd = prev.extensions.mysqlnd.overrideAttrs (attrs: {\n        patches = attrs.patches or [] ++ [\n          \u2026\n        ];\n      });\n    };\n  };\n}\n</code></pre>"},{"location":"languages-frameworks/php.section.html#ssec-building-php-projects","title":"Building PHP projects","text":"<p>With Composer, you can effectively build PHP projects by streamlining dependency management. As the de-facto standard dependency manager for PHP, Composer enables you to declare and manage the libraries your project relies on, ensuring a more organized and efficient development process.</p> <p>Composer is not a package manager in the same sense as <code>Yum</code> or <code>Apt</code> are. Yes, it deals with \"packages\" or libraries, but it manages them on a per-project basis, installing them in a directory (e.g. <code>vendor</code>) inside your project. By default, it does not install anything globally. This idea is not new and Composer is strongly inspired by node's <code>npm</code> and ruby's <code>bundler</code>.</p> <p>Currently, there is no other PHP tool that offers the same functionality as Composer. Consequently, incorporating a helper in Nix to facilitate building such applications is a logical choice.</p> <p>In a Composer project, dependencies are defined in a <code>composer.json</code> file, while their specific versions are locked in a <code>composer.lock</code> file. Some Composer-based projects opt to include this <code>composer.lock</code> file in their source code, while others choose not to.</p> <p>In Nix, there are multiple approaches to building a Composer-based project.</p> <p>One such method is the <code>php.buildComposerProject</code> helper function, which serves as a wrapper around <code>mkDerivation</code>.</p> <p>Using this function, you can build a PHP project that includes both a <code>composer.json</code> and <code>composer.lock</code> file. If the project specifies binaries using the <code>bin</code> attribute in <code>composer.json</code>, these binaries will be automatically linked and made accessible in the derivation. In this context, \"binaries\" refer to PHP scripts that are intended to be executable.</p> <p>To use the helper effectively, add the <code>vendorHash</code> attribute, which enables the wrapper to handle the heavy lifting.</p> <p>Internally, the helper operates in three stages:</p> <ol> <li>It constructs a <code>composerRepository</code> attribute derivation by creating a    composer repository on the filesystem containing dependencies specified in    <code>composer.json</code>. This process uses the function    <code>php.mkComposerRepository</code> which in turn uses the    <code>php.composerHooks.composerRepositoryHook</code> hook. Internally this function uses    a custom    Composer plugin to    generate the repository.</li> <li>The resulting <code>composerRepository</code> derivation is then used by the    <code>php.composerHooks.composerInstallHook</code> hook, which is responsible for    creating the final <code>vendor</code> directory.</li> <li>Any \"binary\" specified in the <code>composer.json</code> are linked and made accessible    in the derivation.</li> </ol> <p>As the autoloader optimization can be activated directly within the <code>composer.json</code> file, we do not enable any autoloader optimization flags.</p> <p>To customize the PHP version, you can specify the <code>php</code> attribute. Similarly, if you wish to modify the Composer version, use the <code>composer</code> attribute. It is important to note that both attributes should be of the <code>derivation</code> type.</p> <p>Here's an example of working code example using <code>php.buildComposerProject</code>:</p> <pre><code>{ php, fetchFromGitHub }:\n\nphp.buildComposerProject (finalAttrs: {\n  pname = \"php-app\";\n  version = \"1.0.0\";\n\n  src = fetchFromGitHub {\n    owner = \"git-owner\";\n    repo = \"git-repo\";\n    rev = finalAttrs.version;\n    hash = \"sha256-VcQRSss2dssfkJ+iUb5qT+FJ10GHiFDzySigcmuVI+8=\";\n  };\n\n  # PHP version containing the `ast` extension enabled\n  php = php.buildEnv {\n    extensions = ({ enabled, all }: enabled ++ (with all; [\n      ast\n    ]));\n  };\n\n  # The composer vendor hash\n  vendorHash = \"sha256-86s/F+/5cBAwBqZ2yaGRM5rTGLmou5//aLRK5SA0WiQ=\";\n\n  # If the composer.lock file is missing from the repository, add it:\n  # composerLock = ./path/to/composer.lock;\n})\n</code></pre> <p>In case the file <code>composer.lock</code> is missing from the repository, it is possible to specify it using the <code>composerLock</code> attribute.</p> <p>The other method is to use all these methods and hooks individually. This has the advantage of building a PHP library within another derivation very easily when necessary.</p> <p>Here's a working code example to build a PHP library using <code>mkDerivation</code> and separate functions and hooks:</p> <pre><code>{ stdenvNoCC, fetchFromGitHub, php }:\n\nstdenvNoCC.mkDerivation (finalAttrs:\nlet\n  src = fetchFromGitHub {\n    owner = \"git-owner\";\n    repo = \"git-repo\";\n    rev = finalAttrs.version;\n    hash = \"sha256-VcQRSss2dssfkJ+iUb5qT+FJ10GHiFDzySigcmuVI+8=\";\n  };\nin {\n  inherit src;\n  pname = \"php-app\";\n  version = \"1.0.0\";\n\n  buildInputs = [ php ];\n\n  nativeBuildInputs = [\n    php.packages.composer\n    # This hook will use the attribute `composerRepository`\n    php.composerHooks.composerInstallHook\n  ];\n\n  composerRepository = php.mkComposerRepository {\n    inherit (finalAttrs) src;\n    # Specifying a custom composer.lock since it is not present in the sources.\n    composerLock = ./composer.lock;\n    # The composer vendor hash\n    vendorHash = \"sha256-86s/F+/5cBAwBqZ2yaGRM5rTGLmou5//aLRK5SA0WiQ=\";\n  };\n})\n</code></pre>"},{"location":"languages-frameworks/pkg-config.section.html","title":"pkg-config","text":"<p>pkg-config is a unified interface for declaring and querying built C/C++ libraries.</p> <p>Nixpkgs provides a couple of facilities for working with this tool.</p>"},{"location":"languages-frameworks/pkg-config.section.html#pkg-config-writing-packages","title":"Writing packages providing pkg-config modules","text":"<p>Packages should set <code>meta.pkgConfigModules</code> with the list of package config modules they provide. They should also use <code>testers.testMetaPkgConfig</code> to check that the final built package matches that list. Additionally, the <code>validatePkgConfig</code> setup hook, will do extra checks on to-be-installed pkg-config modules.</p> <p>A good example of all these things is zlib:</p> <pre><code>{ pkg-config, testers, ... }:\n\nstdenv.mkDerivation (finalAttrs: {\n  ...\n\n  nativeBuildInputs = [ pkg-config validatePkgConfig ];\n\n  passthru.tests.pkg-config = testers.testMetaPkgConfig finalAttrs.finalPackage;\n\n  meta = {\n    ...\n    pkgConfigModules = [ \"zlib\" ];\n  };\n})\n</code></pre>"},{"location":"languages-frameworks/pkg-config.section.html#sec-pkg-config-usage","title":"Accessing packages via pkg-config module name","text":""},{"location":"languages-frameworks/pkg-config.section.html#sec-pkg-config-usage-internal","title":"Within Nixpkgs","text":"<p>A setup hook is bundled in the <code>pkg-config</code> package to bring a derivation's declared build inputs into the environment. This will populate environment variables like <code>PKG_CONFIG_PATH</code>, <code>PKG_CONFIG_PATH_FOR_BUILD</code>, and <code>PKG_CONFIG_PATH_HOST</code> based on:</p> <ul> <li> <p>how <code>pkg-config</code> itself is depended upon</p> </li> <li> <p>how other dependencies are depended upon</p> </li> </ul> <p>For more details see the section on specifying dependencies in general.</p> <p>Normal pkg-config commands to look up dependencies by name will then work with those environment variables defined by the hook.</p>"},{"location":"languages-frameworks/pkg-config.section.html#sec-pkg-config-usage-external","title":"Externally","text":"<p>The <code>defaultPkgConfigPackages</code> package set is a set of aliases, named after the modules they provide. This is meant to be used by language-to-nix integrations. Hand-written packages should use the normal Nixpkgs attribute name instead.</p>"},{"location":"languages-frameworks/python.section.html","title":"Python","text":""},{"location":"languages-frameworks/python.section.html#reference","title":"Reference","text":""},{"location":"languages-frameworks/python.section.html#interpreters","title":"Interpreters","text":"Package Aliases Interpreter python27 python2, python CPython 2.7 python38 CPython 3.8 python39 CPython 3.9 python310 CPython 3.10 python311 python3 CPython 3.11 python312 CPython 3.12 python313 CPython 3.13 pypy27 pypy2, pypy PyPy2.7 pypy39 pypy3 PyPy 3.9 <p>The Nix expressions for the interpreters can be found in <code>pkgs/development/interpreters/python</code>.</p> <p>All packages depending on any Python interpreter get appended <code>out/{python.sitePackages}</code> to <code>$PYTHONPATH</code> if such directory exists.</p>"},{"location":"languages-frameworks/python.section.html#missing-tkinter-module-standard-library","title":"Missing <code>tkinter</code> module standard library","text":"<p>To reduce closure size the <code>Tkinter</code>/<code>tkinter</code> is available as a separate package, <code>pythonPackages.tkinter</code>.</p>"},{"location":"languages-frameworks/python.section.html#attributes-on-interpreters-packages","title":"Attributes on interpreters packages","text":"<p>Each interpreter has the following attributes:</p> <ul> <li><code>libPrefix</code>. Name of the folder in <code>${python}/lib/</code> for corresponding interpreter.</li> <li><code>interpreter</code>. Alias for <code>${python}/bin/${executable}</code>.</li> <li><code>buildEnv</code>. Function to build python interpreter environments with extra packages bundled together. See  for usage and documentation.</li> <li><code>withPackages</code>. Simpler interface to <code>buildEnv</code>. See  for usage and documentation.</li> <li><code>sitePackages</code>. Alias for <code>lib/${libPrefix}/site-packages</code>.</li> <li><code>executable</code>. Name of the interpreter executable, e.g. <code>python3.10</code>.</li> <li><code>pkgs</code>. Set of Python packages for that specific interpreter. The package set can be modified by overriding the interpreter and passing <code>packageOverrides</code>.</li> </ul>"},{"location":"languages-frameworks/python.section.html#building-packages-and-applications","title":"Building packages and applications","text":"<p>Python libraries and applications that use <code>setuptools</code> or <code>distutils</code> are typically built with respectively the <code>buildPythonPackage</code> and <code>buildPythonApplication</code> functions. These two functions also support installing a <code>wheel</code>.</p> <p>All Python packages reside in <code>pkgs/top-level/python-packages.nix</code> and all applications elsewhere. In case a package is used as both a library and an application, then the package should be in <code>pkgs/top-level/python-packages.nix</code> since only those packages are made available for all interpreter versions. The preferred location for library expressions is in <code>pkgs/development/python-modules</code>. It is important that these packages are called from <code>pkgs/top-level/python-packages.nix</code> and not elsewhere, to guarantee the right version of the package is built.</p> <p>Based on the packages defined in <code>pkgs/top-level/python-packages.nix</code> an attribute set is created for each available Python interpreter. The available sets are</p> <ul> <li><code>pkgs.python27Packages</code></li> <li><code>pkgs.python3Packages</code></li> <li><code>pkgs.python38Packages</code></li> <li><code>pkgs.python39Packages</code></li> <li><code>pkgs.python310Packages</code></li> <li><code>pkgs.python311Packages</code></li> <li><code>pkgs.python312Packages</code></li> <li><code>pkgs.python313Packages</code></li> <li><code>pkgs.pypyPackages</code></li> </ul> <p>and the aliases</p> <ul> <li><code>pkgs.python2Packages</code> pointing to <code>pkgs.python27Packages</code></li> <li><code>pkgs.python3Packages</code> pointing to <code>pkgs.python311Packages</code></li> <li><code>pkgs.pythonPackages</code> pointing to <code>pkgs.python2Packages</code></li> </ul>"},{"location":"languages-frameworks/python.section.html#buildpythonpackage-function","title":"<code>buildPythonPackage</code> function","text":"<p>The <code>buildPythonPackage</code> function is implemented in <code>pkgs/development/interpreters/python/mk-python-derivation.nix</code> using setup hooks.</p> <p>The following is an example:</p> <pre><code>{ lib\n, buildPythonPackage\n, fetchPypi\n\n# build-system\n, setuptools-scm\n\n# dependencies\n, attrs\n, pluggy\n, py\n, setuptools\n, six\n\n# tests\n, hypothesis\n }:\n\nbuildPythonPackage rec {\n  pname = \"pytest\";\n  version = \"3.3.1\";\n  pyproject = true;\n\n  src = fetchPypi {\n    inherit pname version;\n    hash = \"sha256-z4Q23FnYaVNG/NOrKW3kZCXsqwDWQJbOvnn7Ueyy65M=\";\n  };\n\n  postPatch = ''\n    # don't test bash builtins\n    rm testing/test_argcomplete.py\n  '';\n\n  nativeBuildInputs = [\n    setuptools-scm\n  ];\n\n  propagatedBuildInputs = [\n    attrs\n    py\n    setuptools\n    six\n    pluggy\n  ];\n\n  nativeCheckInputs = [\n    hypothesis\n  ];\n\n  meta = with lib; {\n    changelog = \"https://github.com/pytest-dev/pytest/releases/tag/${version}\";\n    description = \"Framework for writing tests\";\n    homepage = \"https://github.com/pytest-dev/pytest\";\n    license = licenses.mit;\n    maintainers = with maintainers; [ domenkozar lovek323 madjar lsix ];\n  };\n}\n</code></pre> <p>The <code>buildPythonPackage</code> mainly does four things:</p> <ul> <li>In the <code>buildPhase</code>, it calls <code>${python.pythonOnBuildForHost.interpreter} setup.py bdist_wheel</code> to   build a wheel binary zipfile.</li> <li>In the <code>installPhase</code>, it installs the wheel file using <code>pip install *.whl</code>.</li> <li>In the <code>postFixup</code> phase, the <code>wrapPythonPrograms</code> bash function is called to   wrap all programs in the <code>$out/bin/*</code> directory to include <code>$PATH</code>   environment variable and add dependent libraries to script's <code>sys.path</code>.</li> <li>In the <code>installCheck</code> phase, <code>${python.interpreter} setup.py test</code> is run.</li> </ul> <p>By default tests are run because <code>doCheck = true</code>. Test dependencies, like e.g. the test runner, should be added to <code>nativeCheckInputs</code>.</p> <p>By default <code>meta.platforms</code> is set to the same value as the interpreter unless overridden otherwise.</p>"},{"location":"languages-frameworks/python.section.html#buildpythonpackage-parameters","title":"<code>buildPythonPackage</code> parameters","text":"<p>All parameters from <code>stdenv.mkDerivation</code> function are still supported. The following are specific to <code>buildPythonPackage</code>:</p> <ul> <li><code>catchConflicts ? true</code>: If <code>true</code>, abort package build if a package name   appears more than once in dependency tree. Default is <code>true</code>.</li> <li><code>disabled ? false</code>: If <code>true</code>, package is not built for the particular Python   interpreter version.</li> <li><code>dontWrapPythonPrograms ? false</code>: Skip wrapping of Python programs.</li> <li><code>permitUserSite ? false</code>: Skip setting the <code>PYTHONNOUSERSITE</code> environment   variable in wrapped programs.</li> <li><code>pyproject</code>: Whether the pyproject format should be used. When set to <code>true</code>,   <code>pypaBuildHook</code> will be used, and you can add the required build dependencies   from <code>build-system.requires</code> to <code>nativeBuildInputs</code>. Note that the pyproject   format falls back to using <code>setuptools</code>, so you can use <code>pyproject = true</code>   even if the package only has a <code>setup.py</code>. When set to <code>false</code>, you can   use the existing [hooks](#setup-hooks0 or provide your own logic to build the   package. This can be useful for packages that don't support the pyproject   format. When unset, the legacy <code>setuptools</code> hooks are used for backwards   compatibility.</li> <li><code>makeWrapperArgs ? []</code>: A list of strings. Arguments to be passed to   <code>makeWrapper</code>, which wraps generated binaries. By default, the arguments to   <code>makeWrapper</code> set <code>PATH</code> and <code>PYTHONPATH</code> environment variables before calling   the binary. Additional arguments here can allow a developer to set environment   variables which will be available when the binary is run. For example,   <code>makeWrapperArgs = [\"--set FOO BAR\" \"--set BAZ QUX\"]</code>.</li> <li><code>namePrefix</code>: Prepends text to <code>${name}</code> parameter. In case of libraries, this   defaults to <code>\"python3.8-\"</code> for Python 3.8, etc., and in case of applications to <code>\"\"</code>.</li> <li><code>pipInstallFlags ? []</code>: A list of strings. Arguments to be passed to <code>pip   install</code>. To pass options to <code>python setup.py install</code>, use   <code>--install-option</code>. E.g., <code>pipInstallFlags=[\"--install-option='--cpp_implementation'\"]</code>.</li> <li><code>pipBuildFlags ? []</code>: A list of strings. Arguments to be passed to <code>pip wheel</code>.</li> <li><code>pypaBuildFlags ? []</code>: A list of strings. Arguments to be passed to <code>python -m build --wheel</code>.</li> <li><code>pythonPath ? []</code>: List of packages to be added into <code>$PYTHONPATH</code>. Packages   in <code>pythonPath</code> are not propagated (contrary to <code>propagatedBuildInputs</code>).</li> <li><code>preShellHook</code>: Hook to execute commands before <code>shellHook</code>.</li> <li><code>postShellHook</code>: Hook to execute commands after <code>shellHook</code>.</li> <li><code>removeBinByteCode ? true</code>: Remove bytecode from <code>/bin</code>. Bytecode is only   created when the filenames end with <code>.py</code>.</li> <li><code>setupPyGlobalFlags ? []</code>: List of flags passed to <code>setup.py</code> command.</li> <li><code>setupPyBuildFlags ? []</code>: List of flags passed to <code>setup.py build_ext</code> command.</li> </ul> <p>The <code>stdenv.mkDerivation</code> function accepts various parameters for describing build inputs (see \"Specifying dependencies\"). The following are of special interest for Python packages, either because these are primarily used, or because their behaviour is different:</p> <ul> <li><code>nativeBuildInputs ? []</code>: Build-time only dependencies. Typically executables   as well as the items listed in <code>setup_requires</code>.</li> <li><code>buildInputs ? []</code>: Build and/or run-time dependencies that need to be   compiled for the host machine. Typically non-Python libraries which are being   linked.</li> <li><code>nativeCheckInputs ? []</code>: Dependencies needed for running the <code>checkPhase</code>. These   are added to <code>nativeBuildInputs</code> when <code>doCheck = true</code>. Items listed in   <code>tests_require</code> go here.</li> <li><code>propagatedBuildInputs ? []</code>: Aside from propagating dependencies,   <code>buildPythonPackage</code> also injects code into and wraps executables with the   paths included in this list. Items listed in <code>install_requires</code> go here.</li> </ul>"},{"location":"languages-frameworks/python.section.html#overriding-python-packages","title":"Overriding Python packages","text":"<p>The <code>buildPythonPackage</code> function has a <code>overridePythonAttrs</code> method that can be used to override the package. In the following example we create an environment where we have the <code>blaze</code> package using an older version of <code>pandas</code>. We override first the Python interpreter and pass <code>packageOverrides</code> which contains the overrides for packages in the package set.</p> <pre><code>with import &lt;nixpkgs&gt; {};\n\n(let\n  python = let\n    packageOverrides = self: super: {\n      pandas = super.pandas.overridePythonAttrs(old: rec {\n        version = \"0.19.1\";\n        src =  fetchPypi {\n          pname = \"pandas\";\n          inherit version;\n          hash = \"sha256-JQn+rtpy/OA2deLszSKEuxyttqBzcAil50H+JDHUdCE=\";\n        };\n      });\n    };\n  in pkgs.python3.override {inherit packageOverrides; self = python;};\n\nin python.withPackages(ps: [ ps.blaze ])).env\n</code></pre> <p>The next example shows a non trivial overriding of the <code>blas</code> implementation to be used through out all of the Python package set:</p> <pre><code>python3MyBlas = pkgs.python3.override {\n  packageOverrides = self: super: {\n    # We need toPythonModule for the package set to evaluate this\n    blas = super.toPythonModule(super.pkgs.blas.override {\n      blasProvider = super.pkgs.mkl;\n    });\n    lapack = super.toPythonModule(super.pkgs.lapack.override {\n      lapackProvider = super.pkgs.mkl;\n    });\n  };\n};\n</code></pre> <p>This is particularly useful for numpy and scipy users who want to gain speed with other blas implementations. Note that using <code>scipy = super.scipy.override { blas = super.pkgs.mkl; };</code> will likely result in compilation issues, because scipy dependencies need to use the same blas implementation as well.</p>"},{"location":"languages-frameworks/python.section.html#buildpythonapplication-function","title":"<code>buildPythonApplication</code> function","text":"<p>The <code>buildPythonApplication</code> function is practically the same as <code>buildPythonPackage</code>. The main purpose of this function is to build a Python package where one is interested only in the executables, and not importable modules. For that reason, when adding this package to a <code>python.buildEnv</code>, the modules won't be made available.</p> <p>Another difference is that <code>buildPythonPackage</code> by default prefixes the names of the packages with the version of the interpreter. Because this is irrelevant for applications, the prefix is omitted.</p> <p>When packaging a Python application with <code>buildPythonApplication</code>, it should be called with <code>callPackage</code> and passed <code>python3</code> or <code>python3Packages</code> (possibly specifying an interpreter version), like this:</p> <pre><code>{ lib\n, python3Packages\n, fetchPypi\n}:\n\npython3Packages.buildPythonApplication rec {\n  pname = \"luigi\";\n  version = \"2.7.9\";\n  pyproject = true;\n\n  src = fetchPypi {\n    inherit pname version;\n    hash  = \"sha256-Pe229rT0aHwA98s+nTHQMEFKZPo/yw6sot8MivFDvAw=\";\n  };\n\n  nativeBuildInputs = with python3Packages; [\n    setuptools\n  ];\n\n  propagatedBuildInputs = with python3Packages; [\n    tornado\n    python-daemon\n  ];\n\n  meta = with lib; {\n    # ...\n  };\n}\n</code></pre> <p>This is then added to <code>all-packages.nix</code> just as any other application would be.</p> <pre><code>luigi = callPackage ../applications/networking/cluster/luigi { };\n</code></pre> <p>Since the package is an application, a consumer doesn't need to care about Python versions or modules, which is why they don't go in <code>python3Packages</code>.</p>"},{"location":"languages-frameworks/python.section.html#topythonapplication-function","title":"<code>toPythonApplication</code> function","text":"<p>A distinction is made between applications and libraries, however, sometimes a package is used as both. In this case the package is added as a library to <code>python-packages.nix</code> and as an application to <code>all-packages.nix</code>. To reduce duplication the <code>toPythonApplication</code> can be used to convert a library to an application.</p> <p>The Nix expression shall use <code>buildPythonPackage</code> and be called from <code>python-packages.nix</code>. A reference shall be created from <code>all-packages.nix</code> to the attribute in <code>python-packages.nix</code>, and the <code>toPythonApplication</code> shall be applied to the reference:</p> <pre><code>youtube-dl = with python3Packages; toPythonApplication youtube-dl;\n</code></pre>"},{"location":"languages-frameworks/python.section.html#topythonmodule-function","title":"<code>toPythonModule</code> function","text":"<p>In some cases, such as bindings, a package is created using <code>stdenv.mkDerivation</code> and added as attribute in <code>all-packages.nix</code>. The Python bindings should be made available from <code>python-packages.nix</code>. The <code>toPythonModule</code> function takes a derivation and makes certain Python-specific modifications.</p> <pre><code>opencv = toPythonModule (pkgs.opencv.override {\n  enablePython = true;\n  pythonPackages = self;\n});\n</code></pre> <p>Do pay attention to passing in the right Python version!</p>"},{"location":"languages-frameworks/python.section.html#python.buildenv-function","title":"<code>python.buildEnv</code> function","text":"<p>Python environments can be created using the low-level <code>pkgs.buildEnv</code> function. This example shows how to create an environment that has the Pyramid Web Framework. Saving the following as <code>default.nix</code></p> <pre><code>with import &lt;nixpkgs&gt; {};\n\npython3.buildEnv.override {\n  extraLibs = [ python3Packages.pyramid ];\n  ignoreCollisions = true;\n}\n</code></pre> <p>and running <code>nix-build</code> will create</p> <pre><code>/nix/store/cf1xhjwzmdki7fasgr4kz6di72ykicl5-python-2.7.8-env\n</code></pre> <p>with wrapped binaries in <code>bin/</code>.</p> <p>You can also use the <code>env</code> attribute to create local environments with needed packages installed. This is somewhat comparable to <code>virtualenv</code>. For example, running <code>nix-shell</code> with the following <code>shell.nix</code></p> <pre><code>with import &lt;nixpkgs&gt; {};\n\n(python3.buildEnv.override {\n  extraLibs = with python3Packages; [\n    numpy\n    requests\n  ];\n}).env\n</code></pre> <p>will drop you into a shell where Python will have the specified packages in its path.</p>"},{"location":"languages-frameworks/python.section.html#python.buildenv-arguments","title":"<code>python.buildEnv</code> arguments","text":"<ul> <li><code>extraLibs</code>: List of packages installed inside the environment.</li> <li><code>postBuild</code>: Shell command executed after the build of environment.</li> <li><code>ignoreCollisions</code>: Ignore file collisions inside the environment (default is <code>false</code>).</li> <li><code>permitUserSite</code>: Skip setting the <code>PYTHONNOUSERSITE</code> environment variable in   wrapped binaries in the environment.</li> </ul>"},{"location":"languages-frameworks/python.section.html#python.withpackages-function","title":"<code>python.withPackages</code> function","text":"<p>The <code>python.withPackages</code> function provides a simpler interface to the <code>python.buildEnv</code> functionality. It takes a function as an argument that is passed the set of python packages and returns the list of the packages to be included in the environment. Using the <code>withPackages</code> function, the previous example for the Pyramid Web Framework environment can be written like this:</p> <pre><code>with import &lt;nixpkgs&gt; {};\n\npython.withPackages (ps: [ ps.pyramid ])\n</code></pre> <p><code>withPackages</code> passes the correct package set for the specific interpreter version as an argument to the function. In the above example, <code>ps</code> equals <code>pythonPackages</code>. But you can also easily switch to using python3:</p> <pre><code>with import &lt;nixpkgs&gt; {};\n\npython3.withPackages (ps: [ ps.pyramid ])\n</code></pre> <p>Now, <code>ps</code> is set to <code>python3Packages</code>, matching the version of the interpreter.</p> <p>As <code>python.withPackages</code> uses <code>python.buildEnv</code> under the hood, it also supports the <code>env</code> attribute. The <code>shell.nix</code> file from the previous section can thus be also written like this:</p> <pre><code>with import &lt;nixpkgs&gt; {};\n\n(python3.withPackages (ps: with ps; [\n  numpy\n  requests\n])).env\n</code></pre> <p>In contrast to <code>python.buildEnv</code>, <code>python.withPackages</code> does not support the more advanced options such as <code>ignoreCollisions = true</code> or <code>postBuild</code>. If you need them, you have to use <code>python.buildEnv</code>.</p> <p>Python 2 namespace packages may provide <code>__init__.py</code> that collide. In that case <code>python.buildEnv</code> should be used with <code>ignoreCollisions = true</code>.</p>"},{"location":"languages-frameworks/python.section.html#setup-hooks","title":"Setup hooks","text":"<p>The following are setup hooks specifically for Python packages. Most of these are used in <code>buildPythonPackage</code>.</p> <ul> <li><code>eggUnpackhook</code> to move an egg to the correct folder so it can be installed   with the <code>eggInstallHook</code></li> <li><code>eggBuildHook</code> to skip building for eggs.</li> <li><code>eggInstallHook</code> to install eggs.</li> <li><code>pipBuildHook</code> to build a wheel using <code>pip</code> and PEP 517. Note a build system   (e.g. <code>setuptools</code> or <code>flit</code>) should still be added as <code>nativeBuildInput</code>.</li> <li><code>pypaBuildHook</code> to build a wheel using   <code>pypa/build</code> and   PEP 517/518. Note a build system (e.g. <code>setuptools</code> or <code>flit</code>) should still   be added as <code>nativeBuildInput</code>.</li> <li><code>pipInstallHook</code> to install wheels.</li> <li><code>pytestCheckHook</code> to run tests with <code>pytest</code>. See example usage.</li> <li><code>pythonCatchConflictsHook</code> to check whether a Python package is not already existing.</li> <li><code>pythonImportsCheckHook</code> to check whether importing the listed modules works.</li> <li><code>pythonRelaxDepsHook</code> will relax Python dependencies restrictions for the package.   See example usage.</li> <li><code>pythonRemoveBinBytecode</code> to remove bytecode from the <code>/bin</code> folder.</li> <li><code>setuptoolsBuildHook</code> to build a wheel using <code>setuptools</code>.</li> <li><code>setuptoolsCheckHook</code> to run tests with <code>python setup.py test</code>.</li> <li><code>sphinxHook</code> to build documentation and manpages using Sphinx.</li> <li><code>venvShellHook</code> to source a Python 3 <code>venv</code> at the <code>venvDir</code> location. A   <code>venv</code> is created if it does not yet exist. <code>postVenvCreation</code> can be used to   to run commands only after venv is first created.</li> <li><code>wheelUnpackHook</code> to move a wheel to the correct folder so it can be installed   with the <code>pipInstallHook</code>.</li> <li><code>unittestCheckHook</code> will run tests with <code>python -m unittest discover</code>. See example usage.</li> </ul>"},{"location":"languages-frameworks/python.section.html#development-mode","title":"Development mode","text":"<p>Development or editable mode is supported. To develop Python packages <code>buildPythonPackage</code> has additional logic inside <code>shellPhase</code> to run <code>pip install -e . --prefix $TMPDIR/</code>for the package.</p> <p>Warning: <code>shellPhase</code> is executed only if <code>setup.py</code> exists.</p> <p>Given a <code>default.nix</code>:</p> <pre><code>with import &lt;nixpkgs&gt; {};\n\npython3Packages.buildPythonPackage {\n  name = \"myproject\";\n  buildInputs = with python3Packages; [ pyramid ];\n\n  src = ./.;\n}\n</code></pre> <p>Running <code>nix-shell</code> with no arguments should give you the environment in which the package would be built with <code>nix-build</code>.</p> <p>Shortcut to setup environments with C headers/libraries and Python packages:</p> <pre><code>nix-shell -p python3Packages.pyramid zlib libjpeg git\n</code></pre> <p>::: {.note} There is a boolean value <code>lib.inNixShell</code> set to <code>true</code> if nix-shell is invoked. :::</p>"},{"location":"languages-frameworks/python.section.html#user-guide","title":"User Guide","text":""},{"location":"languages-frameworks/python.section.html#using-python","title":"Using Python","text":""},{"location":"languages-frameworks/python.section.html#overview","title":"Overview","text":"<p>Several versions of the Python interpreter are available on Nix, as well as a high amount of packages. The attribute <code>python3</code> refers to the default interpreter, which is currently CPython 3.11. The attribute <code>python</code> refers to CPython 2.7 for backwards-compatibility. It is also possible to refer to specific versions, e.g. <code>python311</code> refers to CPython 3.11, and <code>pypy</code> refers to the default PyPy interpreter.</p> <p>Python is used a lot, and in different ways. This affects also how it is packaged. In the case of Python on Nix, an important distinction is made between whether the package is considered primarily an application, or whether it should be used as a library, i.e., of primary interest are the modules in <code>site-packages</code> that should be importable.</p> <p>In the Nixpkgs tree Python applications can be found throughout, depending on what they do, and are called from the main package set. Python libraries, however, are in separate sets, with one set per interpreter version.</p> <p>The interpreters have several common attributes. One of these attributes is <code>pkgs</code>, which is a package set of Python libraries for this specific interpreter. E.g., the <code>toolz</code> package corresponding to the default interpreter is <code>python3.pkgs.toolz</code>, and the CPython 3.11 version is <code>python311.pkgs.toolz</code>. The main package set contains aliases to these package sets, e.g. <code>pythonPackages</code> refers to <code>python.pkgs</code> and <code>python311Packages</code> to <code>python311.pkgs</code>.</p>"},{"location":"languages-frameworks/python.section.html#installing-python-and-packages","title":"Installing Python and packages","text":"<p>The Nix and NixOS manuals explain how packages are generally installed. In the case of Python and Nix, it is important to make a distinction between whether the package is considered an application or a library.</p> <p>Applications on Nix are typically installed into your user profile imperatively using <code>nix-env -i</code>, and on NixOS declaratively by adding the package name to <code>environment.systemPackages</code> in <code>/etc/nixos/configuration.nix</code>. Dependencies such as libraries are automatically installed and should not be installed explicitly.</p> <p>The same goes for Python applications. Python applications can be installed in your profile, and will be wrapped to find their exact library dependencies, without impacting other applications or polluting your user environment.</p> <p>But Python libraries you would like to use for development cannot be installed, at least not individually, because they won't be able to find each other resulting in import errors. Instead, it is possible to create an environment with <code>python.buildEnv</code> or <code>python.withPackages</code> where the interpreter and other executables are wrapped to be able to find each other and all of the modules.</p> <p>In the following examples we will start by creating a simple, ad-hoc environment with a nix-shell that has <code>numpy</code> and <code>toolz</code> in Python 3.11; then we will create a re-usable environment in a single-file Python script; then we will create a full Python environment for development with this same environment.</p> <p>Philosophically, this should be familiar to users who are used to a <code>venv</code> style of development: individual projects create their own Python environments without impacting the global environment or each other.</p>"},{"location":"languages-frameworks/python.section.html#ad-hoc-temporary-python-environment-with-nix-shell","title":"Ad-hoc temporary Python environment with <code>nix-shell</code>","text":"<p>The simplest way to start playing with the way nix wraps and sets up Python environments is with <code>nix-shell</code> at the cmdline. These environments create a temporary shell session with a Python and a precise list of packages (plus their runtime dependencies), with no other Python packages in the Python interpreter's scope.</p> <p>To create a Python 3.11 session with <code>numpy</code> and <code>toolz</code> available, run:</p> <pre><code>$ nix-shell -p 'python311.withPackages(ps: with ps; [ numpy toolz ])'\n</code></pre> <p>By default <code>nix-shell</code> will start a <code>bash</code> session with this interpreter in our <code>PATH</code>, so if we then run:</p> <p>```Python console [nix-shell:~/src/nixpkgs]$ python3 Python 3.11.3 (main, Apr  4 2023, 22:36:41) [GCC 12.2.0] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.</p> <p>import numpy; import toolz <pre><code>Note that no other modules are in scope, even if they were imperatively\ninstalled into our user environment as a dependency of a Python application:\n\n```Python console\n&gt;&gt;&gt; import requests\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nModuleNotFoundError: No module named 'requests'\n</code></pre></p> <p>We can add as many additional modules onto the <code>nix-shell</code> as we need, and we will still get 1 wrapped Python interpreter. We can start the interpreter directly like so:</p> <pre><code>$ nix-shell -p \"python311.withPackages (ps: with ps; [ numpy toolz requests ])\" --run python3\nthis derivation will be built:\n  /nix/store/r19yf5qgfiakqlhkgjahbg3zg79549n4-python3-3.11.2-env.drv\nbuilding '/nix/store/r19yf5qgfiakqlhkgjahbg3zg79549n4-python3-3.11.2-env.drv'...\ncreated 273 symlinks in user environment\nPython 3.11.2 (main, Feb  7 2023, 13:52:42) [GCC 12.2.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; import requests\n&gt;&gt;&gt;\n</code></pre> <p>Notice that this time it built a new Python environment, which now includes <code>requests</code>. Building an environment just creates wrapper scripts that expose the selected dependencies to the interpreter while re-using the actual modules. This means if any other env has installed <code>requests</code> or <code>numpy</code> in a different context, we don't need to recompile them -- we just recompile the wrapper script that sets up an interpreter pointing to them. This matters much more for \"big\" modules like <code>pytorch</code> or <code>tensorflow</code>.</p> <p>Module names usually match their names on pypi.org, but you can use the Nixpkgs search website to find them as well (along with non-python packages).</p> <p>At this point we can create throwaway experimental Python environments with arbitrary dependencies. This is a good way to get a feel for how the Python interpreter and dependencies work in Nix and NixOS, but to do some actual development, we'll want to make it a bit more persistent.</p>"},{"location":"languages-frameworks/python.section.html#running-python-scripts-and-using-nix-shell-as-shebang","title":"Running Python scripts and using <code>nix-shell</code> as shebang","text":"<p>Sometimes, we have a script whose header looks like this:</p> <pre><code>#!/usr/bin/env python3\nimport numpy as np\na = np.array([1,2])\nb = np.array([3,4])\nprint(f\"The dot product of {a} and {b} is: {np.dot(a, b)}\")\n</code></pre> <p>Executing this script requires a <code>python3</code> that has <code>numpy</code>. Using what we learned in the previous section, we could startup a shell and just run it like so:</p> <pre><code>$ nix-shell -p 'python311.withPackages (ps: with ps; [ numpy ])' --run 'python3 foo.py'\nThe dot product of [1 2] and [3 4] is: 11\n</code></pre> <p>But if we maintain the script ourselves, and if there are more dependencies, it may be nice to encode those dependencies in source to make the script re-usable without that bit of knowledge. That can be done by using <code>nix-shell</code> as a shebang, like so:</p> <pre><code>#!/usr/bin/env nix-shell\n#!nix-shell -i python3 -p \"python3.withPackages(ps: [ ps.numpy ])\"\nimport numpy as np\na = np.array([1,2])\nb = np.array([3,4])\nprint(f\"The dot product of {a} and {b} is: {np.dot(a, b)}\")\n</code></pre> <p>Then we execute it, without requiring any environment setup at all!</p> <pre><code>$ ./foo.py\nThe dot product of [1 2] and [3 4] is: 11\n</code></pre> <p>If the dependencies are not available on the host where <code>foo.py</code> is executed, it will build or download them from a Nix binary cache prior to starting up, prior that it is executed on a machine with a multi-user nix installation.</p> <p>This provides a way to ship a self bootstrapping Python script, akin to a statically linked binary, where it can be run on any machine (provided nix is installed) without having to assume that <code>numpy</code> is installed globally on the system.</p> <p>By default it is pulling the import checkout of Nixpkgs itself from our nix channel, which is nice as it cache aligns with our other package builds, but we can make it fully reproducible by pinning the <code>nixpkgs</code> import:</p> <pre><code>#!/usr/bin/env nix-shell\n#!nix-shell -i python3 -p \"python3.withPackages (ps: [ ps.numpy ])\"\n#!nix-shell -I nixpkgs=https://github.com/NixOS/nixpkgs/archive/e51209796c4262bfb8908e3d6d72302fe4e96f5f.tar.gz\nimport numpy as np\na = np.array([1,2])\nb = np.array([3,4])\nprint(f\"The dot product of {a} and {b} is: {np.dot(a, b)}\")\n</code></pre> <p>This will execute with the exact same versions of Python 3.10, numpy, and system dependencies a year from now as it does today, because it will always use exactly git commit <code>e51209796c4262bfb8908e3d6d72302fe4e96f5f</code> of Nixpkgs for all of the package versions.</p> <p>This is also a great way to ensure the script executes identically on different servers.</p>"},{"location":"languages-frameworks/python.section.html#load-environment-from-.nix-expression","title":"Load environment from <code>.nix</code> expression","text":"<p>We've now seen how to create an ad-hoc temporary shell session, and how to create a single script with Python dependencies, but in the course of normal development we're usually working in an entire package repository.</p> <p>As explained in the <code>nix-shell</code> section of the Nix manual, <code>nix-shell</code> can also load an expression from a <code>.nix</code> file. Say we want to have Python 3.11, <code>numpy</code> and <code>toolz</code>, like before, in an environment. We can add a <code>shell.nix</code> file describing our dependencies:</p> <pre><code>with import &lt;nixpkgs&gt; {};\n(python311.withPackages (ps: with ps; [\n  numpy\n  toolz\n])).env\n</code></pre> <p>And then at the command line, just typing <code>nix-shell</code> produces the same environment as before. In a normal project, we'll likely have many more dependencies; this can provide a way for developers to share the environments with each other and with CI builders.</p> <p>What's happening here?</p> <ol> <li>We begin with importing the Nix Packages collections. <code>import &lt;nixpkgs&gt;</code>    imports the <code>&lt;nixpkgs&gt;</code> function, <code>{}</code> calls it and the <code>with</code> statement    brings all attributes of <code>nixpkgs</code> in the local scope. These attributes form    the main package set.</li> <li>Then we create a Python 3.11 environment with the <code>withPackages</code> function, as before.</li> <li>The <code>withPackages</code> function expects us to provide a function as an argument    that takes the set of all Python packages and returns a list of packages to    include in the environment. Here, we select the packages <code>numpy</code> and <code>toolz</code>    from the package set.</li> </ol> <p>To combine this with <code>mkShell</code> you can:</p> <pre><code>with import &lt;nixpkgs&gt; {};\nlet\n  pythonEnv = python311.withPackages (ps: [\n    ps.numpy\n    ps.toolz\n  ]);\nin mkShell {\n  packages = [\n    pythonEnv\n\n    black\n    mypy\n\n    libffi\n    openssl\n  ];\n}\n</code></pre> <p>This will create a unified environment that has not just our Python interpreter and its Python dependencies, but also tools like <code>black</code> or <code>mypy</code> and libraries like <code>libffi</code> the <code>openssl</code> in scope. This is generic and can span any number of tools or languages across the Nixpkgs ecosystem.</p>"},{"location":"languages-frameworks/python.section.html#installing-environments-globally-on-the-system","title":"Installing environments globally on the system","text":"<p>Up to now, we've been creating environments scoped to an ad-hoc shell session, or a single script, or a single project. This is generally advisable, as it avoids pollution across contexts.</p> <p>However, sometimes we know we will often want a Python with some basic packages, and want this available without having to enter into a shell or build context. This can be useful to have things like vim/emacs editors and plugins or shell tools \"just work\" without having to set them up, or when running other software that expects packages to be installed globally.</p> <p>To create your own custom environment, create a file in <code>~/.config/nixpkgs/overlays/</code> that looks like this:</p> <pre><code># ~/.config/nixpkgs/overlays/myEnv.nix\nself: super: {\n  myEnv = super.buildEnv {\n    name = \"myEnv\";\n    paths = [\n      # A Python 3 interpreter with some packages\n      (self.python3.withPackages (\n        ps: with ps; [\n          pyflakes\n          pytest\n          black\n        ]\n      ))\n\n      # Some other packages we'd like as part of this env\n      self.mypy\n      self.black\n      self.ripgrep\n      self.tmux\n    ];\n  };\n}\n</code></pre> <p>You can then build and install this to your profile with:</p> <pre><code>nix-env -iA myEnv\n</code></pre> <p>One limitation of this is that you can only have 1 Python env installed globally, since they conflict on the <code>python</code> to load out of your <code>PATH</code>.</p> <p>If you get a conflict or prefer to keep the setup clean, you can have <code>nix-env</code> atomically uninstall all other imperatively installed packages and replace your profile with just <code>myEnv</code> by using the <code>--replace</code> flag.</p>"},{"location":"languages-frameworks/python.section.html#environment-defined-in-etcnixosconfiguration.nix","title":"Environment defined in <code>/etc/nixos/configuration.nix</code>","text":"<p>For the sake of completeness, here's how to install the environment system-wide on NixOS.</p> <pre><code>{ # ...\n\n  environment.systemPackages = with pkgs; [\n    (python310.withPackages(ps: with ps; [ numpy toolz ]))\n  ];\n}\n</code></pre>"},{"location":"languages-frameworks/python.section.html#developing-with-python","title":"Developing with Python","text":"<p>Above, we were mostly just focused on use cases and what to do to get started creating working Python environments in nix.</p> <p>Now that you know the basics to be up and running, it is time to take a step back and take a deeper look at how Python packages are packaged on Nix. Then, we will look at how you can use development mode with your code.</p>"},{"location":"languages-frameworks/python.section.html#python-library-packages-in-nixpkgs","title":"Python library packages in Nixpkgs","text":"<p>With Nix all packages are built by functions. The main function in Nix for building Python libraries is <code>buildPythonPackage</code>. Let's see how we can build the <code>toolz</code> package.</p> <pre><code>{ lib\n, buildPythonPackage\n, fetchPypi\n, setuptools\n, wheel\n}:\n\nbuildPythonPackage rec {\n  pname = \"toolz\";\n  version = \"0.10.0\";\n  pyproject = true;\n\n  src = fetchPypi {\n    inherit pname version;\n    hash = \"sha256-CP3V73yWSArRHBLUct4hrNMjWZlvaaUlkpm1QP66RWA=\";\n  };\n\n  nativeBuildInputs = [\n    setuptools\n    wheel\n  ];\n\n  # has no tests\n  doCheck = false;\n\n  pythonImportsCheck = [\n    \"toolz.itertoolz\"\n    \"toolz.functoolz\"\n    \"toolz.dicttoolz\"\n  ];\n\n  meta = with lib; {\n    changelog = \"https://github.com/pytoolz/toolz/releases/tag/${version}\";\n    homepage = \"https://github.com/pytoolz/toolz\";\n    description = \"List processing tools and functional utilities\";\n    license = licenses.bsd3;\n    maintainers = with maintainers; [ fridh ];\n  };\n}\n</code></pre> <p>What happens here? The function <code>buildPythonPackage</code> is called and as argument it accepts a set. In this case the set is a recursive set, <code>rec</code>. One of the arguments is the name of the package, which consists of a basename (generally following the name on PyPi) and a version. Another argument, <code>src</code> specifies the source, which in this case is fetched from PyPI using the helper function <code>fetchPypi</code>. The argument <code>doCheck</code> is used to set whether tests should be run when building the package. Since there are no tests, we rely on <code>pythonImportsCheck</code> to test whether the package can be imported. Furthermore, we specify some meta information. The output of the function is a derivation.</p> <p>An expression for <code>toolz</code> can be found in the Nixpkgs repository. As explained in the introduction of this Python section, a derivation of <code>toolz</code> is available for each interpreter version, e.g. <code>python311.pkgs.toolz</code> refers to the <code>toolz</code> derivation corresponding to the CPython 3.11 interpreter.</p> <p>The above example works when you're directly working on <code>pkgs/top-level/python-packages.nix</code> in the Nixpkgs repository. Often though, you will want to test a Nix expression outside of the Nixpkgs tree.</p> <p>The following expression creates a derivation for the <code>toolz</code> package, and adds it along with a <code>numpy</code> package to a Python environment.</p> <pre><code>with import &lt;nixpkgs&gt; {};\n\n( let\n    my_toolz = python311.pkgs.buildPythonPackage rec {\n      pname = \"toolz\";\n      version = \"0.10.0\";\n      pyproject = true;\n\n      src = fetchPypi {\n        inherit pname version;\n        hash = \"sha256-CP3V73yWSArRHBLUct4hrNMjWZlvaaUlkpm1QP66RWA=\";\n      };\n\n      nativeBuildInputs = [\n        python311.pkgs.setuptools\n        python311.pkgs.wheel\n      ];\n\n      # has no tests\n      doCheck = false;\n\n      meta = {\n        homepage = \"https://github.com/pytoolz/toolz/\";\n        description = \"List processing tools and functional utilities\";\n        # [...]\n      };\n    };\n\n  in python311.withPackages (ps: with ps; [\n    numpy\n    my_toolz\n  ])\n).env\n</code></pre> <p>Executing <code>nix-shell</code> will result in an environment in which you can use Python 3.11 and the <code>toolz</code> package. As you can see we had to explicitly mention for which Python version we want to build a package.</p> <p>So, what did we do here? Well, we took the Nix expression that we used earlier to build a Python environment, and said that we wanted to include our own version of <code>toolz</code>, named <code>my_toolz</code>. To introduce our own package in the scope of <code>withPackages</code> we used a <code>let</code> expression. You can see that we used <code>ps.numpy</code> to select numpy from the nixpkgs package set (<code>ps</code>). We did not take <code>toolz</code> from the Nixpkgs package set this time, but instead took our own version that we introduced with the <code>let</code> expression.</p>"},{"location":"languages-frameworks/python.section.html#handling-dependencies","title":"Handling dependencies","text":"<p>Our example, <code>toolz</code>, does not have any dependencies on other Python packages or system libraries. According to the manual, <code>buildPythonPackage</code> uses the arguments <code>buildInputs</code> and <code>propagatedBuildInputs</code> to specify dependencies. If something is exclusively a build-time dependency, then the dependency should be included in <code>buildInputs</code>, but if it is (also) a runtime dependency, then it should be added to <code>propagatedBuildInputs</code>. Test dependencies are considered build-time dependencies and passed to <code>nativeCheckInputs</code>.</p> <p>The following example shows which arguments are given to <code>buildPythonPackage</code> in order to build <code>datashape</code>.</p> <pre><code>{ lib\n, buildPythonPackage\n, fetchPypi\n\n# build dependencies\n, setuptools, wheel\n\n# dependencies\n, numpy, multipledispatch, python-dateutil\n\n# tests\n, pytest\n}:\n\nbuildPythonPackage rec {\n  pname = \"datashape\";\n  version = \"0.4.7\";\n  pyproject = true;\n\n  src = fetchPypi {\n    inherit pname version;\n    hash = \"sha256-FLLvdm1MllKrgTGC6Gb0k0deZeVYvtCCLji/B7uhong=\";\n  };\n\n  nativeBuildInputs = [\n    setuptools\n    wheel\n  ];\n\n  propagatedBuildInputs = [\n    multipledispatch\n    numpy\n    python-dateutil\n  ];\n\n  nativeCheckInputs = [\n    pytest\n  ];\n\n  meta = with lib; {\n    changelog = \"https://github.com/blaze/datashape/releases/tag/${version}\";\n    homepage = \"https://github.com/ContinuumIO/datashape\";\n    description = \"A data description language\";\n    license = licenses.bsd2;\n    maintainers = with maintainers; [ fridh ];\n  };\n}\n</code></pre> <p>We can see several runtime dependencies, <code>numpy</code>, <code>multipledispatch</code>, and <code>python-dateutil</code>. Furthermore, we have <code>nativeCheckInputs</code> with <code>pytest</code>. <code>pytest</code> is a test runner and is only used during the <code>checkPhase</code> and is therefore not added to <code>propagatedBuildInputs</code>.</p> <p>In the previous case we had only dependencies on other Python packages to consider. Occasionally you have also system libraries to consider. E.g., <code>lxml</code> provides Python bindings to <code>libxml2</code> and <code>libxslt</code>. These libraries are only required when building the bindings and are therefore added as <code>buildInputs</code>.</p> <pre><code>{ lib\n, buildPythonPackage\n, fetchPypi\n, setuptools\n, wheel\n, libxml2\n, libxslt\n}:\n\nbuildPythonPackage rec {\n  pname = \"lxml\";\n  version = \"3.4.4\";\n  pyproject = true;\n\n  src = fetchPypi {\n    inherit pname version;\n    hash = \"sha256-s9NiusRxFydHzaNRMjjxFcvWxfi45jGb9ql6eJJyQJk=\";\n  };\n\n  nativeBuildInputs = [\n    setuptools\n    wheel\n  ];\n\n  buildInputs = [\n    libxml2\n    libxslt\n  ];\n\n  meta = with lib; {\n    changelog = \"https://github.com/lxml/lxml/releases/tag/lxml-${version}\";\n    description = \"Pythonic binding for the libxml2 and libxslt libraries\";\n    homepage = \"https://lxml.de\";\n    license = licenses.bsd3;\n    maintainers = with maintainers; [ sjourdois ];\n  };\n}\n</code></pre> <p>In this example <code>lxml</code> and Nix are able to work out exactly where the relevant files of the dependencies are. This is not always the case.</p> <p>The example below shows bindings to The Fastest Fourier Transform in the West, commonly known as FFTW. On Nix we have separate packages of FFTW for the different types of floats (<code>\"single\"</code>, <code>\"double\"</code>, <code>\"long-double\"</code>). The bindings need all three types, and therefore we add all three as <code>buildInputs</code>. The bindings don't expect to find each of them in a different folder, and therefore we have to set <code>LDFLAGS</code> and <code>CFLAGS</code>.</p> <pre><code>{ lib\n, buildPythonPackage\n, fetchPypi\n\n# build dependencies\n, setuptools\n, wheel\n\n# dependencies\n, fftw\n, fftwFloat\n, fftwLongDouble\n, numpy\n, scipy\n}:\n\nbuildPythonPackage rec {\n  pname = \"pyFFTW\";\n  version = \"0.9.2\";\n  pyproject = true;\n\n  src = fetchPypi {\n    inherit pname version;\n    hash = \"sha256-9ru2r6kwhUCaskiFoaPNuJCfCVoUL01J40byvRt4kHQ=\";\n  };\n\n  nativeBuildInputs = [\n    setuptools\n    wheel\n  ];\n\n  buildInputs = [\n    fftw\n    fftwFloat\n    fftwLongDouble\n  ];\n\n  propagatedBuildInputs = [\n    numpy\n    scipy\n  ];\n\n  preConfigure = ''\n    export LDFLAGS=\"-L${fftw.dev}/lib -L${fftwFloat.out}/lib -L${fftwLongDouble.out}/lib\"\n    export CFLAGS=\"-I${fftw.dev}/include -I${fftwFloat.dev}/include -I${fftwLongDouble.dev}/include\"\n  '';\n\n  # Tests cannot import pyfftw. pyfftw works fine though.\n  doCheck = false;\n\n  meta = with lib; {\n    changelog = \"https://github.com/pyFFTW/pyFFTW/releases/tag/v${version}\";\n    description = \"A pythonic wrapper around FFTW, the FFT library, presenting a unified interface for all the supported transforms\";\n    homepage = \"http://hgomersall.github.com/pyFFTW\";\n    license = with licenses; [ bsd2 bsd3 ];\n    maintainers = with maintainers; [ fridh ];\n  };\n}\n</code></pre> <p>Note also the line <code>doCheck = false;</code>, we explicitly disabled running the test-suite.</p>"},{"location":"languages-frameworks/python.section.html#testing-python-packages","title":"Testing Python Packages","text":"<p>It is highly encouraged to have testing as part of the package build. This helps to avoid situations where the package was able to build and install, but is not usable at runtime. Currently, all packages will use the <code>test</code> command provided by the setup.py (i.e. <code>python setup.py test</code>). However, this is currently deprecated https://github.com/pypa/setuptools/pull/1878 and your package should provide its own <code>checkPhase</code>.</p> <p>::: {.note} The <code>checkPhase</code> for python maps to the <code>installCheckPhase</code> on a normal derivation. This is due to many python packages not behaving well to the pre-installed version of the package. Version info, and natively compiled extensions generally only exist in the install directory, and thus can cause issues when a test suite asserts on that behavior. :::</p> <p>::: {.note} Tests should only be disabled if they don't agree with nix (e.g. external dependencies, network access, flakey tests), however, as many tests should be enabled as possible. Failing tests can still be a good indication that the package is not in a valid state. :::</p>"},{"location":"languages-frameworks/python.section.html#using-pytest","title":"Using pytest","text":"<p>Pytest is the most common test runner for python repositories. A trivial test run would be:</p> <pre><code>  nativeCheckInputs = [ pytest ];\n  checkPhase = ''\n    runHook preCheck\n\n    pytest\n\n    runHook postCheck\n  '';\n</code></pre> <p>However, many repositories' test suites do not translate well to nix's build sandbox, and will generally need many tests to be disabled.</p> <p>To filter tests using pytest, one can do the following:</p> <pre><code>  nativeCheckInputs = [ pytest ];\n  # avoid tests which need additional data or touch network\n  checkPhase = ''\n    runHook preCheck\n\n    pytest tests/ --ignore=tests/integration -k 'not download and not update' --ignore=tests/test_failing.py\n\n    runHook postCheck\n  '';\n</code></pre> <p><code>--ignore</code> will tell pytest to ignore that file or directory from being collected as part of a test run. This is useful is a file uses a package which is not available in nixpkgs, thus skipping that test file is much easier than having to create a new package.</p> <p><code>-k</code> is used to define a predicate for test names. In this example, we are filtering out tests which contain <code>download</code> or <code>update</code> in their test case name. Only one <code>-k</code> argument is allowed, and thus a long predicate should be concatenated with \u201c\\\u201d and wrapped to the next line.</p> <p>::: {.note} In pytest==6.0.1, the use of \u201c\\\u201d to continue a line (e.g. <code>-k 'not download \\'</code>) has been removed, in this case, it's recommended to use <code>pytestCheckHook</code>. :::</p>"},{"location":"languages-frameworks/python.section.html#using-pytestcheckhook","title":"Using pytestCheckHook","text":"<p><code>pytestCheckHook</code> is a convenient hook which will substitute the setuptools <code>test</code> command for a <code>checkPhase</code> which runs <code>pytest</code>. This is also beneficial when a package may need many items disabled to run the test suite.</p> <p>Using the example above, the analogous <code>pytestCheckHook</code> usage would be:</p> <pre><code>  nativeCheckInputs = [\n    pytestCheckHook\n  ];\n\n  # requires additional data\n  pytestFlagsArray = [\n    \"tests/\"\n    \"--ignore=tests/integration\"\n  ];\n\n  disabledTests = [\n    # touches network\n    \"download\"\n    \"update\"\n  ];\n\n  disabledTestPaths = [\n    \"tests/test_failing.py\"\n  ];\n</code></pre> <p>This is especially useful when tests need to be conditionally disabled, for example:</p> <pre><code>  disabledTests = [\n    # touches network\n    \"download\"\n    \"update\"\n  ] ++ lib.optionals (pythonAtLeast \"3.8\") [\n    # broken due to python3.8 async changes\n    \"async\"\n  ] ++ lib.optionals stdenv.isDarwin [\n    # can fail when building with other packages\n    \"socket\"\n  ];\n</code></pre> <p>Trying to concatenate the related strings to disable tests in a regular <code>checkPhase</code> would be much harder to read. This also enables us to comment on why specific tests are disabled.</p>"},{"location":"languages-frameworks/python.section.html#using-pythonimportscheck","title":"Using pythonImportsCheck","text":"<p>Although unit tests are highly preferred to validate correctness of a package, not all packages have test suites that can be run easily, and some have none at all. To help ensure the package still works, <code>pythonImportsCheck</code> can attempt to import the listed modules.</p> <pre><code>  pythonImportsCheck = [\n    \"requests\"\n    \"urllib\"\n  ];\n</code></pre> <p>roughly translates to:</p> <pre><code>  postCheck = ''\n    PYTHONPATH=$out/${python.sitePackages}:$PYTHONPATH\n    python -c \"import requests; import urllib\"\n  '';\n</code></pre> <p>However, this is done in its own phase, and not dependent on whether <code>doCheck = true;</code>.</p> <p>This can also be useful in verifying that the package doesn't assume commonly present packages (e.g. <code>setuptools</code>).</p>"},{"location":"languages-frameworks/python.section.html#using-pythonrelaxdepshook","title":"Using pythonRelaxDepsHook","text":"<p>It is common for upstream to specify a range of versions for its package dependencies. This makes sense, since it ensures that the package will be built with a subset of packages that is well tested. However, this commonly causes issues when packaging in Nixpkgs, because the dependencies that this package may need are too new or old for the package to build correctly. We also cannot package multiple versions of the same package since this may cause conflicts in <code>PYTHONPATH</code>.</p> <p>One way to side step this issue is to relax the dependencies. This can be done by either removing the package version range or by removing the package declaration entirely. This can be done using the <code>pythonRelaxDepsHook</code> hook. For example, given the following <code>requirements.txt</code> file:</p> <pre><code>pkg1&lt;1.0\npkg2\npkg3&gt;=1.0,&lt;=2.0\n</code></pre> <p>we can do:</p> <pre><code>  nativeBuildInputs = [\n    pythonRelaxDepsHook\n  ];\n  pythonRelaxDeps = [\n    \"pkg1\"\n    \"pkg3\"\n  ];\n  pythonRemoveDeps = [\n    \"pkg2\"\n  ];\n</code></pre> <p>which would result in the following <code>requirements.txt</code> file:</p> <pre><code>pkg1\npkg3\n</code></pre> <p>Another option is to pass <code>true</code>, that will relax/remove all dependencies, for example:</p> <pre><code>  nativeBuildInputs = [ pythonRelaxDepsHook ];\n  pythonRelaxDeps = true;\n</code></pre> <p>which would result in the following <code>requirements.txt</code> file:</p> <pre><code>pkg1\npkg2\npkg3\n</code></pre> <p>In general you should always use <code>pythonRelaxDeps</code>, because <code>pythonRemoveDeps</code> will convert build errors into runtime errors. However <code>pythonRemoveDeps</code> may still be useful in exceptional cases, and also to remove dependencies wrongly declared by upstream (for example, declaring <code>black</code> as a runtime dependency instead of a dev dependency).</p> <p>Keep in mind that while the examples above are done with <code>requirements.txt</code>, <code>pythonRelaxDepsHook</code> works by modifying the resulting wheel file, so it should work with any of the existing hooks.</p>"},{"location":"languages-frameworks/python.section.html#using-unittestcheckhook","title":"Using unittestCheckHook","text":"<p><code>unittestCheckHook</code> is a hook which will substitute the setuptools <code>test</code> command for a <code>checkPhase</code> which runs <code>python -m unittest discover</code>:</p> <pre><code>  nativeCheckInputs = [\n    unittestCheckHook\n  ];\n\n  unittestFlagsArray = [\n    \"-s\" \"tests\" \"-v\"\n  ];\n</code></pre>"},{"location":"languages-frameworks/python.section.html#using-sphinxhook","title":"Using sphinxHook","text":"<p>The <code>sphinxHook</code> is a helpful tool to build documentation and manpages using the popular Sphinx documentation generator. It is setup to automatically find common documentation source paths and render them using the default <code>html</code> style.</p> <pre><code>  outputs = [\n    \"out\"\n    \"doc\"\n  ];\n\n  nativeBuildInputs = [\n    sphinxHook\n  ];\n</code></pre> <p>The hook will automatically build and install the artifact into the <code>doc</code> output, if it exists. It also provides an automatic diversion for the artifacts of the <code>man</code> builder into the <code>man</code> target.</p> <pre><code>  outputs = [\n    \"out\"\n    \"doc\"\n    \"man\"\n  ];\n\n  # Use multiple builders\n  sphinxBuilders = [\n    \"singlehtml\"\n    \"man\"\n  ];\n</code></pre> <p>Overwrite <code>sphinxRoot</code> when the hook is unable to find your documentation source root.</p> <pre><code>  # Configure sphinxRoot for uncommon paths\n  sphinxRoot = \"weird/docs/path\";\n</code></pre> <p>The hook is also available to packages outside the python ecosystem by referencing it using <code>sphinxHook</code> from top-level.</p>"},{"location":"languages-frameworks/python.section.html#develop-local-package","title":"Develop local package","text":"<p>As a Python developer you're likely aware of development mode (<code>python setup.py develop</code>); instead of installing the package this command creates a special link to the project code. That way, you can run updated code without having to reinstall after each and every change you make. Development mode is also available. Let's see how you can use it.</p> <p>In the previous Nix expression the source was fetched from a url. We can also refer to a local source instead using <code>src = ./path/to/source/tree;</code></p> <p>If we create a <code>shell.nix</code> file which calls <code>buildPythonPackage</code>, and if <code>src</code> is a local source, and if the local source has a <code>setup.py</code>, then development mode is activated.</p> <p>In the following example, we create a simple environment that has a Python 3.11 version of our package in it, as well as its dependencies and other packages we like to have in the environment, all specified with <code>propagatedBuildInputs</code>. Indeed, we can just add any package we like to have in our environment to <code>propagatedBuildInputs</code>.</p> <pre><code>with import &lt;nixpkgs&gt; {};\nwith python311Packages;\n\nbuildPythonPackage rec {\n  name = \"mypackage\";\n  src = ./path/to/package/source;\n  propagatedBuildInputs = [\n    pytest\n    numpy\n    pkgs.libsndfile\n  ];\n}\n</code></pre> <p>It is important to note that due to how development mode is implemented on Nix it is not possible to have multiple packages simultaneously in development mode.</p>"},{"location":"languages-frameworks/python.section.html#organising-your-packages","title":"Organising your packages","text":"<p>So far we discussed how you can use Python on Nix, and how you can develop with it. We've looked at how you write expressions to package Python packages, and we looked at how you can create environments in which specified packages are available.</p> <p>At some point you'll likely have multiple packages which you would like to be able to use in different projects. In order to minimise unnecessary duplication we now look at how you can maintain a repository with your own packages. The important functions here are <code>import</code> and <code>callPackage</code>.</p>"},{"location":"languages-frameworks/python.section.html#including-a-derivation-using-callpackage","title":"Including a derivation using <code>callPackage</code>","text":"<p>Earlier we created a Python environment using <code>withPackages</code>, and included the <code>toolz</code> package via a <code>let</code> expression. Let's split the package definition from the environment definition.</p> <p>We first create a function that builds <code>toolz</code> in <code>~/path/to/toolz/release.nix</code></p> <pre><code>{ lib\n, buildPythonPackage\n, fetchPypi\n, setuptools\n, wheel\n}:\n\nbuildPythonPackage rec {\n  pname = \"toolz\";\n  version = \"0.10.0\";\n  pyproject = true;\n\n  src = fetchPypi {\n    inherit pname version;\n    hash = \"sha256-CP3V73yWSArRHBLUct4hrNMjWZlvaaUlkpm1QP66RWA=\";\n  };\n\n  nativeBuildInputs = [\n    setuptools\n    wheel\n  ];\n\n  meta = with lib; {\n    changelog = \"https://github.com/pytoolz/toolz/releases/tag/${version}\";\n    homepage = \"https://github.com/pytoolz/toolz/\";\n    description = \"List processing tools and functional utilities\";\n    license = licenses.bsd3;\n    maintainers = with maintainers; [ fridh ];\n  };\n}\n</code></pre> <p>It takes an argument <code>buildPythonPackage</code>. We now call this function using <code>callPackage</code> in the definition of our environment</p> <pre><code>with import &lt;nixpkgs&gt; {};\n\n( let\n    toolz = callPackage /path/to/toolz/release.nix {\n      buildPythonPackage = python310\nPackages.buildPythonPackage;\n    };\n  in python310.withPackages (ps: [\n    ps.numpy\n    toolz\n  ])\n).env\n</code></pre> <p>Important to remember is that the Python version for which the package is made depends on the <code>python</code> derivation that is passed to <code>buildPythonPackage</code>. Nix tries to automatically pass arguments when possible, which is why generally you don't explicitly define which <code>python</code> derivation should be used. In the above example we use <code>buildPythonPackage</code> that is part of the set <code>python3Packages</code>, and in this case the <code>python3</code> interpreter is automatically used.</p>"},{"location":"languages-frameworks/python.section.html#faq","title":"FAQ","text":""},{"location":"languages-frameworks/python.section.html#how-to-solve-circular-dependencies","title":"How to solve circular dependencies?","text":"<p>Consider the packages <code>A</code> and <code>B</code> that depend on each other. When packaging <code>B</code>, a solution is to override package <code>A</code> not to depend on <code>B</code> as an input. The same should also be done when packaging <code>A</code>.</p>"},{"location":"languages-frameworks/python.section.html#how-to-override-a-python-package","title":"How to override a Python package?","text":"<p>We can override the interpreter and pass <code>packageOverrides</code>. In the following example we rename the <code>pandas</code> package and build it.</p> <pre><code>with import &lt;nixpkgs&gt; {};\n\n(let\n  python = let\n    packageOverrides = self: super: {\n      pandas = super.pandas.overridePythonAttrs(old: {name=\"foo\";});\n    };\n  in pkgs.python310.override {\n    inherit packageOverrides;\n  };\n\nin python.withPackages (ps: [\n  ps.pandas\n])).env\n</code></pre> <p>Using <code>nix-build</code> on this expression will build an environment that contains the package <code>pandas</code> but with the new name <code>foo</code>.</p> <p>All packages in the package set will use the renamed package. A typical use case is to switch to another version of a certain package. For example, in the Nixpkgs repository we have multiple versions of <code>django</code> and <code>scipy</code>. In the following example we use a different version of <code>scipy</code> and create an environment that uses it. All packages in the Python package set will now use the updated <code>scipy</code> version.</p> <pre><code>with import &lt;nixpkgs&gt; {};\n\n( let\n    packageOverrides = self: super: {\n      scipy = super.scipy_0_17;\n    };\n  in (pkgs.python310.override {\n    inherit packageOverrides;\n  }).withPackages (ps: [\n    ps.blaze\n  ])\n).env\n</code></pre> <p>The requested package <code>blaze</code> depends on <code>pandas</code> which itself depends on <code>scipy</code>.</p> <p>If you want the whole of Nixpkgs to use your modifications, then you can use <code>overlays</code> as explained in this manual. In the following example we build a <code>inkscape</code> using a different version of <code>numpy</code>.</p> <pre><code>let\n  pkgs = import &lt;nixpkgs&gt; {};\n  newpkgs = import pkgs.path { overlays = [ (self: super: {\n    python310 = let\n      packageOverrides = python-self: python-super: {\n        numpy = python-super.numpy_1_18;\n      };\n    in super.python310.override {inherit packageOverrides;};\n  } ) ]; };\nin newpkgs.inkscape\n</code></pre>"},{"location":"languages-frameworks/python.section.html#python-setup.py-bdist_wheel-cannot-create-.whl","title":"<code>python setup.py bdist_wheel</code> cannot create .whl","text":"<p>Executing <code>python setup.py bdist_wheel</code> in a <code>nix-shell</code>fails with</p> <pre><code>ValueError: ZIP does not support timestamps before 1980\n</code></pre> <p>This is because files from the Nix store (which have a timestamp of the UNIX epoch of January 1, 1970) are included in the .ZIP, but .ZIP archives follow the DOS convention of counting timestamps from 1980.</p> <p>The command <code>bdist_wheel</code> reads the <code>SOURCE_DATE_EPOCH</code> environment variable, which <code>nix-shell</code> sets to 1. Unsetting this variable or giving it a value corresponding to 1980 or later enables building wheels.</p> <p>Use 1980 as timestamp:</p> <pre><code>nix-shell --run \"SOURCE_DATE_EPOCH=315532800 python3 setup.py bdist_wheel\"\n</code></pre> <p>or the current time:</p> <pre><code>nix-shell --run \"SOURCE_DATE_EPOCH=$(date +%s) python3 setup.py bdist_wheel\"\n</code></pre> <p>or unset <code>SOURCE_DATE_EPOCH</code>:</p> <pre><code>nix-shell --run \"unset SOURCE_DATE_EPOCH; python3 setup.py bdist_wheel\"\n</code></pre>"},{"location":"languages-frameworks/python.section.html#install_data-data_files-problems","title":"<code>install_data</code> / <code>data_files</code> problems","text":"<p>If you get the following error:</p> <pre><code>could not create '/nix/store/6l1bvljpy8gazlsw2aw9skwwp4pmvyxw-python-2.7.8/etc':\nPermission denied\n</code></pre> <p>This is a known bug in <code>setuptools</code>. Setuptools <code>install_data</code> does not respect <code>--prefix</code>. An example of such package using the feature is <code>pkgs/tools/X11/xpra/default.nix</code>.</p> <p>As workaround install it as an extra <code>preInstall</code> step:</p> <pre><code>${python.pythonOnBuildForHost.interpreter} setup.py install_data --install-dir=$out --root=$out\nsed -i '/ = data\\_files/d' setup.py\n</code></pre>"},{"location":"languages-frameworks/python.section.html#rationale-of-non-existent-global-site-packages","title":"Rationale of non-existent global site-packages","text":"<p>On most operating systems a global <code>site-packages</code> is maintained. This however becomes problematic if you want to run multiple Python versions or have multiple versions of certain libraries for your projects. Generally, you would solve such issues by creating virtual environments using <code>virtualenv</code>.</p> <p>On Nix each package has an isolated dependency tree which, in the case of Python, guarantees the right versions of the interpreter and libraries or packages are available. There is therefore no need to maintain a global <code>site-packages</code>.</p> <p>If you want to create a Python environment for development, then the recommended method is to use <code>nix-shell</code>, either with or without the <code>python.buildEnv</code> function.</p>"},{"location":"languages-frameworks/python.section.html#how-to-consume-python-modules-using-pip-in-a-virtual-environment-like-i-am-used-to-on-other-operating-systems","title":"How to consume Python modules using pip in a virtual environment like I am used to on other Operating Systems?","text":"<p>While this approach is not very idiomatic from Nix perspective, it can still be useful when dealing with pre-existing projects or in situations where it's not feasible or desired to write derivations for all required dependencies.</p> <p>This is an example of a <code>default.nix</code> for a <code>nix-shell</code>, which allows to consume a virtual environment created by <code>venv</code>, and install Python modules through <code>pip</code> the traditional way.</p> <p>Create this <code>default.nix</code> file, together with a <code>requirements.txt</code> and execute <code>nix-shell</code>.</p> <pre><code>with import &lt;nixpkgs&gt; { };\n\nlet\n  pythonPackages = python3Packages;\nin pkgs.mkShell rec {\n  name = \"impurePythonEnv\";\n  venvDir = \"./.venv\";\n  buildInputs = [\n    # A Python interpreter including the 'venv' module is required to bootstrap\n    # the environment.\n    pythonPackages.python\n\n    # This executes some shell code to initialize a venv in $venvDir before\n    # dropping into the shell\n    pythonPackages.venvShellHook\n\n    # Those are dependencies that we would like to use from nixpkgs, which will\n    # add them to PYTHONPATH and thus make them accessible from within the venv.\n    pythonPackages.numpy\n    pythonPackages.requests\n\n    # In this particular example, in order to compile any binary extensions they may\n    # require, the Python modules listed in the hypothetical requirements.txt need\n    # the following packages to be installed locally:\n    taglib\n    openssl\n    git\n    libxml2\n    libxslt\n    libzip\n    zlib\n  ];\n\n  # Run this command, only after creating the virtual environment\n  postVenvCreation = ''\n    unset SOURCE_DATE_EPOCH\n    pip install -r requirements.txt\n  '';\n\n  # Now we can execute any commands within the virtual environment.\n  # This is optional and can be left out to run pip manually.\n  postShellHook = ''\n    # allow pip to install wheels\n    unset SOURCE_DATE_EPOCH\n  '';\n\n}\n</code></pre> <p>In case the supplied venvShellHook is insufficient, or when Python 2 support is needed, you can define your own shell hook and adapt to your needs like in the following example:</p> <pre><code>with import &lt;nixpkgs&gt; { };\n\nlet\n  venvDir = \"./.venv\";\n  pythonPackages = python3Packages;\nin pkgs.mkShell rec {\n  name = \"impurePythonEnv\";\n  buildInputs = [\n    pythonPackages.python\n    # Needed when using python 2.7\n    # pythonPackages.virtualenv\n    # ...\n  ];\n\n  # This is very close to how venvShellHook is implemented, but\n  # adapted to use 'virtualenv'\n  shellHook = ''\n    SOURCE_DATE_EPOCH=$(date +%s)\n\n    if [ -d \"${venvDir}\" ]; then\n      echo \"Skipping venv creation, '${venvDir}' already exists\"\n    else\n      echo \"Creating new venv environment in path: '${venvDir}'\"\n      # Note that the module venv was only introduced in python 3, so for 2.7\n      # this needs to be replaced with a call to virtualenv\n      ${pythonPackages.python.interpreter} -m venv \"${venvDir}\"\n    fi\n\n    # Under some circumstances it might be necessary to add your virtual\n    # environment to PYTHONPATH, which you can do here too;\n    # PYTHONPATH=$PWD/${venvDir}/${pythonPackages.python.sitePackages}/:$PYTHONPATH\n\n    source \"${venvDir}/bin/activate\"\n\n    # As in the previous example, this is optional.\n    pip install -r requirements.txt\n  '';\n}\n</code></pre> <p>Note that the <code>pip install</code> is an imperative action. So every time <code>nix-shell</code> is executed it will attempt to download the Python modules listed in requirements.txt. However these will be cached locally within the <code>virtualenv</code> folder and not downloaded again.</p>"},{"location":"languages-frameworks/python.section.html#how-to-override-a-python-package-from-configuration.nix","title":"How to override a Python package from <code>configuration.nix</code>?","text":"<p>If you need to change a package's attribute(s) from <code>configuration.nix</code> you could do:</p> <pre><code>  nixpkgs.config.packageOverrides = super: {\n    python3 = super.python3.override {\n      packageOverrides = python-self: python-super: {\n        twisted = python-super.twisted.overridePythonAttrs (oldAttrs: {\n          src = super.fetchPypi {\n            pname = \"Twisted\";\n            version = \"19.10.0\";\n            hash = \"sha256-c5S6fycq5yKnTz2Wnc9Zm8TvCTvDkgOHSKSQ8XJKUV0=\";\n            extension = \"tar.bz2\";\n          };\n        });\n      };\n    };\n  };\n</code></pre> <p><code>python3Packages.twisted</code> is now globally overridden. All packages and also all NixOS services that reference <code>twisted</code> (such as <code>services.buildbot-worker</code>) now use the new definition. Note that <code>python-super</code> refers to the old package set and <code>python-self</code> to the new, overridden version.</p> <p>To modify only a Python package set instead of a whole Python derivation, use this snippet:</p> <pre><code>  myPythonPackages = python3Packages.override {\n    overrides = self: super: {\n      twisted = ...;\n    };\n  }\n</code></pre>"},{"location":"languages-frameworks/python.section.html#how-to-override-a-python-package-using-overlays","title":"How to override a Python package using overlays?","text":"<p>Use the following overlay template:</p> <pre><code>self: super: {\n  python = super.python.override {\n    packageOverrides = python-self: python-super: {\n      twisted = python-super.twisted.overrideAttrs (oldAttrs: {\n        src = super.fetchPypi {\n          pname = \"Twisted\";\n          version = \"19.10.0\";\n          hash = \"sha256-c5S6fycq5yKnTz2Wnc9Zm8TvCTvDkgOHSKSQ8XJKUV0=\";\n          extension = \"tar.bz2\";\n        };\n      });\n    };\n  };\n}\n</code></pre>"},{"location":"languages-frameworks/python.section.html#how-to-override-a-python-package-for-all-python-versions-using-extensions","title":"How to override a Python package for all Python versions using extensions?","text":"<p>The following overlay overrides the call to <code>buildPythonPackage</code> for the <code>foo</code> package for all interpreters by appending a Python extension to the <code>pythonPackagesExtensions</code> list of extensions.</p> <pre><code>final: prev: {\n  pythonPackagesExtensions = prev.pythonPackagesExtensions ++ [\n    (\n      python-final: python-prev: {\n        foo = python-prev.foo.overridePythonAttrs (oldAttrs: {\n          ...\n        });\n      }\n    )\n  ];\n}\n</code></pre>"},{"location":"languages-frameworks/python.section.html#how-to-use-intels-mkl-with-numpy-and-scipy","title":"How to use Intel\u2019s MKL with numpy and scipy?","text":"<p>MKL can be configured using an overlay. See the section \"Using overlays to configure alternatives\".</p>"},{"location":"languages-frameworks/python.section.html#what-inputs-do-setup_requires-install_requires-and-tests_require-map-to","title":"What inputs do <code>setup_requires</code>, <code>install_requires</code> and <code>tests_require</code> map to?","text":"<p>In a <code>setup.py</code> or <code>setup.cfg</code> it is common to declare dependencies:</p> <ul> <li><code>setup_requires</code> corresponds to <code>nativeBuildInputs</code></li> <li><code>install_requires</code> corresponds to <code>propagatedBuildInputs</code></li> <li><code>tests_require</code> corresponds to <code>nativeCheckInputs</code></li> </ul>"},{"location":"languages-frameworks/python.section.html#optimizations","title":"How to enable interpreter optimizations?","text":"<p>The Python interpreters are by default not built with optimizations enabled, because the builds are in that case not reproducible. To enable optimizations, override the interpreter of interest, e.g using</p> <pre><code>let\n  pkgs = import ./. {};\n  mypython = pkgs.python3.override {\n    enableOptimizations = true;\n    reproducibleBuild = false;\n    self = mypython;\n  };\nin mypython\n</code></pre>"},{"location":"languages-frameworks/python.section.html#python-optional-dependencies","title":"How to add optional dependencies?","text":"<p>Some packages define optional dependencies for additional features. With <code>setuptools</code> this is called <code>extras_require</code> and <code>flit</code> calls it <code>extras-require</code>, while PEP 621 calls these <code>optional-dependencies</code>. A method for supporting this is by declaring the extras of a package in its <code>passthru</code>, e.g. in case of the package <code>dask</code></p> <pre><code>passthru.optional-dependencies = {\n  complete = [ distributed ];\n};\n</code></pre> <p>and letting the package requiring the extra add the list to its dependencies</p> <pre><code>propagatedBuildInputs = [\n  ...\n] ++ dask.optional-dependencies.complete;\n</code></pre> <p>Note this method is preferred over adding parameters to builders, as that can result in packages depending on different variants and thereby causing collisions.</p>"},{"location":"languages-frameworks/python.section.html#tools","title":"How to contribute a Python package to nixpkgs?","text":"<p>Packages inside nixpkgs must use the <code>buildPythonPackage</code> or <code>buildPythonApplication</code> function directly, because we can only provide security support for non-vendored dependencies.</p> <p>We recommend nix-init for creating new python packages within nixpkgs, as it already prefetches the source, parses dependencies for common formats and prefills most things in <code>meta</code>.</p>"},{"location":"languages-frameworks/python.section.html#deterministic-builds","title":"Are Python interpreters built deterministically?","text":"<p>The Python interpreters are now built deterministically. Minor modifications had to be made to the interpreters in order to generate deterministic bytecode. This has security implications and is relevant for those using Python in a <code>nix-shell</code>.</p> <p>When the environment variable <code>DETERMINISTIC_BUILD</code> is set, all bytecode will have timestamp 1. The <code>buildPythonPackage</code> function sets <code>DETERMINISTIC_BUILD=1</code> and PYTHONHASHSEED=0. Both are also exported in <code>nix-shell</code>.</p>"},{"location":"languages-frameworks/python.section.html#automatic-tests","title":"How to provide automatic tests to Python packages?","text":"<p>It is recommended to test packages as part of the build process. Source distributions (<code>sdist</code>) often include test files, but not always.</p> <p>By default the command <code>python setup.py test</code> is run as part of the <code>checkPhase</code>, but often it is necessary to pass a custom <code>checkPhase</code>. An example of such a situation is when <code>py.test</code> is used.</p>"},{"location":"languages-frameworks/python.section.html#common-issues","title":"Common issues","text":"<ul> <li>Non-working tests can often be deselected. By default <code>buildPythonPackage</code>   runs <code>python setup.py test</code>. which is deprecated. Most Python modules however   do follow the standard test protocol where the pytest runner can be used   instead. <code>pytest</code> supports the <code>-k</code> and <code>--ignore</code> parameters to ignore test   methods or classes as well as whole files. For <code>pytestCheckHook</code> these are   conveniently exposed as <code>disabledTests</code> and <code>disabledTestPaths</code> respectively.</li> </ul> <pre><code>buildPythonPackage {\n  # ...\n  nativeCheckInputs = [\n    pytestCheckHook\n  ];\n\n  disabledTests = [\n    \"function_name\"\n    \"other_function\"\n  ];\n\n  disabledTestPaths = [\n    \"this/file.py\"\n  ];\n}\n</code></pre> <ul> <li>Tests that attempt to access <code>$HOME</code> can be fixed by using the following   work-around before running tests (e.g. <code>preCheck</code>): <code>export HOME=$(mktemp -d)</code></li> </ul>"},{"location":"languages-frameworks/python.section.html#contributing","title":"Contributing","text":""},{"location":"languages-frameworks/python.section.html#contributing-guidelines","title":"Contributing guidelines","text":"<p>The following rules are desired to be respected:</p> <ul> <li>Python libraries are called from <code>python-packages.nix</code> and packaged with   <code>buildPythonPackage</code>. The expression of a library should be in   <code>pkgs/development/python-modules/&lt;name&gt;/default.nix</code>.</li> <li>Python applications live outside of <code>python-packages.nix</code> and are packaged   with <code>buildPythonApplication</code>.</li> <li>Make sure libraries build for all Python interpreters.</li> <li>By default we enable tests. Make sure the tests are found and, in the case of   libraries, are passing for all interpreters. If certain tests fail they can be   disabled individually. Try to avoid disabling the tests altogether. In any   case, when you disable tests, leave a comment explaining why.</li> <li>Commit names of Python libraries should reflect that they are Python   libraries, so write for example <code>python311Packages.numpy: 1.11 -&gt; 1.12</code>.   It is highly recommended to specify the current default version to enable   automatic build by ofborg.</li> <li>Attribute names in <code>python-packages.nix</code> as well as <code>pname</code>s should match the   library's name on PyPI, but be normalized according to PEP   0503. This means   that characters should be converted to lowercase and <code>.</code> and <code>_</code> should be   replaced by a single <code>-</code> (foo-bar-baz instead of Foo__Bar.baz).   If necessary, <code>pname</code> has to be given a different value within <code>fetchPypi</code>.</li> <li>Packages from sources such as GitHub and GitLab that do not exist on PyPI   should not use a name that is already used on PyPI. When possible, they should   use the package repository name prefixed with the owner (e.g. organization) name   and using a <code>-</code> as delimiter.</li> <li>Attribute names in <code>python-packages.nix</code> should be sorted alphanumerically to   avoid merge conflicts and ease locating attributes.</li> </ul>"},{"location":"languages-frameworks/python.section.html#python-package-set-maintenance","title":"Package set maintenance","text":"<p>The whole Python package set has a lot of packages that do not see regular updates, because they either are a very fragile component in the Python ecosystem, like for example the <code>hypothesis</code> package, or packages that have no maintainer, so maintenance falls back to the package set maintainers.</p>"},{"location":"languages-frameworks/python.section.html#python-package-bulk-updates","title":"Updating packages in bulk","text":"<p>There is a tool to update alot of python libraries in bulk, it exists at <code>maintainers/scripts/update-python-libraries</code> with this repository.</p> <p>It can quickly update minor or major versions for all packages selected and create update commits, and supports the <code>fetchPypi</code>, <code>fetchurl</code> and <code>fetchFromGitHub</code> fetchers. When updating lots of packages that are hosted on GitHub, exporting a <code>GITHUB_API_TOKEN</code> is highly recommended.</p> <p>Updating packages in bulk leads to lots of breakages, which is why a stabilization period on the <code>python-updates</code> branch is required.</p> <p>If a package is fragile and often breaks during these bulks updates, it may be reasonable to set <code>passthru.skipBulkUpdate = true</code> in the derivation. This decision should not be made on a whim and should always be supported by a qualifying comment.</p> <p>Once the branch is sufficiently stable it should normally be merged into the <code>staging</code> branch.</p> <p>An exemplary call to update all python libraries between minor versions would be:</p> <pre><code>$ maintainers/scripts/update-python-libraries --target minor --commit --use-pkgs-prefix pkgs/development/python-modules/**/default.nix\n</code></pre>"},{"location":"languages-frameworks/python.section.html#python-cpython-update-schedule","title":"CPython Update Schedule","text":"<p>With PEP 602, CPython now follows a yearly release cadence. In nixpkgs, all supported interpreters are made available, but only the most recent two interpreters package sets are built; this is a compromise between being the latest interpreter, and what the majority of the Python packages support.</p> <p>New CPython interpreters are released in October. Generally, it takes some time for the majority of active Python projects to support the latest stable interpreter. To help ease the migration for Nixpkgs users between Python interpreters the schedule below will be used:</p> When Event After YY.11 Release Bump CPython package set window. The latest and previous latest stable should now be built. After YY.05 Release Bump default CPython interpreter to latest stable. <p>In practice, this means that the Python community will have had a stable interpreter for ~2 months before attempting to update the package set. And this will allow for ~7 months for Python applications to support the latest interpreter.</p>"},{"location":"languages-frameworks/qt.section.html","title":"Qt","text":"<p>Writing Nix expressions for Qt libraries and applications is largely similar as for other C++ software. This section assumes some knowledge of the latter.</p> <p>The major caveat with Qt applications is that Qt uses a plugin system to load additional modules at runtime, from a list of well-known locations. In Nixpkgs, we patch QtCore to instead use an environment variable, and wrap Qt applications to set it to the right paths. This effectively makes the runtime dependencies pure and explicit at build-time, at the cost of introducing an extra indirection.</p>"},{"location":"languages-frameworks/qt.section.html#qt-default-nix","title":"Nix expression for a Qt package (default.nix)","text":"<pre><code>{ stdenv, lib, qtbase, wrapQtAppsHook }:\n\nstdenv.mkDerivation {\n  pname = \"myapp\";\n  version = \"1.0\";\n\n  buildInputs = [ qtbase ];\n  nativeBuildInputs = [ wrapQtAppsHook ];\n}\n</code></pre> <p>It is important to import Qt modules directly, that is: <code>qtbase</code>, <code>qtdeclarative</code>, etc. Do not import Qt package sets such as <code>qt5</code> because the Qt versions of dependencies may not be coherent, causing build and runtime failures.</p> <p>Additionally all Qt packages must include <code>wrapQtAppsHook</code> in <code>nativeBuildInputs</code>, or you must explicitly set <code>dontWrapQtApps</code>.</p> <p><code>pkgs.callPackage</code> does not provide injections for <code>qtbase</code> or the like. Instead you want to either use <code>pkgs.libsForQt5.callPackage</code>, or <code>pkgs.qt6Packages.callPackage</code>, depending on the Qt version you want to use.</p> <p>For example (from here)</p> <pre><code>  zeal-qt5 = libsForQt5.callPackage ../data/documentation/zeal { };\n  zeal-qt6 = qt6Packages.callPackage ../data/documentation/zeal { };\n  zeal = zeal-qt5;\n</code></pre>"},{"location":"languages-frameworks/qt.section.html#qt-runtime-dependencies","title":"Locating runtime dependencies","text":"<p>Qt applications must be wrapped to find runtime dependencies. Include <code>wrapQtAppsHook</code> in <code>nativeBuildInputs</code>:</p> <pre><code>{ stdenv, wrapQtAppsHook }:\n\nstdenv.mkDerivation {\n  # ...\n  nativeBuildInputs = [ wrapQtAppsHook ];\n}\n</code></pre> <p>Add entries to <code>qtWrapperArgs</code> are to modify the wrappers created by <code>wrapQtAppsHook</code>:</p> <pre><code>{ stdenv, wrapQtAppsHook }:\n\nstdenv.mkDerivation {\n  # ...\n  nativeBuildInputs = [ wrapQtAppsHook ];\n  qtWrapperArgs = [ ''--prefix PATH : /path/to/bin'' ];\n}\n</code></pre> <p>The entries are passed as arguments to wrapProgram.</p> <p>Set <code>dontWrapQtApps</code> to stop applications from being wrapped automatically. Wrap programs manually with <code>wrapQtApp</code>, using the syntax of wrapProgram:</p> <pre><code>{ stdenv, lib, wrapQtAppsHook }:\n\nstdenv.mkDerivation {\n  # ...\n  nativeBuildInputs = [ wrapQtAppsHook ];\n  dontWrapQtApps = true;\n  preFixup = ''\n      wrapQtApp \"$out/bin/myapp\" --prefix PATH : /path/to/bin\n  '';\n}\n</code></pre> <p>::: {.note} <code>wrapQtAppsHook</code> ignores files that are non-ELF executables. This means that scripts won't be automatically wrapped so you'll need to manually wrap them as previously mentioned. An example of when you'd always need to do this is with Python applications that use PyQt. :::</p>"},{"location":"languages-frameworks/r.section.html","title":"R","text":""},{"location":"languages-frameworks/r.section.html#installation","title":"Installation","text":"<p>Define an environment for R that contains all the libraries that you'd like to use by adding the following snippet to your $HOME/.config/nixpkgs/config.nix file:</p> <pre><code>{\n    packageOverrides = super: let self = super.pkgs; in\n    {\n\n        rEnv = super.rWrapper.override {\n            packages = with self.rPackages; [\n                devtools\n                ggplot2\n                reshape2\n                yaml\n                optparse\n                ];\n        };\n    };\n}\n</code></pre> <p>Then you can use <code>nix-env -f \"&lt;nixpkgs&gt;\" -iA rEnv</code> to install it into your user profile. The set of available libraries can be discovered by running the command <code>nix-env -f \"&lt;nixpkgs&gt;\" -qaP -A rPackages</code>. The first column from that output is the name that has to be passed to rWrapper in the code snipped above.</p> <p>However, if you'd like to add a file to your project source to make the environment available for other contributors, you can create a <code>default.nix</code> file like so:</p> <p><pre><code>with import &lt;nixpkgs&gt; {};\n{\n  myProject = stdenv.mkDerivation {\n    name = \"myProject\";\n    version = \"1\";\n    src = if lib.inNixShell then null else nix;\n\n    buildInputs = with rPackages; [\n      R\n      ggplot2\n      knitr\n    ];\n  };\n}\n</code></pre> and then run <code>nix-shell .</code> to be dropped into a shell with those packages available.</p>"},{"location":"languages-frameworks/r.section.html#rstudio","title":"RStudio","text":"<p>RStudio uses a standard set of packages and ignores any custom R environments or installed packages you may have.  To create a custom environment, see <code>rstudioWrapper</code>, which functions similarly to <code>rWrapper</code>:</p> <pre><code>{\n    packageOverrides = super: let self = super.pkgs; in\n    {\n\n        rstudioEnv = super.rstudioWrapper.override {\n            packages = with self.rPackages; [\n                dplyr\n                ggplot2\n                reshape2\n                ];\n        };\n    };\n}\n</code></pre> <p>Then like above, <code>nix-env -f \"&lt;nixpkgs&gt;\" -iA rstudioEnv</code> will install this into your user profile.</p> <p>Alternatively, you can create a self-contained <code>shell.nix</code> without the need to modify any configuration files:</p> <pre><code>{ pkgs ? import &lt;nixpkgs&gt; {}\n}:\n\npkgs.rstudioWrapper.override {\n  packages = with pkgs.rPackages; [ dplyr ggplot2 reshape2 ];\n}\n</code></pre> <p>Executing <code>nix-shell</code> will then drop you into an environment equivalent to the one above. If you need additional packages just add them to the list and re-enter the shell.</p>"},{"location":"languages-frameworks/r.section.html#updating-the-package-set","title":"Updating the package set","text":"<p>There is a script and associated environment for regenerating the package sets and synchronising the rPackages tree to the current CRAN and matching BIOC release. These scripts are found in the <code>pkgs/development/r-modules</code> directory and executed as follows:</p> <pre><code>nix-shell generate-shell.nix\n\nRscript generate-r-packages.R cran  &gt; cran-packages.nix.new\nmv cran-packages.nix.new cran-packages.nix\n\nRscript generate-r-packages.R bioc  &gt; bioc-packages.nix.new\nmv bioc-packages.nix.new bioc-packages.nix\n\nRscript generate-r-packages.R bioc-annotation &gt; bioc-annotation-packages.nix.new\nmv bioc-annotation-packages.nix.new bioc-annotation-packages.nix\n\nRscript generate-r-packages.R bioc-experiment &gt; bioc-experiment-packages.nix.new\nmv bioc-experiment-packages.nix.new bioc-experiment-packages.nix\n</code></pre> <p><code>generate-r-packages.R &lt;repo&gt;</code> reads  <code>&lt;repo&gt;-packages.nix</code>, therefore the renaming.</p> <p>Some packages require overrides to specify external dependencies or other patches and special requirements. These overrides are specified in the <code>pkgs/development/r-modules/default.nix</code> file. As the <code>*-packages.nix</code> contents are automatically generated it should not be edited and broken builds should be addressed using overrides.</p>"},{"location":"languages-frameworks/ruby.section.html","title":"Ruby","text":""},{"location":"languages-frameworks/ruby.section.html#using-ruby","title":"Using Ruby","text":"<p>Several versions of Ruby interpreters are available on Nix, as well as over 250 gems and many applications written in Ruby. The attribute <code>ruby</code> refers to the default Ruby interpreter, which is currently MRI 3.1. It's also possible to refer to specific versions, e.g. <code>ruby_3_y</code>, <code>jruby</code>, or <code>mruby</code>.</p> <p>In the Nixpkgs tree, Ruby packages can be found throughout, depending on what they do, and are called from the main package set. Ruby gems, however are separate sets, and there's one default set for each interpreter (currently MRI only).</p> <p>There are two main approaches for using Ruby with gems. One is to use a specifically locked <code>Gemfile</code> for an application that has very strict dependencies. The other is to depend on the common gems, which we'll explain further down, and rely on them being updated regularly.</p> <p>The interpreters have common attributes, namely <code>gems</code>, and <code>withPackages</code>. So you can refer to <code>ruby.gems.nokogiri</code>, or <code>ruby_3_2.gems.nokogiri</code> to get the Nokogiri gem already compiled and ready to use.</p> <p>Since not all gems have executables like <code>nokogiri</code>, it's usually more convenient to use the <code>withPackages</code> function like this: <code>ruby.withPackages (p: with p; [ nokogiri ])</code>. This will also make sure that the Ruby in your environment will be able to find the gem and it can be used in your Ruby code (for example via <code>ruby</code> or <code>irb</code> executables) via <code>require \"nokogiri\"</code> as usual.</p>"},{"location":"languages-frameworks/ruby.section.html#temporary-ruby-environment-with-nix-shell","title":"Temporary Ruby environment with <code>nix-shell</code>","text":"<p>Rather than having a single Ruby environment shared by all Ruby development projects on a system, Nix allows you to create separate environments per project. <code>nix-shell</code> gives you the possibility to temporarily load another environment akin to a combined <code>chruby</code> or <code>rvm</code> and <code>bundle exec</code>.</p> <p>There are two methods for loading a shell with Ruby packages. The first and recommended method is to create an environment with <code>ruby.withPackages</code> and load that.</p> <pre><code>$ nix-shell -p \"ruby.withPackages (ps: with ps; [ nokogiri pry ])\"\n</code></pre> <p>The other method, which is not recommended, is to create an environment and list all the packages directly.</p> <pre><code>$ nix-shell -p ruby.gems.nokogiri ruby.gems.pry\n</code></pre> <p>Again, it's possible to launch the interpreter from the shell. The Ruby interpreter has the attribute <code>gems</code> which contains all Ruby gems for that specific interpreter.</p>"},{"location":"languages-frameworks/ruby.section.html#load-ruby-environment-from-.nix-expression","title":"Load Ruby environment from <code>.nix</code> expression","text":"<p>As explained in the <code>nix-shell</code> section of the Nix manual, <code>nix-shell</code> can also load an expression from a <code>.nix</code> file. Say we want to have Ruby, <code>nokogori</code>, and <code>pry</code>. Consider a <code>shell.nix</code> file with:</p> <pre><code>with import &lt;nixpkgs&gt; {};\nruby.withPackages (ps: with ps; [ nokogiri pry ])\n</code></pre> <p>What's happening here?</p> <ol> <li>We begin with importing the Nix Packages collections. <code>import &lt;nixpkgs&gt;</code> imports the <code>&lt;nixpkgs&gt;</code> function, <code>{}</code> calls it and the <code>with</code> statement brings all attributes of <code>nixpkgs</code> in the local scope. These attributes form the main package set.</li> <li>Then we create a Ruby environment with the <code>withPackages</code> function.</li> <li>The <code>withPackages</code> function expects us to provide a function as an argument that takes the set of all ruby gems and returns a list of packages to include in the environment. Here, we select the packages <code>nokogiri</code> and <code>pry</code> from the package set.</li> </ol>"},{"location":"languages-frameworks/ruby.section.html#execute-command-with---run","title":"Execute command with <code>--run</code>","text":"<p>A convenient flag for <code>nix-shell</code> is <code>--run</code>. It executes a command in the <code>nix-shell</code>. We can e.g. directly open a <code>pry</code> REPL:</p> <pre><code>$ nix-shell -p \"ruby.withPackages (ps: with ps; [ nokogiri pry ])\" --run \"pry\"\n</code></pre> <p>Or immediately require <code>nokogiri</code> in pry:</p> <pre><code>$ nix-shell -p \"ruby.withPackages (ps: with ps; [ nokogiri pry ])\" --run \"pry -rnokogiri\"\n</code></pre> <p>Or run a script using this environment:</p> <pre><code>$ nix-shell -p \"ruby.withPackages (ps: with ps; [ nokogiri pry ])\" --run \"ruby example.rb\"\n</code></pre>"},{"location":"languages-frameworks/ruby.section.html#using-nix-shell-as-shebang","title":"Using <code>nix-shell</code> as shebang","text":"<p>In fact, for the last case, there is a more convenient method. You can add a shebang to your script specifying which dependencies <code>nix-shell</code> needs. With the following shebang, you can just execute <code>./example.rb</code>, and it will run with all dependencies.</p> <pre><code>#! /usr/bin/env nix-shell\n#! nix-shell -i ruby -p \"ruby.withPackages (ps: with ps; [ nokogiri rest-client ])\"\n\nrequire 'nokogiri'\nrequire 'rest-client'\n\nbody = RestClient.get('http://example.com').body\nputs Nokogiri::HTML(body).at('h1').text\n</code></pre>"},{"location":"languages-frameworks/ruby.section.html#developing-with-ruby","title":"Developing with Ruby","text":""},{"location":"languages-frameworks/ruby.section.html#using-an-existing-gemfile","title":"Using an existing Gemfile","text":"<p>In most cases, you'll already have a <code>Gemfile.lock</code> listing all your dependencies. This can be used to generate a <code>gemset.nix</code> which is used to fetch the gems and combine them into a single environment. The reason why you need to have a separate file for this, is that Nix requires you to have a checksum for each input to your build. Since the <code>Gemfile.lock</code> that <code>bundler</code> generates doesn't provide us with checksums, we have to first download each gem, calculate its SHA256, and store it in this separate file.</p> <p>So the steps from having just a <code>Gemfile</code> to a <code>gemset.nix</code> are:</p> <pre><code>$ bundle lock\n$ bundix\n</code></pre> <p>If you already have a <code>Gemfile.lock</code>, you can run <code>bundix</code> and it will work the same.</p> <p>To update the gems in your <code>Gemfile.lock</code>, you may use the <code>bundix -l</code> flag, which will create a new <code>Gemfile.lock</code> in case the <code>Gemfile</code> has a more recent time of modification.</p> <p>Once the <code>gemset.nix</code> is generated, it can be used in a <code>bundlerEnv</code> derivation. Here is an example you could use for your <code>shell.nix</code>:</p> <pre><code># ...\nlet\n  gems = bundlerEnv {\n    name = \"gems-for-some-project\";\n    gemdir = ./.;\n  };\nin mkShell { packages = [ gems gems.wrappedRuby ]; }\n</code></pre> <p>With this file in your directory, you can run <code>nix-shell</code> to build and use the gems. The important parts here are <code>bundlerEnv</code> and <code>wrappedRuby</code>.</p> <p>The <code>bundlerEnv</code> is a wrapper over all the gems in your gemset. This means that all the <code>/lib</code> and <code>/bin</code> directories will be available, and the executables of all gems (even of indirect dependencies) will end up in your <code>$PATH</code>. The <code>wrappedRuby</code> provides you with all executables that come with Ruby itself, but wrapped so they can easily find the gems in your gemset.</p> <p>One common issue that you might have is that you have Ruby, but also <code>bundler</code> in your gemset. That leads to a conflict for <code>/bin/bundle</code> and <code>/bin/bundler</code>. You can resolve this by wrapping either your Ruby or your gems in a <code>lowPrio</code> call. So in order to give the <code>bundler</code> from your gemset priority, it would be used like this:</p> <pre><code># ...\nmkShell { buildInputs = [ gems (lowPrio gems.wrappedRuby) ]; }\n</code></pre> <p>Sometimes a Gemfile references other files. Such as <code>.ruby-version</code> or vendored gems. When copying the Gemfile to the nix store we need to copy those files alongside. This can be done using <code>extraConfigPaths</code>. For example:</p> <pre><code>  gems = bundlerEnv {\n    name = \"gems-for-some-project\";\n    gemdir = ./.;\n    extraConfigPaths = [ \"${./.}/.ruby-version\" ];\n  };\n</code></pre>"},{"location":"languages-frameworks/ruby.section.html#gem-specific-configurations-and-workarounds","title":"Gem-specific configurations and workarounds","text":"<p>In some cases, especially if the gem has native extensions, you might need to modify the way the gem is built.</p> <p>This is done via a common configuration file that includes all of the workarounds for each gem.</p> <p>This file lives at <code>/pkgs/development/ruby-modules/gem-config/default.nix</code>, since it already contains a lot of entries, it should be pretty easy to add the modifications you need for your needs.</p> <p>In the meanwhile, or if the modification is for a private gem, you can also add the configuration to only your own environment.</p> <p>Two places that allow this modification are the <code>ruby</code> derivation, or <code>bundlerEnv</code>.</p> <p>Here's the <code>ruby</code> one:</p> <pre><code>{ pg_version ? \"10\", pkgs ? import &lt;nixpkgs&gt; { } }:\nlet\n  myRuby = pkgs.ruby.override {\n    defaultGemConfig = pkgs.defaultGemConfig // {\n      pg = attrs: {\n        buildFlags =\n        [ \"--with-pg-config=${pkgs.\"postgresql_${pg_version}\"}/bin/pg_config\" ];\n      };\n    };\n  };\nin myRuby.withPackages (ps: with ps; [ pg ])\n</code></pre> <p>And an example with <code>bundlerEnv</code>:</p> <pre><code>{ pg_version ? \"10\", pkgs ? import &lt;nixpkgs&gt; { } }:\nlet\n  gems = pkgs.bundlerEnv {\n    name = \"gems-for-some-project\";\n    gemdir = ./.;\n    gemConfig = pkgs.defaultGemConfig // {\n      pg = attrs: {\n        buildFlags =\n        [ \"--with-pg-config=${pkgs.\"postgresql_${pg_version}\"}/bin/pg_config\" ];\n      };\n    };\n  };\nin mkShell { buildInputs = [ gems gems.wrappedRuby ]; }\n</code></pre> <p>And finally via overlays:</p> <pre><code>{ pg_version ? \"10\" }:\nlet\n  pkgs = import &lt;nixpkgs&gt; {\n    overlays = [\n      (self: super: {\n        defaultGemConfig = super.defaultGemConfig // {\n          pg = attrs: {\n            buildFlags = [\n              \"--with-pg-config=${\n                pkgs.\"postgresql_${pg_version}\"\n              }/bin/pg_config\"\n            ];\n          };\n        };\n      })\n    ];\n  };\nin pkgs.ruby.withPackages (ps: with ps; [ pg ])\n</code></pre> <p>Then we can get whichever postgresql version we desire and the <code>pg</code> gem will always reference it correctly:</p> <pre><code>$ nix-shell --argstr pg_version 9_4 --run 'ruby -rpg -e \"puts PG.library_version\"'\n90421\n\n$ nix-shell --run 'ruby -rpg -e \"puts PG.library_version\"'\n100007\n</code></pre> <p>Of course for this use-case one could also use overlays since the configuration for <code>pg</code> depends on the <code>postgresql</code> alias, but for demonstration purposes this has to suffice.</p>"},{"location":"languages-frameworks/ruby.section.html#ruby-platform-specif-gems","title":"Platform-specific gems","text":"<p>Right now, bundix has some issues with pre-built, platform-specific gems: bundix PR #68. Until this is solved, you can tell bundler to not use platform-specific gems and instead build them from source each time: - globally (will be set in <code>~/.config/.bundle/config</code>): <pre><code>$ bundle config set force_ruby_platform true\n</code></pre> - locally (will be set in <code>&lt;project-root&gt;/.bundle/config</code>): <pre><code>$ bundle config set --local force_ruby_platform true\n</code></pre></p>"},{"location":"languages-frameworks/ruby.section.html#adding-a-gem-to-the-default-gemset","title":"Adding a gem to the default gemset","text":"<p>Now that you know how to get a working Ruby environment with Nix, it's time to go forward and start actually developing with Ruby. We will first have a look at how Ruby gems are packaged on Nix. Then, we will look at how you can use development mode with your code.</p> <p>All gems in the standard set are automatically generated from a single <code>Gemfile</code>. The dependency resolution is done with <code>bundler</code> and makes it more likely that all gems are compatible to each other.</p> <p>In order to add a new gem to nixpkgs, you can put it into the <code>/pkgs/development/ruby-modules/with-packages/Gemfile</code> and run <code>./maintainers/scripts/update-ruby-packages</code>.</p> <p>To test that it works, you can then try using the gem with:</p> <pre><code>NIX_PATH=nixpkgs=$PWD nix-shell -p \"ruby.withPackages (ps: with ps; [ name-of-your-gem ])\"\n</code></pre>"},{"location":"languages-frameworks/ruby.section.html#packaging-applications","title":"Packaging applications","text":"<p>A common task is to add a ruby executable to nixpkgs, popular examples would be <code>chef</code>, <code>jekyll</code>, or <code>sass</code>. A good way to do that is to use the <code>bundlerApp</code> function, that allows you to make a package that only exposes the listed executables, otherwise the package may cause conflicts through common paths like <code>bin/rake</code> or <code>bin/bundler</code> that aren't meant to be used.</p> <p>The absolute easiest way to do that is to write a <code>Gemfile</code> along these lines:</p> <pre><code>source 'https://rubygems.org' do\n  gem 'mdl'\nend\n</code></pre> <p>If you want to package a specific version, you can use the standard Gemfile syntax for that, e.g. <code>gem 'mdl', '0.5.0'</code>, but if you want the latest stable version anyway, it's easier to update by running the <code>bundle lock</code> and <code>bundix</code> steps again.</p> <p>Now you can also make a <code>default.nix</code> that looks like this:</p> <pre><code>{ bundlerApp }:\n\nbundlerApp {\n  pname = \"mdl\";\n  gemdir = ./.;\n  exes = [ \"mdl\" ];\n}\n</code></pre> <p>All that's left to do is to generate the corresponding <code>Gemfile.lock</code> and <code>gemset.nix</code> as described above in the <code>Using an existing Gemfile</code> section.</p>"},{"location":"languages-frameworks/ruby.section.html#packaging-executables-that-require-wrapping","title":"Packaging executables that require wrapping","text":"<p>Sometimes your app will depend on other executables at runtime, and tries to find it through the <code>PATH</code> environment variable.</p> <p>In this case, you can provide a <code>postBuild</code> hook to <code>bundlerApp</code> that wraps the gem in another script that prefixes the <code>PATH</code>.</p> <p>Of course you could also make a custom <code>gemConfig</code> if you know exactly how to patch it, but it's usually much easier to maintain with a simple wrapper so the patch doesn't have to be adjusted for each version.</p> <p>Here's another example:</p> <pre><code>{ lib, bundlerApp, makeWrapper, git, gnutar, gzip }:\n\nbundlerApp {\n  pname = \"r10k\";\n  gemdir = ./.;\n  exes = [ \"r10k\" ];\n\n  nativeBuildInputs = [ makeWrapper ];\n\n  postBuild = ''\n    wrapProgram $out/bin/r10k --prefix PATH : ${lib.makeBinPath [ git gnutar gzip ]}\n  '';\n}\n</code></pre>"},{"location":"languages-frameworks/rust.section.html","title":"Rust","text":"<p>To install the rust compiler and cargo put</p> <pre><code>environment.systemPackages = [\n  rustc\n  cargo\n];\n</code></pre> <p>into your <code>configuration.nix</code> or bring them into scope with <code>nix-shell -p rustc cargo</code>.</p> <p>For other versions such as daily builds (beta and nightly), use either <code>rustup</code> from nixpkgs (which will manage the rust installation in your home directory), or use community maintained Rust toolchains.</p>"},{"location":"languages-frameworks/rust.section.html#compiling-rust-applications-with-cargo","title":"<code>buildRustPackage</code>: Compiling Rust applications with Cargo","text":"<p>Rust applications are packaged by using the <code>buildRustPackage</code> helper from <code>rustPlatform</code>:</p> <pre><code>{ lib, fetchFromGitHub, rustPlatform }:\n\nrustPlatform.buildRustPackage rec {\n  pname = \"ripgrep\";\n  version = \"12.1.1\";\n\n  src = fetchFromGitHub {\n    owner = \"BurntSushi\";\n    repo = pname;\n    rev = version;\n    hash = \"sha256-+s5RBC3XSgb8omTbUNLywZnP6jSxZBKSS1BmXOjRF8M=\";\n  };\n\n  cargoHash = \"sha256-jtBw4ahSl88L0iuCXxQgZVm1EcboWRJMNtjxLVTtzts=\";\n\n  meta = with lib; {\n    description = \"A fast line-oriented regex search tool, similar to ag and ack\";\n    homepage = \"https://github.com/BurntSushi/ripgrep\";\n    license = licenses.unlicense;\n    maintainers = [];\n  };\n}\n</code></pre> <p><code>buildRustPackage</code> requires either the <code>cargoHash</code> or the <code>cargoSha256</code> attribute which is computed over all crate sources of this package. <code>cargoSha256</code> is used for traditional Nix SHA-256 hashes. <code>cargoHash</code> should instead be used for SRI hashes and should be preferred. For example:</p> <pre><code>  cargoHash = \"sha256-l1vL2ZdtDRxSGvP0X/l3nMw8+6WF67KPutJEzUROjg8=\";\n</code></pre> <p>Exception: If the application has cargo <code>git</code> dependencies, the <code>cargoHash</code>/<code>cargoSha256</code> approach will not work, and you will need to copy the <code>Cargo.lock</code> file of the application to nixpkgs and continue with the next section for specifying the options of the <code>cargoLock</code> section.</p> <p>Both types of hashes are permitted when contributing to nixpkgs. The Cargo hash is obtained by inserting a fake checksum into the expression and building the package once. The correct checksum can then be taken from the failed build. A fake hash can be used for <code>cargoSha256</code> as follows:</p> <pre><code>  cargoSha256 = lib.fakeSha256;\n</code></pre> <p>For <code>cargoHash</code> you can use:</p> <pre><code>  cargoHash = lib.fakeHash;\n</code></pre> <p>Per the instructions in the Cargo Book best practices guide, Rust applications should always commit the <code>Cargo.lock</code> file in git to ensure a reproducible build. However, a few packages do not, and Nix depends on this file, so if it is missing you can use <code>cargoPatches</code> to apply it in the <code>patchPhase</code>. Consider sending a PR upstream with a note to the maintainer describing why it's important to include in the application.</p> <p>The fetcher will verify that the <code>Cargo.lock</code> file is in sync with the <code>src</code> attribute, and fail the build if not. It will also will compress the vendor directory into a tar.gz archive.</p> <p>The tarball with vendored dependencies contains a directory with the package's <code>name</code>, which is normally composed of <code>pname</code> and <code>version</code>. This means that the vendored dependencies hash (<code>cargoSha256</code>/<code>cargoHash</code>) is dependent on the package name and version. The <code>cargoDepsName</code> attribute can be used to use another name for the directory of vendored dependencies. For example, the hash can be made invariant to the version by setting <code>cargoDepsName</code> to <code>pname</code>:</p> <pre><code>rustPlatform.buildRustPackage rec {\n  pname = \"broot\";\n  version = \"1.2.0\";\n\n  src = fetchCrate {\n    inherit pname version;\n    hash = \"sha256-aDQA4A5mScX9or3Lyiv/5GyAehidnpKKE0grhbP1Ctc=\";\n  };\n\n  cargoHash = \"sha256-tbrTbutUs5aPSV+yE0IBUZAAytgmZV7Eqxia7g+9zRs=\";\n  cargoDepsName = pname;\n\n  # ...\n}\n</code></pre>"},{"location":"languages-frameworks/rust.section.html#importing-a-cargo.lock-file","title":"Importing a <code>Cargo.lock</code> file","text":"<p>Using <code>cargoSha256</code> or <code>cargoHash</code> is tedious when using <code>buildRustPackage</code> within a project, since it requires that the hash is updated after every change to <code>Cargo.lock</code>. Therefore, <code>buildRustPackage</code> also supports vendoring dependencies directly from a <code>Cargo.lock</code> file using the <code>cargoLock</code> argument. For example:</p> <pre><code>rustPlatform.buildRustPackage {\n  pname = \"myproject\";\n  version = \"1.0.0\";\n\n  cargoLock = {\n    lockFile = ./Cargo.lock;\n  };\n\n  # ...\n}\n</code></pre> <p>This will retrieve the dependencies using fixed-output derivations from the specified lockfile.</p> <p>One caveat is that <code>Cargo.lock</code> cannot be patched in the <code>patchPhase</code> because it runs after the dependencies have already been fetched. If you need to patch or generate the lockfile you can alternatively set <code>cargoLock.lockFileContents</code> to a string of its contents:</p> <pre><code>rustPlatform.buildRustPackage {\n  pname = \"myproject\";\n  version = \"1.0.0\";\n\n  cargoLock = let\n    fixupLockFile = path: f (builtins.readFile path);\n  in {\n    lockFileContents = fixupLockFile ./Cargo.lock;\n  };\n\n  # ...\n}\n</code></pre> <p>Note that setting <code>cargoLock.lockFile</code> or <code>cargoLock.lockFileContents</code> doesn't add a <code>Cargo.lock</code> to your <code>src</code>, and a <code>Cargo.lock</code> is still required to build a rust package. A simple fix is to use:</p> <pre><code>postPatch = ''\n  ln -s ${./Cargo.lock} Cargo.lock\n'';\n</code></pre> <p>The output hash of each dependency that uses a git source must be specified in the <code>outputHashes</code> attribute. For example:</p> <pre><code>rustPlatform.buildRustPackage rec {\n  pname = \"myproject\";\n  version = \"1.0.0\";\n\n  cargoLock = {\n    lockFile = ./Cargo.lock;\n    outputHashes = {\n      \"finalfusion-0.14.0\" = \"17f4bsdzpcshwh74w5z119xjy2if6l2wgyjy56v621skr2r8y904\";\n    };\n  };\n\n  # ...\n}\n</code></pre> <p>If you do not specify an output hash for a git dependency, building the package will fail and inform you of which crate needs to be added. To find the correct hash, you can first use <code>lib.fakeSha256</code> or <code>lib.fakeHash</code> as a stub hash. Building the package (and thus the vendored dependencies) will then inform you of the correct hash.</p> <p>For usage outside nixpkgs, <code>allowBuiltinFetchGit</code> could be used to avoid having to specify <code>outputHashes</code>. For example:</p> <pre><code>rustPlatform.buildRustPackage rec {\n  pname = \"myproject\";\n  version = \"1.0.0\";\n\n  cargoLock = {\n    lockFile = ./Cargo.lock;\n    allowBuiltinFetchGit = true;\n  };\n\n  # ...\n}\n</code></pre>"},{"location":"languages-frameworks/rust.section.html#cargo-features","title":"Cargo features","text":"<p>You can disable default features using <code>buildNoDefaultFeatures</code>, and extra features can be added with <code>buildFeatures</code>.</p> <p>If you want to use different features for check phase, you can use <code>checkNoDefaultFeatures</code> and <code>checkFeatures</code>. They are only passed to <code>cargo test</code> and not <code>cargo build</code>. If left unset, they default to <code>buildNoDefaultFeatures</code> and <code>buildFeatures</code>.</p> <p>For example:</p> <pre><code>rustPlatform.buildRustPackage rec {\n  pname = \"myproject\";\n  version = \"1.0.0\";\n\n  buildNoDefaultFeatures = true;\n  buildFeatures = [ \"color\" \"net\" ];\n\n  # disable network features in tests\n  checkFeatures = [ \"color\" ];\n\n  # ...\n}\n</code></pre>"},{"location":"languages-frameworks/rust.section.html#cross-compilation","title":"Cross compilation","text":"<p>By default, Rust packages are compiled for the host platform, just like any other package is.  The <code>--target</code> passed to rust tools is computed from this. By default, it takes the <code>stdenv.hostPlatform.config</code> and replaces components where they are known to differ. But there are ways to customize the argument:</p> <ul> <li>To choose a different target by name, define    <code>stdenv.hostPlatform.rustc.config</code> as that name (a string), and that    name will be used instead.</li> </ul> <p>For example:</p> <pre><code>import &lt;nixpkgs&gt; {\n  crossSystem = (import &lt;nixpkgs/lib&gt;).systems.examples.armhf-embedded // {\n    rustc.config = \"thumbv7em-none-eabi\";\n  };\n}\n</code></pre> <p>will result in:</p> <pre><code>--target thumbv7em-none-eabi\n</code></pre> <ul> <li>To pass a completely custom target, define    <code>stdenv.hostPlatform.rustc.config</code> with its name, and    <code>stdenv.hostPlatform.rustc.platform</code> with the value.  The value will be    serialized to JSON in a file called    <code>${stdenv.hostPlatform.rustc.config}.json</code>, and the path of that file    will be used instead.</li> </ul> <p>For example:</p> <pre><code>import &lt;nixpkgs&gt; {\n  crossSystem = (import &lt;nixpkgs/lib&gt;).systems.examples.armhf-embedded // {\n    rustc.config = \"thumb-crazy\";\n    rustc.platform = { foo = \"\"; bar = \"\"; };\n  };\n}\n</code></pre> <p>will result in:</p> <pre><code>--target /nix/store/asdfasdfsadf-thumb-crazy.json # contains {\"foo\":\"\",\"bar\":\"\"}\n</code></pre> <p>Note that currently custom targets aren't compiled with <code>std</code>, so <code>cargo test</code> will fail. This can be ignored by adding <code>doCheck = false;</code> to your derivation.</p>"},{"location":"languages-frameworks/rust.section.html#running-package-tests","title":"Running package tests","text":"<p>When using <code>buildRustPackage</code>, the <code>checkPhase</code> is enabled by default and runs <code>cargo test</code> on the package to build. To make sure that we don't compile the sources twice and to actually test the artifacts that will be used at runtime, the tests will be ran in the <code>release</code> mode by default.</p> <p>However, in some cases the test-suite of a package doesn't work properly in the <code>release</code> mode. For these situations, the mode for <code>checkPhase</code> can be changed like so:</p> <pre><code>rustPlatform.buildRustPackage {\n  /* ... */\n  checkType = \"debug\";\n}\n</code></pre> <p>Please note that the code will be compiled twice here: once in <code>release</code> mode for the <code>buildPhase</code>, and again in <code>debug</code> mode for the <code>checkPhase</code>.</p> <p>Test flags, e.g., <code>--package foo</code>, can be passed to <code>cargo test</code> via the <code>cargoTestFlags</code> attribute.</p> <p>Another attribute, called <code>checkFlags</code>, is used to pass arguments to the test binary itself, as stated here.</p>"},{"location":"languages-frameworks/rust.section.html#tests-relying-on-the-structure-of-the-target-directory","title":"Tests relying on the structure of the <code>target/</code> directory","text":"<p>Some tests may rely on the structure of the <code>target/</code> directory. Those tests are likely to fail because we use <code>cargo --target</code> during the build. This means that the artifacts are stored in <code>target/&lt;architecture&gt;/release/</code>, rather than in <code>target/release/</code>.</p> <p>This can only be worked around by patching the affected tests accordingly.</p>"},{"location":"languages-frameworks/rust.section.html#disabling-package-tests","title":"Disabling package-tests","text":"<p>In some instances, it may be necessary to disable testing altogether (with <code>doCheck = false;</code>):</p> <ul> <li>If no tests exist -- the <code>checkPhase</code> should be explicitly disabled to skip   unnecessary build steps to speed up the build.</li> <li>If tests are highly impure (e.g. due to network usage).</li> </ul> <p>There will obviously be some corner-cases not listed above where it's sensible to disable tests. The above are just guidelines, and exceptions may be granted on a case-by-case basis.</p> <p>However, please check if it's possible to disable a problematic subset of the test suite and leave a comment explaining your reasoning.</p> <p>This can be achieved with <code>--skip</code> in <code>checkFlags</code>:</p> <pre><code>rustPlatform.buildRustPackage {\n  /* ... */\n  checkFlags = [\n    # reason for disabling test\n    \"--skip=example::tests:example_test\"\n  ];\n}\n</code></pre>"},{"location":"languages-frameworks/rust.section.html#using-cargo-nextest","title":"Using <code>cargo-nextest</code>","text":"<p>Tests can be run with cargo-nextest by setting <code>useNextest = true</code>. The same options still apply, but nextest accepts a different set of arguments and the settings might need to be adapted to be compatible with cargo-nextest.</p> <pre><code>rustPlatform.buildRustPackage {\n  /* ... */\n  useNextest = true;\n}\n</code></pre>"},{"location":"languages-frameworks/rust.section.html#setting-test-threads","title":"Setting <code>test-threads</code>","text":"<p><code>buildRustPackage</code> will use parallel test threads by default, sometimes it may be necessary to disable this so the tests run consecutively.</p> <pre><code>rustPlatform.buildRustPackage {\n  /* ... */\n  dontUseCargoParallelTests = true;\n}\n</code></pre>"},{"location":"languages-frameworks/rust.section.html#building-a-package-in-debug-mode","title":"Building a package in <code>debug</code> mode","text":"<p>By default, <code>buildRustPackage</code> will use <code>release</code> mode for builds. If a package should be built in <code>debug</code> mode, it can be configured like so:</p> <pre><code>rustPlatform.buildRustPackage {\n  /* ... */\n  buildType = \"debug\";\n}\n</code></pre> <p>In this scenario, the <code>checkPhase</code> will be ran in <code>debug</code> mode as well.</p>"},{"location":"languages-frameworks/rust.section.html#custom-buildinstall-procedures","title":"Custom <code>build</code>/<code>install</code>-procedures","text":"<p>Some packages may use custom scripts for building/installing, e.g. with a <code>Makefile</code>. In these cases, it's recommended to override the <code>buildPhase</code>/<code>installPhase</code>/<code>checkPhase</code>.</p> <p>Otherwise, some steps may fail because of the modified directory structure of <code>target/</code>.</p>"},{"location":"languages-frameworks/rust.section.html#building-a-crate-with-an-absent-or-out-of-date-cargo.lock-file","title":"Building a crate with an absent or out-of-date Cargo.lock file","text":"<p><code>buildRustPackage</code> needs a <code>Cargo.lock</code> file to get all dependencies in the source code in a reproducible way. If it is missing or out-of-date one can use the <code>cargoPatches</code> attribute to update or add it.</p> <pre><code>rustPlatform.buildRustPackage rec {\n  (...)\n  cargoPatches = [\n    # a patch file to add/update Cargo.lock in the source code\n    ./add-Cargo.lock.patch\n  ];\n}\n</code></pre>"},{"location":"languages-frameworks/rust.section.html#compiling-non-rust-packages-that-include-rust-code","title":"Compiling non-Rust packages that include Rust code","text":"<p>Several non-Rust packages incorporate Rust code for performance- or security-sensitive parts. <code>rustPlatform</code> exposes several functions and hooks that can be used to integrate Cargo in non-Rust packages.</p>"},{"location":"languages-frameworks/rust.section.html#vendoring-of-dependencies","title":"Vendoring of dependencies","text":"<p>Since network access is not allowed in sandboxed builds, Rust crate dependencies need to be retrieved using a fetcher. <code>rustPlatform</code> provides the <code>fetchCargoTarball</code> fetcher, which vendors all dependencies of a crate. For example, given a source path <code>src</code> containing <code>Cargo.toml</code> and <code>Cargo.lock</code>, <code>fetchCargoTarball</code> can be used as follows:</p> <pre><code>cargoDeps = rustPlatform.fetchCargoTarball {\n  inherit src;\n  hash = \"sha256-BoHIN/519Top1NUBjpB/oEMqi86Omt3zTQcXFWqrek0=\";\n};\n</code></pre> <p>The <code>src</code> attribute is required, as well as a hash specified through one of the <code>hash</code> attribute. The following optional attributes can also be used:</p> <ul> <li><code>name</code>: the name that is used for the dependencies tarball.  If   <code>name</code> is not specified, then the name <code>cargo-deps</code> will be used.</li> <li><code>sourceRoot</code>: when the <code>Cargo.lock</code>/<code>Cargo.toml</code> are in a   subdirectory, <code>sourceRoot</code> specifies the relative path to these   files.</li> <li><code>patches</code>: patches to apply before vendoring. This is useful when   the <code>Cargo.lock</code>/<code>Cargo.toml</code> files need to be patched before   vendoring.</li> </ul> <p>If a <code>Cargo.lock</code> file is available, you can alternatively use the <code>importCargoLock</code> function. In contrast to <code>fetchCargoTarball</code>, this function does not require a hash (unless git dependencies are used) and fetches every dependency as a separate fixed-output derivation. <code>importCargoLock</code> can be used as follows:</p> <pre><code>cargoDeps = rustPlatform.importCargoLock {\n  lockFile = ./Cargo.lock;\n};\n</code></pre> <p>If the <code>Cargo.lock</code> file includes git dependencies, then their output hashes need to be specified since they are not available through the lock file. For example:</p> <pre><code>cargoDeps = rustPlatform.importCargoLock {\n  lockFile = ./Cargo.lock;\n  outputHashes = {\n    \"rand-0.8.3\" = \"0ya2hia3cn31qa8894s3av2s8j5bjwb6yq92k0jsnlx7jid0jwqa\";\n  };\n};\n</code></pre> <p>If you do not specify an output hash for a git dependency, building <code>cargoDeps</code> will fail and inform you of which crate needs to be added. To find the correct hash, you can first use <code>lib.fakeSha256</code> or <code>lib.fakeHash</code> as a stub hash. Building <code>cargoDeps</code> will then inform you of the correct hash.</p>"},{"location":"languages-frameworks/rust.section.html#hooks","title":"Hooks","text":"<p><code>rustPlatform</code> provides the following hooks to automate Cargo builds:</p> <ul> <li><code>cargoSetupHook</code>: configure Cargo to use dependencies vendored   through <code>fetchCargoTarball</code>. This hook uses the <code>cargoDeps</code>   environment variable to find the vendored dependencies. If a project   already vendors its dependencies, the variable <code>cargoVendorDir</code> can   be used instead. When the <code>Cargo.toml</code>/<code>Cargo.lock</code> files are not in   <code>sourceRoot</code>, then the optional <code>cargoRoot</code> is used to specify the   Cargo root directory relative to <code>sourceRoot</code>.</li> <li><code>cargoBuildHook</code>: use Cargo to build a crate. If the crate to be   built is a crate in e.g. a Cargo workspace, the relative path to the   crate to build can be set through the optional <code>buildAndTestSubdir</code>   environment variable. Features can be specified with   <code>cargoBuildNoDefaultFeatures</code> and <code>cargoBuildFeatures</code>. Additional   Cargo build flags can be passed through <code>cargoBuildFlags</code>.</li> <li><code>maturinBuildHook</code>: use Maturin   to build a Python wheel. Similar to <code>cargoBuildHook</code>, the optional   variable <code>buildAndTestSubdir</code> can be used to build a crate in a   Cargo workspace. Additional Maturin flags can be passed through   <code>maturinBuildFlags</code>.</li> <li><code>cargoCheckHook</code>: run tests using Cargo. The build type for checks   can be set using <code>cargoCheckType</code>. Features can be specified with   <code>cargoCheckNoDefaultFeatures</code> and <code>cargoCheckFeatures</code>. Additional   flags can be passed to the tests using <code>checkFlags</code> and   <code>checkFlagsArray</code>. By default, tests are run in parallel. This can   be disabled by setting <code>dontUseCargoParallelTests</code>.</li> <li><code>cargoNextestHook</code>: run tests using   cargo-nextest. The same   options for <code>cargoCheckHook</code> also applies to <code>cargoNextestHook</code>.</li> <li><code>cargoInstallHook</code>: install binaries and static/shared libraries   that were built using <code>cargoBuildHook</code>.</li> <li><code>bindgenHook</code>: for crates which use <code>bindgen</code> as a build dependency, lets   <code>bindgen</code> find <code>libclang</code> and <code>libclang</code> find the libraries in <code>buildInputs</code>.</li> </ul>"},{"location":"languages-frameworks/rust.section.html#examples","title":"Examples","text":""},{"location":"languages-frameworks/rust.section.html#python-package-using-setuptools-rust","title":"Python package using <code>setuptools-rust</code>","text":"<p>For Python packages using <code>setuptools-rust</code>, you can use <code>fetchCargoTarball</code> and <code>cargoSetupHook</code> to retrieve and set up Cargo dependencies. The build itself is then performed by <code>buildPythonPackage</code>.</p> <p>The following example outlines how the <code>tokenizers</code> Python package is built. Since the Python package is in the <code>source/bindings/python</code> directory of the <code>tokenizers</code> project's source archive, we use <code>sourceRoot</code> to point the tooling to this directory:</p> <pre><code>{ fetchFromGitHub\n, buildPythonPackage\n, cargo\n, rustPlatform\n, rustc\n, setuptools-rust\n}:\n\nbuildPythonPackage rec {\n  pname = \"tokenizers\";\n  version = \"0.10.0\";\n\n  src = fetchFromGitHub {\n    owner = \"huggingface\";\n    repo = pname;\n    rev = \"python-v${version}\";\n    hash = \"sha256-rQ2hRV52naEf6PvRsWVCTN7B1oXAQGmnpJw4iIdhamw=\";\n  };\n\n  cargoDeps = rustPlatform.fetchCargoTarball {\n    inherit src sourceRoot;\n    name = \"${pname}-${version}\";\n    hash = \"sha256-miW//pnOmww2i6SOGbkrAIdc/JMDT4FJLqdMFojZeoY=\";\n  };\n\n  sourceRoot = \"${src.name}/bindings/python\";\n\n  nativeBuildInputs = [\n    cargo\n    rustPlatform.cargoSetupHook\n    rustc\n    setuptools-rust\n  ];\n\n  # ...\n}\n</code></pre> <p>In some projects, the Rust crate is not in the main Python source directory.  In such cases, the <code>cargoRoot</code> attribute can be used to specify the crate's directory relative to <code>sourceRoot</code>. In the following example, the crate is in <code>src/rust</code>, as specified in the <code>cargoRoot</code> attribute. Note that we also need to specify the correct path for <code>fetchCargoTarball</code>.</p> <pre><code>{ buildPythonPackage\n, fetchPypi\n, rustPlatform\n, setuptools-rust\n, openssl\n}:\n\nbuildPythonPackage rec {\n  pname = \"cryptography\";\n  version = \"3.4.2\"; # Also update the hash in vectors.nix\n\n  src = fetchPypi {\n    inherit pname version;\n    hash = \"sha256-xGDilsjLOnls3MfVbGKnj80KCUCczZxlis5PmHzpNcQ=\";\n  };\n\n  cargoDeps = rustPlatform.fetchCargoTarball {\n    inherit src;\n    sourceRoot = \"${pname}-${version}/${cargoRoot}\";\n    name = \"${pname}-${version}\";\n    hash = \"sha256-PS562W4L1NimqDV2H0jl5vYhL08H9est/pbIxSdYVfo=\";\n  };\n\n  cargoRoot = \"src/rust\";\n\n  # ...\n}\n</code></pre>"},{"location":"languages-frameworks/rust.section.html#python-package-using-maturin","title":"Python package using <code>maturin</code>","text":"<p>Python packages that use Maturin can be built with <code>fetchCargoTarball</code>, <code>cargoSetupHook</code>, and <code>maturinBuildHook</code>. For example, the following (partial) derivation builds the <code>retworkx</code> Python package. <code>fetchCargoTarball</code> and <code>cargoSetupHook</code> are used to fetch and set up the crate dependencies. <code>maturinBuildHook</code> is used to perform the build.</p> <pre><code>{ lib\n, buildPythonPackage\n, rustPlatform\n, fetchFromGitHub\n}:\n\nbuildPythonPackage rec {\n  pname = \"retworkx\";\n  version = \"0.6.0\";\n\n  src = fetchFromGitHub {\n    owner = \"Qiskit\";\n    repo = \"retworkx\";\n    rev = version;\n    hash = \"sha256-11n30ldg3y3y6qxg3hbj837pnbwjkqw3nxq6frds647mmmprrd20=\";\n  };\n\n  cargoDeps = rustPlatform.fetchCargoTarball {\n    inherit src;\n    name = \"${pname}-${version}\";\n    hash = \"sha256-heOBK8qi2nuc/Ib+I/vLzZ1fUUD/G/KTw9d7M4Hz5O0=\";\n  };\n\n  format = \"pyproject\";\n\n  nativeBuildInputs = with rustPlatform; [ cargoSetupHook maturinBuildHook ];\n\n  # ...\n}\n</code></pre>"},{"location":"languages-frameworks/rust.section.html#compiling-rust-crates-using-nix-instead-of-cargo","title":"<code>buildRustCrate</code>: Compiling Rust crates using Nix instead of Cargo","text":""},{"location":"languages-frameworks/rust.section.html#simple-operation","title":"Simple operation","text":"<p>When run, <code>cargo build</code> produces a file called <code>Cargo.lock</code>, containing pinned versions of all dependencies. Nixpkgs contains a tool called <code>crate2Nix</code> (<code>nix-shell -p crate2nix</code>), which can be used to turn a <code>Cargo.lock</code> into a Nix expression.  That Nix expression calls <code>rustc</code> directly (hence bypassing Cargo), and can be used to compile a crate and all its dependencies.</p> <p>See <code>crate2nix</code>'s documentation for instructions on how to use it.</p>"},{"location":"languages-frameworks/rust.section.html#handling-external-dependencies","title":"Handling external dependencies","text":"<p>Some crates require external libraries. For crates from crates.io, such libraries can be specified in <code>defaultCrateOverrides</code> package in nixpkgs itself.</p> <p>Starting from that file, one can add more overrides, to add features or build inputs by overriding the hello crate in a separate file.</p> <pre><code>with import &lt;nixpkgs&gt; {};\n((import ./hello.nix).hello {}).override {\n  crateOverrides = defaultCrateOverrides // {\n    hello = attrs: { buildInputs = [ openssl ]; };\n  };\n}\n</code></pre> <p>Here, <code>crateOverrides</code> is expected to be a attribute set, where the key is the crate name without version number and the value a function. The function gets all attributes passed to <code>buildRustCrate</code> as first argument and returns a set that contains all attribute that should be overwritten.</p> <p>For more complicated cases, such as when parts of the crate's derivation depend on the crate's version, the <code>attrs</code> argument of the override above can be read, as in the following example, which patches the derivation:</p> <pre><code>with import &lt;nixpkgs&gt; {};\n((import ./hello.nix).hello {}).override {\n  crateOverrides = defaultCrateOverrides // {\n    hello = attrs: lib.optionalAttrs (lib.versionAtLeast attrs.version \"1.0\")  {\n      postPatch = ''\n        substituteInPlace lib/zoneinfo.rs \\\n          --replace-fail \"/usr/share/zoneinfo\" \"${tzdata}/share/zoneinfo\"\n      '';\n    };\n  };\n}\n</code></pre> <p>Another situation is when we want to override a nested dependency. This actually works in the exact same way, since the <code>crateOverrides</code> parameter is forwarded to the crate's dependencies. For instance, to override the build inputs for crate <code>libc</code> in the example above, where <code>libc</code> is a dependency of the main crate, we could do:</p> <pre><code>with import &lt;nixpkgs&gt; {};\n((import hello.nix).hello {}).override {\n  crateOverrides = defaultCrateOverrides // {\n    libc = attrs: { buildInputs = []; };\n  };\n}\n</code></pre>"},{"location":"languages-frameworks/rust.section.html#options-and-phases-configuration","title":"Options and phases configuration","text":"<p>Actually, the overrides introduced in the previous section are more general. A number of other parameters can be overridden:</p> <ul> <li>The version of <code>rustc</code> used to compile the crate:</li> </ul> <pre><code>(hello {}).override { rust = pkgs.rust; };\n</code></pre> <ul> <li>Whether to build in release mode or debug mode (release mode by   default):</li> </ul> <pre><code>(hello {}).override { release = false; };\n</code></pre> <ul> <li>Whether to print the commands sent to <code>rustc</code> when building   (equivalent to <code>--verbose</code> in cargo:</li> </ul> <pre><code>(hello {}).override { verbose = false; };\n</code></pre> <ul> <li>Extra arguments to be passed to <code>rustc</code>:</li> </ul> <pre><code>(hello {}).override { extraRustcOpts = \"-Z debuginfo=2\"; };\n</code></pre> <ul> <li>Phases, just like in any other derivation, can be specified using   the following attributes: <code>preUnpack</code>, <code>postUnpack</code>, <code>prePatch</code>,   <code>patches</code>, <code>postPatch</code>, <code>preConfigure</code> (in the case of a Rust crate,   this is run before calling the \"build\" script), <code>postConfigure</code>   (after the \"build\" script),<code>preBuild</code>, <code>postBuild</code>, <code>preInstall</code> and   <code>postInstall</code>. As an example, here is how to create a new module   before running the build script:</li> </ul> <pre><code>(hello {}).override {\n  preConfigure = ''\n     echo \"pub const PATH=\\\"${hi.out}\\\";\" &gt;&gt; src/path.rs\"\n  '';\n};\n</code></pre>"},{"location":"languages-frameworks/rust.section.html#setting-up-nix-shell","title":"Setting Up <code>nix-shell</code>","text":"<p>Oftentimes you want to develop code from within <code>nix-shell</code>. Unfortunately <code>buildRustCrate</code> does not support common <code>nix-shell</code> operations directly (see this issue) so we will use <code>stdenv.mkDerivation</code> instead.</p> <p>Using the example <code>hello</code> project above, we want to do the following:</p> <ul> <li>Have access to <code>cargo</code> and <code>rustc</code></li> <li>Have the <code>openssl</code> library available to a crate through it's normal   compilation mechanism (<code>pkg-config</code>).</li> </ul> <p>A typical <code>shell.nix</code> might look like:</p> <pre><code>with import &lt;nixpkgs&gt; {};\n\nstdenv.mkDerivation {\n  name = \"rust-env\";\n  nativeBuildInputs = [\n    rustc cargo\n\n    # Example Build-time Additional Dependencies\n    pkg-config\n  ];\n  buildInputs = [\n    # Example Run-time Additional Dependencies\n    openssl\n  ];\n\n  # Set Environment Variables\n  RUST_BACKTRACE = 1;\n}\n</code></pre> <p>You should now be able to run the following:</p> <pre><code>$ nix-shell --pure\n$ cargo build\n$ cargo test\n</code></pre>"},{"location":"languages-frameworks/rust.section.html#using-community-maintained-rust-toolchains","title":"Using community maintained Rust toolchains","text":"<p>::: {.note} The following projects cannot be used within Nixpkgs since Import From Derivation (IFD) is disallowed in Nixpkgs. To package things that require Rust nightly, <code>RUSTC_BOOTSTRAP = true;</code> can sometimes be used as a hack. :::</p> <p>There are two community maintained approaches to Rust toolchain management: - oxalica's Rust overlay - fenix</p> <p>Despite their names, both projects provides a similar set of packages and overlays under different APIs.</p> <p>Oxalica's overlay allows you to select a particular Rust version without you providing a hash or a flake input, but comes with a larger git repository than fenix.</p> <p>Fenix also provides rust-analyzer nightly in addition to the Rust toolchains.</p> <p>Both oxalica's overlay and fenix better integrate with nix and cache optimizations. Because of this and ergonomics, either of those community projects should be preferred to the Mozilla's Rust overlay (nixpkgs-mozilla).</p> <p>The following documentation demonstrates examples using fenix and oxalica's Rust overlay with <code>nix-shell</code> and building derivations. More advanced usages like flake usage are documented in their own repositories.</p>"},{"location":"languages-frameworks/rust.section.html#using-rust-nightly-with-nix-shell","title":"Using Rust nightly with <code>nix-shell</code>","text":"<p>Here is a simple <code>shell.nix</code> that provides Rust nightly (default profile) using fenix:</p> <pre><code>with import &lt;nixpkgs&gt; { };\nlet\n  fenix = callPackage\n    (fetchFromGitHub {\n      owner = \"nix-community\";\n      repo = \"fenix\";\n      # commit from: 2023-03-03\n      rev = \"e2ea04982b892263c4d939f1cc3bf60a9c4deaa1\";\n      hash = \"sha256-AsOim1A8KKtMWIxG+lXh5Q4P2bhOZjoUhFWJ1EuZNNk=\";\n    })\n    { };\nin\nmkShell {\n  name = \"rust-env\";\n  nativeBuildInputs = [\n    # Note: to use stable, just replace `default` with `stable`\n    fenix.default.toolchain\n\n    # Example Build-time Additional Dependencies\n    pkg-config\n  ];\n  buildInputs = [\n    # Example Run-time Additional Dependencies\n    openssl\n  ];\n\n  # Set Environment Variables\n  RUST_BACKTRACE = 1;\n}\n</code></pre> <p>Save this to <code>shell.nix</code>, then run:</p> <pre><code>$ rustc --version\nrustc 1.69.0-nightly (13471d3b2 2023-03-02)\n</code></pre> <p>To see that you are using nightly.</p> <p>Oxalica's Rust overlay has more complete examples of <code>shell.nix</code> (and cross compilation) under its <code>examples</code> directory.</p>"},{"location":"languages-frameworks/rust.section.html#using-rust-nightly-in-a-derivation-with-buildrustpackage","title":"Using Rust nightly in a derivation with <code>buildRustPackage</code>","text":"<p>You can also use Rust nightly to build rust packages using <code>makeRustPlatform</code>. The below snippet demonstrates invoking <code>buildRustPackage</code> with a Rust toolchain from oxalica's overlay:</p> <pre><code>with import &lt;nixpkgs&gt;\n{\n  overlays = [\n    (import (fetchTarball \"https://github.com/oxalica/rust-overlay/archive/master.tar.gz\"))\n  ];\n};\nlet\n  rustPlatform = makeRustPlatform {\n    cargo = rust-bin.stable.latest.minimal;\n    rustc = rust-bin.stable.latest.minimal;\n  };\nin\n\nrustPlatform.buildRustPackage rec {\n  pname = \"ripgrep\";\n  version = \"12.1.1\";\n\n  src = fetchFromGitHub {\n    owner = \"BurntSushi\";\n    repo = \"ripgrep\";\n    rev = version;\n    hash = \"sha256-+s5RBC3XSgb8omTbUNLywZnP6jSxZBKSS1BmXOjRF8M=\";\n  };\n\n  cargoHash = \"sha256-l1vL2ZdtDRxSGvP0X/l3nMw8+6WF67KPutJEzUROjg8=\";\n\n  doCheck = false;\n\n  meta = with lib; {\n    description = \"A fast line-oriented regex search tool, similar to ag and ack\";\n    homepage = \"https://github.com/BurntSushi/ripgrep\";\n    license = with licenses; [ mit unlicense ];\n    maintainers = with maintainers; [];\n  };\n}\n</code></pre> <p>Follow the below steps to try that snippet. 1. save the above snippet as <code>default.nix</code> in that directory 2. cd into that directory and run <code>nix-build</code></p> <p>Fenix also has examples with <code>buildRustPackage</code>, crane, naersk, and cross compilation in its Examples section.</p>"},{"location":"languages-frameworks/rust.section.html#using-git-bisect-on-the-rust-compiler","title":"Using <code>git bisect</code> on the Rust compiler","text":"<p>Sometimes an upgrade of the Rust compiler (<code>rustc</code>) will break a downstream package.  In these situations, being able to <code>git bisect</code> the <code>rustc</code> version history to find the offending commit is quite useful.  Nixpkgs makes it easy to do this.</p> <p>First, roll back your nixpkgs to a commit in which its <code>rustc</code> used the most recent one which doesn't have the problem.  You'll need to do this because of <code>rustc</code>'s extremely aggressive version-pinning.</p> <p>Next, add the following overlay, updating the Rust version to the one in your rolled-back nixpkgs, and replacing <code>/git/scratch/rust</code> with the path into which you have <code>git clone</code>d the <code>rustc</code> git repository:</p> <pre><code> (final: prev: /*lib.optionalAttrs prev.stdenv.targetPlatform.isAarch64*/ {\n   rust_1_72 =\n     lib.updateManyAttrsByPath [{\n       path = [ \"packages\" \"stable\" ];\n       update = old: old.overrideScope(final: prev: {\n         rustc-unwrapped = prev.rustc-unwrapped.overrideAttrs (_: {\n           src = lib.cleanSource /git/scratch/rust;\n           # do *not* put passthru.isReleaseTarball=true here\n         });\n       });\n     }]\n       prev.rust_1_72;\n })\n</code></pre> <p>If the problem you're troubleshooting only manifests when cross-compiling you can uncomment the <code>lib.optionalAttrs</code> in the example above, and replace <code>isAarch64</code> with the target that is having problems.  This will speed up your bisect quite a bit, since the host compiler won't need to be rebuilt.</p> <p>Now, you can start a <code>git bisect</code> in the directory where you checked out the <code>rustc</code> source code.  It is recommended to select the endpoint commits by searching backwards from <code>origin/master</code> for the commits which added the release notes for the versions in question.  If you set the endpoints to commits on the release branches (i.e. the release tags), git-bisect will often get confused by the complex merge-commit structures it will need to traverse.</p> <p>The command loop you'll want to use for bisecting looks like this:</p> <pre><code>git bisect {good,bad}  # depending on result of last build\ngit submodule update --init\nCARGO_NET_OFFLINE=false cargo vendor \\\n  --sync ./src/tools/cargo/Cargo.toml \\\n  --sync ./src/tools/rust-analyzer/Cargo.toml \\\n  --sync ./compiler/rustc_codegen_cranelift/Cargo.toml \\\n  --sync ./src/bootstrap/Cargo.toml\nnix-build $NIXPKGS -A package-broken-by-rust-changes\n</code></pre> <p>The <code>git submodule update --init</code> and <code>cargo vendor</code> commands above require network access, so they can't be performed from within the <code>rustc</code> derivation, unfortunately.</p>"},{"location":"languages-frameworks/swift.section.html","title":"Swift","text":"<p>The Swift compiler is provided by the <code>swift</code> package:</p> <pre><code># Compile and link a simple executable.\nnix-shell -p swift --run 'swiftc -' &lt;&lt;&lt; 'print(\"Hello world!\")'\n# Run it!\n./main\n</code></pre> <p>The <code>swift</code> package also provides the <code>swift</code> command, with some caveats:</p> <ul> <li>Swift Package Manager (SwiftPM) is packaged separately as <code>swiftpm</code>. If you   need functionality like <code>swift build</code>, <code>swift run</code>, <code>swift test</code>, you must   also add the <code>swiftpm</code> package to your closure.</li> <li>On Darwin, the <code>swift repl</code> command requires an Xcode installation. This is   because it uses the system LLDB debugserver, which has special entitlements.</li> </ul>"},{"location":"languages-frameworks/swift.section.html#ssec-swift-module-search-paths","title":"Module search paths","text":"<p>Like other toolchains in Nixpkgs, the Swift compiler executables are wrapped to help Swift find your application's dependencies in the Nix store. These wrappers scan the <code>buildInputs</code> of your package derivation for specific directories where Swift modules are placed by convention, and automatically add those directories to the Swift compiler search paths.</p> <p>Swift follows different conventions depending on the platform. The wrappers look for the following directories:</p> <ul> <li>On Darwin platforms: <code>lib/swift/macosx</code>   (If not targeting macOS, replace <code>macosx</code> with the Xcode platform name.)</li> <li>On other platforms: <code>lib/swift/linux/x86_64</code>   (Where <code>linux</code> and <code>x86_64</code> are from lowercase <code>uname -sm</code>.)</li> <li>For convenience, Nixpkgs also adds <code>lib/swift</code> to the search path.   This can save a bit of work packaging Swift modules, because many Nix builds   will produce output for just one target any way.</li> </ul>"},{"location":"languages-frameworks/swift.section.html#ssec-swift-core-libraries","title":"Core libraries","text":"<p>In addition to the standard library, the Swift toolchain contains some additional 'core libraries' that, on Apple platforms, are normally distributed as part of the OS or Xcode. These are packaged separately in Nixpkgs, and can be found (for use in <code>buildInputs</code>) as:</p> <ul> <li><code>swiftPackages.Dispatch</code></li> <li><code>swiftPackages.Foundation</code></li> <li><code>swiftPackages.XCTest</code></li> </ul>"},{"location":"languages-frameworks/swift.section.html#ssec-swift-packaging-with-swiftpm","title":"Packaging with SwiftPM","text":"<p>Nixpkgs includes a small helper <code>swiftpm2nix</code> that can fetch your SwiftPM dependencies for you, when you need to write a Nix expression to package your application.</p> <p>The first step is to run the generator:</p> <pre><code>cd /path/to/my/project\n# Enter a Nix shell with the required tools.\nnix-shell -p swift swiftpm swiftpm2nix\n# First, make sure the workspace is up-to-date.\nswift package resolve\n# Now generate the Nix code.\nswiftpm2nix\n</code></pre> <p>This produces some files in a directory <code>nix</code>, which will be part of your Nix expression. The next step is to write that expression:</p> <pre><code>{ stdenv, swift, swiftpm, swiftpm2nix, fetchFromGitHub }:\n\nlet\n  # Pass the generated files to the helper.\n  generated = swiftpm2nix.helpers ./nix;\nin\n\nstdenv.mkDerivation rec {\n  pname = \"myproject\";\n  version = \"0.0.0\";\n\n  src = fetchFromGitHub {\n    owner = \"nixos\";\n    repo = pname;\n    rev = version;\n    hash = \"sha256-AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=\";\n  };\n\n  # Including SwiftPM as a nativeBuildInput provides a buildPhase for you.\n  # This by default performs a release build using SwiftPM, essentially:\n  #   swift build -c release\n  nativeBuildInputs = [ swift swiftpm ];\n\n  # The helper provides a configure snippet that will prepare all dependencies\n  # in the correct place, where SwiftPM expects them.\n  configurePhase = generated.configure;\n\n  installPhase = ''\n    # This is a special function that invokes swiftpm to find the location\n    # of the binaries it produced.\n    binPath=\"$(swiftpmBinPath)\"\n    # Now perform any installation steps.\n    mkdir -p $out/bin\n    cp $binPath/myproject $out/bin/\n  '';\n}\n</code></pre>"},{"location":"languages-frameworks/swift.section.html#ssec-swiftpm-custom-build-flags","title":"Custom build flags","text":"<p>If you'd like to build a different configuration than <code>release</code>:</p> <pre><code>swiftpmBuildConfig = \"debug\";\n</code></pre> <p>It is also possible to provide additional flags to <code>swift build</code>:</p> <pre><code>swiftpmFlags = [ \"--disable-dead-strip\" ];\n</code></pre> <p>The default <code>buildPhase</code> already passes <code>-j</code> for parallel building.</p> <p>If these two customization options are insufficient, provide your own <code>buildPhase</code> that invokes <code>swift build</code>.</p>"},{"location":"languages-frameworks/swift.section.html#ssec-swiftpm-running-tests","title":"Running tests","text":"<p>Including <code>swiftpm</code> in your <code>nativeBuildInputs</code> also provides a default <code>checkPhase</code>, but it must be enabled with:</p> <pre><code>doCheck = true;\n</code></pre> <p>This essentially runs: <code>swift test -c release</code></p>"},{"location":"languages-frameworks/swift.section.html#ssec-swiftpm-patching-dependencies","title":"Patching dependencies","text":"<p>In some cases, it may be necessary to patch a SwiftPM dependency. SwiftPM dependencies are located in <code>.build/checkouts</code>, but the <code>swiftpm2nix</code> helper provides these as symlinks to read-only <code>/nix/store</code> paths. In order to patch them, we need to make them writable.</p> <p>A special function <code>swiftpmMakeMutable</code> is available to replace the symlink with a writable copy:</p> <pre><code>configurePhase = generated.configure ++ ''\n  # Replace the dependency symlink with a writable copy.\n  swiftpmMakeMutable swift-crypto\n  # Now apply a patch.\n  patch -p1 -d .build/checkouts/swift-crypto -i ${./some-fix.patch}\n'';\n</code></pre>"},{"location":"languages-frameworks/swift.section.html#ssec-swift-considerations-for-custom-build-tools","title":"Considerations for custom build tools","text":""},{"location":"languages-frameworks/swift.section.html#ssec-swift-linking-the-standard-library","title":"Linking the standard library","text":"<p>The <code>swift</code> package has a separate <code>lib</code> output containing just the Swift standard library, to prevent Swift applications needing a dependency on the full Swift compiler at run-time. Linking with the Nixpkgs Swift toolchain already ensures binaries correctly reference the <code>lib</code> output.</p> <p>Sometimes, Swift is used only to compile part of a mixed codebase, and the link step is manual. Custom build tools often locate the standard library relative to the <code>swift</code> compiler executable, and while the result will work, when this path ends up in the binary, it will have the Swift compiler as an unintended dependency.</p> <p>In this case, you should investigate how your build process discovers the standard library, and override the path. The correct path will be something like: <code>\"${swift.swift.lib}/${swift.swiftModuleSubdir}\"</code></p>"},{"location":"languages-frameworks/texlive.section.html","title":"TeX Live","text":"<p>Since release 15.09 there is a new TeX Live packaging that lives entirely under attribute <code>texlive</code>.</p>"},{"location":"languages-frameworks/texlive.section.html#sec-language-texlive-user-guide-experimental","title":"User's guide (experimental new interface)","text":"<p>Release 23.11 ships with a new interface that will eventually replace <code>texlive.combine</code>.</p> <ul> <li> <p>For basic usage, use some of the prebuilt environments available at the top level, such as <code>texliveBasic</code>, <code>texliveSmall</code>. For the full list of prebuilt environments, inspect <code>texlive.schemes</code>.</p> </li> <li> <p>Packages cannot be used directly but must be assembled in an environment. To create or add packages to an environment, use   <pre><code>texliveSmall.withPackages (ps: with ps; [ collection-langkorean algorithms cm-super ])\n</code></pre>   The function <code>withPackages</code> can be called multiple times to add more packages.</p> </li> <li> <p>Note. Within Nixpkgs, packages should only use prebuilt environments as inputs, such as <code>texliveSmall</code> or <code>texliveInfraOnly</code>, and should not depend directly on <code>texlive</code>. Further dependencies should be added by calling <code>withPackages</code>. This is to ensure that there is a consistent and simple way to override the inputs.</p> </li> <li> <p><code>texlive.withPackages</code> uses the same logic as <code>buildEnv</code>. Only parts of a package are installed in an environment: its 'runtime' files (<code>tex</code> output), binaries (<code>out</code> output), and support files (<code>tlpkg</code> output). Moreover, man and info pages are assembled into separate <code>man</code> and <code>info</code> outputs. To add only the TeX files of a package, or its documentation (<code>texdoc</code> output), just specify the outputs:   <pre><code>texlive.withPackages (ps: with ps; [\n  texdoc # recommended package to navigate the documentation\n  perlPackages.LaTeXML.tex # tex files of LaTeXML, omit binaries\n  cm-super\n  cm-super.texdoc # documentation of cm-super\n])\n</code></pre></p> </li> <li> <p>All packages distributed by TeX Live, which contains most of CTAN, are available and can be found under <code>texlive.pkgs</code>:   <pre><code>$ nix repl\nnix-repl&gt; :l &lt;nixpkgs&gt;\nnix-repl&gt; texlive.pkgs.[TAB]\n</code></pre>   Note that the packages in <code>texlive.pkgs</code> are only provided for search purposes and must not be used directly.</p> </li> <li> <p>Experimental and subject to change without notice: to add the documentation for all packages in the environment, use   <pre><code>texliveSmall.__overrideTeXConfig { withDocs = true; }\n</code></pre>   This can be applied before or after calling <code>withPackages</code>.</p> </li> </ul> <p>The function currently support the parameters <code>withDocs</code>, <code>withSources</code>, and <code>requireTeXPackages</code>.</p>"},{"location":"languages-frameworks/texlive.section.html#sec-language-texlive-user-guide","title":"User's guide","text":"<ul> <li> <p>For basic usage just pull <code>texlive.combined.scheme-basic</code> for an environment with basic LaTeX support.</p> </li> <li> <p>It typically won't work to use separately installed packages together. Instead, you can build a custom set of packages like this. Most CTAN packages should be available:</p> </li> </ul> <pre><code>texlive.combine {\n  inherit (texlive) scheme-small collection-langkorean algorithms cm-super;\n}\n</code></pre> <ul> <li> <p>There are all the schemes, collections and a few thousand packages, as defined upstream (perhaps with tiny differences).</p> </li> <li> <p>By default you only get executables and files needed during runtime, and a little documentation for the core packages. To change that, you need to add <code>pkgFilter</code> function to <code>combine</code>.</p> </li> </ul> <pre><code>texlive.combine {\n  # inherit (texlive) whatever-you-want;\n  pkgFilter = pkg:\n    pkg.tlType == \"run\" || pkg.tlType == \"bin\" || pkg.hasManpages || pkg.pname == \"cm-super\";\n  # elem tlType [ \"run\" \"bin\" \"doc\" \"source\" ]\n  # there are also other attributes: version, name\n}\n</code></pre> <ul> <li>You can list packages e.g. by <code>nix repl</code>.</li> </ul> <pre><code>$ nix repl\nnix-repl&gt; :l &lt;nixpkgs&gt;\nnix-repl&gt; texlive.collection-[TAB]\n</code></pre> <ul> <li> <p>Note that the wrapper assumes that the result has a chance to be useful. For example, the core executables should be present, as well as some core data files. The supported way of ensuring this is by including some scheme, for example <code>scheme-basic</code>, into the combination.</p> </li> <li> <p>TeX Live packages are also available under <code>texlive.pkgs</code> as derivations with outputs <code>out</code>, <code>tex</code>, <code>texdoc</code>, <code>texsource</code>, <code>tlpkg</code>, <code>man</code>, <code>info</code>. They cannot be installed outside of <code>texlive.combine</code> but are available for other uses. To repackage a font, for instance, use</p> </li> </ul> <pre><code>stdenvNoCC.mkDerivation rec {\n  src = texlive.pkgs.iwona;\n\n  inherit (src) pname version;\n\n  installPhase = ''\n    runHook preInstall\n    install -Dm644 fonts/opentype/nowacki/iwona/*.otf -t $out/share/fonts/opentype\n    runHook postInstall\n  '';\n}\n</code></pre> <p>See <code>biber</code>, <code>iwona</code> for complete examples.</p>"},{"location":"languages-frameworks/texlive.section.html#sec-language-texlive-custom-packages","title":"Custom packages","text":"<p>You may find that you need to use an external TeX package. A derivation for such package has to provide the contents of the \"texmf\" directory in its <code>\"tex\"</code> output, according to the TeX Directory Structure. Dependencies on other TeX packages can be listed in the attribute <code>tlDeps</code>.</p> <p>The functions <code>texlive.combine</code> and <code>texlive.withPackages</code> recognise the following outputs:</p> <ul> <li><code>\"out\"</code>: contents are linked in the TeX Live environment, and binaries in the <code>$out/bin</code> folder are wrapped;</li> <li><code>\"tex\"</code>: linked in <code>$TEXMFDIST</code>; files should follow the TDS (for instance <code>$tex/tex/latex/foiltex/foiltex.cls</code>);</li> <li><code>\"texdoc\"</code>, <code>\"texsource\"</code>: ignored by default, treated as <code>\"tex\"</code>;</li> <li><code>\"tlpkg\"</code>: linked in <code>$TEXMFROOT/tlpkg</code>;</li> <li><code>\"man\"</code>, <code>\"info\"</code>, ...: the other outputs are combined into separate outputs.</li> </ul> <p>When using <code>pkgFilter</code>, <code>texlive.combine</code> will assign <code>tlType</code> respectively <code>\"bin\"</code>, <code>\"run\"</code>, <code>\"doc\"</code>, <code>\"source\"</code>, <code>\"tlpkg\"</code> to the above outputs.</p> <p>Here is a (very verbose) example. See also the packages <code>auctex</code>, <code>eukleides</code>, <code>mftrace</code> for more examples.</p> <pre><code>with import &lt;nixpkgs&gt; {};\n\nlet\n  foiltex = stdenvNoCC.mkDerivation {\n    pname = \"latex-foiltex\";\n    version = \"2.1.4b\";\n\n    outputs = [ \"tex\" \"texdoc\" ];\n    passthru.tlDeps = with texlive; [ latex ];\n\n    srcs = [\n      (fetchurl {\n        url = \"http://mirrors.ctan.org/macros/latex/contrib/foiltex/foiltex.dtx\";\n        hash = \"sha256-/2I2xHXpZi0S988uFsGuPV6hhMw8e0U5m/P8myf42R0=\";\n      })\n      (fetchurl {\n        url = \"http://mirrors.ctan.org/macros/latex/contrib/foiltex/foiltex.ins\";\n        hash = \"sha256-KTm3pkd+Cpu0nSE2WfsNEa56PeXBaNfx/sOO2Vv0kyc=\";\n      })\n    ];\n\n    unpackPhase = ''\n      runHook preUnpack\n\n      for _src in $srcs; do\n        cp \"$_src\" $(stripHash \"$_src\")\n      done\n\n      runHook postUnpack\n    '';\n\n    nativeBuildInputs = [\n      (texliveSmall.withPackages (ps: with ps; [ cm-super hypdoc latexmk ]))\n      # multiple-outputs.sh fails if $out is not defined\n      (writeShellScript \"force-tex-output.sh\" ''\n        out=\"''${tex-}\"\n      '')\n    ];\n\n    dontConfigure = true;\n\n    buildPhase = ''\n      runHook preBuild\n\n      # Generate the style files\n      latex foiltex.ins\n\n      # Generate the documentation\n      export HOME=.\n      latexmk -pdf foiltex.dtx\n\n      runHook postBuild\n    '';\n\n    installPhase = ''\n      runHook preInstall\n\n      path=\"$tex/tex/latex/foiltex\"\n      mkdir -p \"$path\"\n      cp *.{cls,def,clo,sty} \"$path/\"\n\n      path=\"$texdoc/doc/tex/latex/foiltex\"\n      mkdir -p \"$path\"\n      cp *.pdf \"$path/\"\n\n      runHook postInstall\n    '';\n\n    meta = with lib; {\n      description = \"A LaTeX2e class for overhead transparencies\";\n      license = licenses.unfreeRedistributable;\n      maintainers = with maintainers; [ veprbl ];\n      platforms = platforms.all;\n    };\n  };\n\n  latex_with_foiltex = texliveSmall.withPackages (_: [ foiltex ]);\nin\n  runCommand \"test.pdf\" {\n    nativeBuildInputs = [ latex_with_foiltex ];\n  } ''\ncat &gt;test.tex &lt;&lt;EOF\n\\documentclass{foils}\n\n\\title{Presentation title}\n\\date{}\n\n\\begin{document}\n\\maketitle\n\\end{document}\nEOF\n  pdflatex test.tex\n  cp test.pdf $out\n''\n</code></pre>"},{"location":"languages-frameworks/texlive.section.html#sec-language-texlive-lualatex-font-cache","title":"LuaLaTeX font cache","text":"<p>The font cache for LuaLaTeX is written to <code>$HOME</code>. Therefore, it is necessary to set <code>$HOME</code> to a writable path, e.g. before using LuaLaTeX in nix derivations: <pre><code>runCommandNoCC \"lualatex-hello-world\" {\n  buildInputs = [ texliveFull ];\n} ''\n  mkdir $out\n  echo '\\documentclass{article} \\begin{document} Hello world \\end{document}' &gt; main.tex\n  env HOME=$(mktemp -d) lualatex  -interaction=nonstopmode -output-format=pdf -output-directory=$out ./main.tex\n''\n</code></pre></p> <p>Additionally, the cache of a user can diverge from the nix store. To resolve font issues that might follow, the cache can be removed by the user: <pre><code>luaotfload-tool --cache=erase --flush-lookups --force\n</code></pre></p>"},{"location":"languages-frameworks/titanium.section.html","title":"Titanium","text":"<p>The Nixpkgs repository contains facilities to deploy a variety of versions of the Titanium SDK versions, a cross-platform mobile app development framework using JavaScript as an implementation language, and includes a function abstraction making it possible to build Titanium applications for Android and iOS devices from source code.</p> <p>Not all Titanium features supported -- currently, it can only be used to build Android and iOS apps.</p>"},{"location":"languages-frameworks/titanium.section.html#building-a-titanium-app","title":"Building a Titanium app","text":"<p>We can build a Titanium app from source for Android or iOS and for debugging or release purposes by invoking the <code>titaniumenv.buildApp {}</code> function:</p> <pre><code>titaniumenv.buildApp {\n  name = \"myapp\";\n  src = ./myappsource;\n\n  preBuild = \"\";\n  target = \"android\"; # or 'iphone'\n  tiVersion = \"7.1.0.GA\";\n  release = true;\n\n  androidsdkArgs = {\n    platformVersions = [ \"25\" \"26\" ];\n  };\n  androidKeyStore = ./keystore;\n  androidKeyAlias = \"myfirstapp\";\n  androidKeyStorePassword = \"secret\";\n\n  xcodeBaseDir = \"/Applications/Xcode.app\";\n  xcodewrapperArgs = {\n    version = \"9.3\";\n  };\n  iosMobileProvisioningProfile = ./myprovisioning.profile;\n  iosCertificateName = \"My Company\";\n  iosCertificate = ./mycertificate.p12;\n  iosCertificatePassword = \"secret\";\n  iosVersion = \"11.3\";\n  iosBuildStore = false;\n\n  enableWirelessDistribution = true;\n  installURL = \"/installipa.php\";\n}\n</code></pre> <p>The <code>titaniumenv.buildApp {}</code> function takes the following parameters:</p> <ul> <li>The <code>name</code> parameter refers to the name in the Nix store.</li> <li>The <code>src</code> parameter refers to the source code location of the app that needs   to be built.</li> <li><code>preRebuild</code> contains optional build instructions that are carried out before   the build starts.</li> <li><code>target</code> indicates for which device the app must be built. Currently only   'android' and 'iphone' (for iOS) are supported.</li> <li><code>tiVersion</code> can be used to optionally override the requested Titanium version   in <code>tiapp.xml</code>. If not specified, it will use the version in <code>tiapp.xml</code>.</li> <li><code>release</code> should be set to true when building an app for submission to the   Google Playstore or Apple Appstore. Otherwise, it should be false.</li> </ul> <p>When the <code>target</code> has been set to <code>android</code>, we can configure the following parameters:</p> <ul> <li>The <code>androidSdkArgs</code> parameter refers to an attribute set that propagates all   parameters to the <code>androidenv.composeAndroidPackages {}</code> function. This can   be used to install all relevant Android plugins that may be needed to perform   the Android build. If no parameters are given, it will deploy the platform   SDKs for API-levels 25 and 26 by default.</li> </ul> <p>When the <code>release</code> parameter has been set to true, you need to provide parameters to sign the app:</p> <ul> <li><code>androidKeyStore</code> is the path to the keystore file</li> <li><code>androidKeyAlias</code> is the key alias</li> <li><code>androidKeyStorePassword</code> refers to the password to open the keystore file.</li> </ul> <p>When the <code>target</code> has been set to <code>iphone</code>, we can configure the following parameters:</p> <ul> <li>The <code>xcodeBaseDir</code> parameter refers to the location where Xcode has been   installed. When none value is given, the above value is the default.</li> <li>The <code>xcodewrapperArgs</code> parameter passes arbitrary parameters to the   <code>xcodeenv.composeXcodeWrapper {}</code> function. This can, for example, be used   to adjust the default version of Xcode.</li> </ul> <p>When <code>release</code> has been set to true, you also need to provide the following parameters:</p> <ul> <li><code>iosMobileProvisioningProfile</code> refers to a mobile provisioning profile needed   for signing.</li> <li><code>iosCertificateName</code> refers to the company name in the P12 certificate.</li> <li><code>iosCertificate</code> refers to the path to the P12 file.</li> <li><code>iosCertificatePassword</code> contains the password to open the P12 file.</li> <li><code>iosVersion</code> refers to the iOS SDK version to use. It defaults to the latest   version.</li> <li><code>iosBuildStore</code> should be set to <code>true</code> when building for the Apple Appstore   submission. For enterprise or ad-hoc builds it should be set to <code>false</code>.</li> </ul> <p>When <code>enableWirelessDistribution</code> has been enabled, you must also provide the path of the PHP script (<code>installURL</code>) (that is included with the iOS build environment) to enable wireless ad-hoc installations.</p>"},{"location":"languages-frameworks/titanium.section.html#emulating-or-simulating-the-app","title":"Emulating or simulating the app","text":"<p>It is also possible to simulate the correspond iOS simulator build by using <code>xcodeenv.simulateApp {}</code> and emulate an Android APK by using <code>androidenv.emulateApp {}</code>.</p>"},{"location":"languages-frameworks/vim.section.html","title":"Vim","text":"<p>Both Neovim and Vim can be configured to include your favorite plugins and additional libraries.</p> <p>Loading can be deferred; see examples.</p> <p>At the moment we support two different methods for managing plugins:</p> <ul> <li>Vim packages (recommended)</li> <li>vim-plug (vim only)</li> </ul> <p>Right now two Vim packages are available: <code>vim</code> which has most features that require extra dependencies disabled and <code>vim-full</code> which has them configurable and enabled by default.</p> <p>::: {.note} <code>vim_configurable</code> is a deprecated alias for <code>vim-full</code> and refers to the fact that its build-time features are configurable. It has nothing to do with user configuration, and both the <code>vim</code> and <code>vim-full</code> packages can be customized as explained in the next section. :::</p>"},{"location":"languages-frameworks/vim.section.html#custom-configuration","title":"Custom configuration","text":"<p>Adding custom .vimrc lines can be done using the following code:</p> <pre><code>vim-full.customize {\n  # `name` optionally specifies the name of the executable and package\n  name = \"vim-with-plugins\";\n\n  vimrcConfig.customRC = ''\n    set hidden\n  '';\n}\n</code></pre> <p>This configuration is used when Vim is invoked with the command specified as name, in this case <code>vim-with-plugins</code>. You can also omit <code>name</code> to customize Vim itself. See the definition of <code>vimUtils.makeCustomizable</code> for all supported options.</p> <p>For Neovim the <code>configure</code> argument can be overridden to achieve the same:</p> <pre><code>neovim.override {\n  configure = {\n    customRC = ''\n      # here your custom configuration goes!\n    '';\n  };\n}\n</code></pre> <p>If you want to use <code>neovim-qt</code> as a graphical editor, you can configure it by overriding Neovim in an overlay or passing it an overridden Neovim:</p> <pre><code>neovim-qt.override {\n  neovim = neovim.override {\n    configure = {\n      customRC = ''\n        # your custom configuration\n      '';\n    };\n  };\n}\n</code></pre>"},{"location":"languages-frameworks/vim.section.html#managing-plugins-with-vim-packages","title":"Managing plugins with Vim packages","text":"<p>To store your plugins in Vim packages (the native Vim plugin manager, see <code>:help packages</code>) the following example can be used:</p> <pre><code>vim-full.customize {\n  vimrcConfig.packages.myVimPackage = with pkgs.vimPlugins; {\n    # loaded on launch\n    start = [ youcompleteme fugitive ];\n    # manually loadable by calling `:packadd $plugin-name`\n    # however, if a Vim plugin has a dependency that is not explicitly listed in\n    # opt that dependency will always be added to start to avoid confusion.\n    opt = [ phpCompletion elm-vim ];\n    # To automatically load a plugin when opening a filetype, add vimrc lines like:\n    # autocmd FileType php :packadd phpCompletion\n  };\n}\n</code></pre> <p><code>myVimPackage</code> is an arbitrary name for the generated package. You can choose any name you like. For Neovim the syntax is:</p> <pre><code>neovim.override {\n  configure = {\n    customRC = ''\n      # here your custom configuration goes!\n    '';\n    packages.myVimPackage = with pkgs.vimPlugins; {\n      # see examples below how to use custom packages\n      start = [ ];\n      # If a Vim plugin has a dependency that is not explicitly listed in\n      # opt that dependency will always be added to start to avoid confusion.\n      opt = [ ];\n    };\n  };\n}\n</code></pre> <p>The resulting package can be added to <code>packageOverrides</code> in <code>~/.nixpkgs/config.nix</code> to make it installable:</p> <pre><code>{\n  packageOverrides = pkgs: with pkgs; {\n    myVim = vim-full.customize {\n      # `name` specifies the name of the executable and package\n      name = \"vim-with-plugins\";\n      # add here code from the example section\n    };\n    myNeovim = neovim.override {\n      configure = {\n      # add code from the example section here\n      };\n    };\n  };\n}\n</code></pre> <p>After that you can install your special grafted <code>myVim</code> or <code>myNeovim</code> packages.</p>"},{"location":"languages-frameworks/vim.section.html#what-if-your-favourite-vim-plugin-isnt-already-packaged","title":"What if your favourite Vim plugin isn\u2019t already packaged?","text":"<p>If one of your favourite plugins isn't packaged, you can package it yourself:</p> <pre><code>{ config, pkgs, ... }:\n\nlet\n  easygrep = pkgs.vimUtils.buildVimPlugin {\n    name = \"vim-easygrep\";\n    src = pkgs.fetchFromGitHub {\n      owner = \"dkprice\";\n      repo = \"vim-easygrep\";\n      rev = \"d0c36a77cc63c22648e792796b1815b44164653a\";\n      hash = \"sha256-bL33/S+caNmEYGcMLNCanFZyEYUOUmSsedCVBn4tV3g=\";\n    };\n  };\nin\n{\n  environment.systemPackages = [\n    (\n      pkgs.neovim.override {\n        configure = {\n          packages.myPlugins = with pkgs.vimPlugins; {\n          start = [\n            vim-go # already packaged plugin\n            easygrep # custom package\n          ];\n          opt = [];\n        };\n        # ...\n      };\n     }\n    )\n  ];\n}\n</code></pre> <p>If your package requires building specific parts, use instead <code>pkgs.vimUtils.buildVimPlugin</code>.</p>"},{"location":"languages-frameworks/vim.section.html#vim-plugin-specificities","title":"Specificities for some plugins","text":""},{"location":"languages-frameworks/vim.section.html#vim-plugin-treesitter","title":"Treesitter","text":"<p>By default <code>nvim-treesitter</code> encourages you to download, compile and install the required Treesitter grammars at run time with <code>:TSInstall</code>. This works poorly on NixOS.  Instead, to install the <code>nvim-treesitter</code> plugins with a set of precompiled grammars, you can use <code>nvim-treesitter.withPlugins</code> function:</p> <pre><code>(pkgs.neovim.override {\n  configure = {\n    packages.myPlugins = with pkgs.vimPlugins; {\n      start = [\n        (nvim-treesitter.withPlugins (\n          plugins: with plugins; [\n            nix\n            python\n          ]\n        ))\n      ];\n    };\n  };\n})\n</code></pre> <p>To enable all grammars packaged in nixpkgs, use <code>pkgs.vimPlugins.nvim-treesitter.withAllGrammars</code>.</p>"},{"location":"languages-frameworks/vim.section.html#managing-plugins-with-vim-plug","title":"Managing plugins with vim-plug","text":"<p>To use vim-plug to manage your Vim plugins the following example can be used:</p> <pre><code>vim-full.customize {\n  vimrcConfig.packages.myVimPackage = with pkgs.vimPlugins; {\n    # loaded on launch\n    plug.plugins = [ youcompleteme fugitive phpCompletion elm-vim ];\n  };\n}\n</code></pre> <p>Note: this is not possible anymore for Neovim.</p>"},{"location":"languages-frameworks/vim.section.html#adding-new-plugins-to-nixpkgs","title":"Adding new plugins to nixpkgs","text":"<p>Nix expressions for Vim plugins are stored in pkgs/applications/editors/vim/plugins. For the vast majority of plugins, Nix expressions are automatically generated by running <code>nix-shell -p vimPluginsUpdater --run vim-plugins-updater</code>. This creates a generated.nix file based on the plugins listed in vim-plugin-names.</p> <p>After running the updater, if nvim-treesitter received an update, also run <code>nvim-treesitter/update.py</code> to update the tree sitter grammars for <code>nvim-treesitter</code>.</p> <p>Some plugins require overrides in order to function properly. Overrides are placed in overrides.nix. Overrides are most often required when a plugin requires some dependencies, or extra steps are required during the build process. For example <code>deoplete-fish</code> requires both <code>deoplete-nvim</code> and <code>vim-fish</code>, and so the following override was added:</p> <pre><code>deoplete-fish = super.deoplete-fish.overrideAttrs(old: {\n  dependencies = with super; [ deoplete-nvim vim-fish ];\n});\n</code></pre> <p>Sometimes plugins require an override that must be changed when the plugin is updated. This can cause issues when Vim plugins are auto-updated but the associated override isn't updated. For these plugins, the override should be written so that it specifies all information required to install the plugin, and running <code>./update.py</code> doesn't change the derivation for the plugin. Manually updating the override is required to update these types of plugins. An example of such a plugin is <code>LanguageClient-neovim</code>.</p> <p>To add a new plugin, run <code>./update.py add \"[owner]/[name]\"</code>. NOTE: This script automatically commits to your git repository. Be sure to check out a fresh branch before running.</p> <p>Finally, there are some plugins that are also packaged in nodePackages because they have Javascript-related build steps, such as running webpack. Those plugins are not listed in <code>vim-plugin-names</code> or managed by <code>update.py</code> at all, and are included separately in <code>overrides.nix</code>. Currently, all these plugins are related to the <code>coc.nvim</code> ecosystem of the Language Server Protocol integration with Vim/Neovim.</p>"},{"location":"languages-frameworks/vim.section.html#updating-plugins-in-nixpkgs","title":"Updating plugins in nixpkgs","text":"<p>Run the update script with a GitHub API token that has at least <code>public_repo</code> access. Running the script without the token is likely to result in rate-limiting (429 errors). For steps on creating an API token, please refer to GitHub's token documentation.</p> <pre><code>GITHUB_API_TOKEN=my_token ./pkgs/applications/editors/vim/plugins/update.py\n</code></pre> <p>Alternatively, set the number of processes to a lower count to avoid rate-limiting.</p> <pre><code>nix-shell -p vimPluginsUpdater --run 'vim-plugins-updater --proc 1'\n</code></pre>"},{"location":"languages-frameworks/vim.section.html#vim-out-of-tree-overlays","title":"How to maintain an out-of-tree overlay of vim plugins ?","text":"<p>You can use the updater script to generate basic packages out of a custom vim plugin list:</p> <pre><code>nix-shell -p vimPluginsUpdater --run vim-plugins-updater -i vim-plugin-names -o generated.nix --no-commit\n</code></pre> <p>with the contents of <code>vim-plugin-names</code> being for example:</p> <pre><code>repo,branch,alias\npwntester/octo.nvim,,\n</code></pre> <p>You can then reference the generated vim plugins via:</p> <pre><code>myVimPlugins = pkgs.vimPlugins.extend (\n  (pkgs.callPackage ./generated.nix {})\n);\n</code></pre>"},{"location":"module-system/module-system.chapter.html","title":"Module System","text":""},{"location":"module-system/module-system.chapter.html#module-system-introduction","title":"Introduction","text":"<p>The module system is a language for handling configuration, implemented as a Nix library.</p> <p>Compared to plain Nix, it adds documentation, type checking and composition or extensibility.</p> <p>::: {.note} This chapter is new and not complete yet. For a gentle introduction to the module system, in the context of NixOS, see Writing NixOS Modules in the NixOS manual. :::</p>"},{"location":"module-system/module-system.chapter.html#module-system-lib-evalModules","title":"<code>lib.evalModules</code>","text":"<p>Evaluate a set of modules. This function is typically only used once per application (e.g. once in NixOS, once in Home Manager, ...).</p>"},{"location":"module-system/module-system.chapter.html#module-system-lib-evalModules-parameters","title":"Parameters","text":""},{"location":"module-system/module-system.chapter.html#module-system-lib-evalModules-param-modules","title":"<code>modules</code>","text":"<p>A list of modules. These are merged together to form the final configuration.</p>"},{"location":"module-system/module-system.chapter.html#module-system-lib-evalModules-param-specialArgs","title":"<code>specialArgs</code>","text":"<p>An attribute set of module arguments that can be used in <code>imports</code>.</p> <p>This is in contrast to <code>config._module.args</code>, which is only available after all <code>imports</code> have been resolved.</p>"},{"location":"module-system/module-system.chapter.html#module-system-lib-evalModules-param-class","title":"<code>class</code>","text":"<p>If the <code>class</code> attribute is set and non-<code>null</code>, the module system will reject <code>imports</code> with a different <code>_class</code> declaration.</p> <p>The <code>class</code> value should be a string in lower camel case.</p> <p>If applicable, the <code>class</code> should match the \"prefix\" of the attributes used in (experimental) flakes. Some examples are:</p> <ul> <li><code>nixos</code> as in <code>flake.nixosModules</code></li> <li><code>nixosTest</code>: modules that constitute a NixOS VM test</li> </ul>"},{"location":"module-system/module-system.chapter.html#module-system-lib-evalModules-param-prefix","title":"<code>prefix</code>","text":"<p>A list of strings representing the location at or below which all options are evaluated. This is used by <code>types.submodule</code> to improve error reporting and find the implicit <code>name</code> module argument.</p>"},{"location":"module-system/module-system.chapter.html#module-system-lib-evalModules-return-value","title":"Return value","text":"<p>The result is an attribute set with the following attributes:</p>"},{"location":"module-system/module-system.chapter.html#module-system-lib-evalModules-return-value-options","title":"<code>options</code>","text":"<p>The nested attribute set of all option declarations.</p>"},{"location":"module-system/module-system.chapter.html#module-system-lib-evalModules-return-value-config","title":"<code>config</code>","text":"<p>The nested attribute set of all option values.</p>"},{"location":"module-system/module-system.chapter.html#module-system-lib-evalModules-return-value-type","title":"<code>type</code>","text":"<p>A module system type. This type is an instance of <code>types.submoduleWith</code> containing the current <code>modules</code>.</p> <p>The option definitions that are typed with this type will extend the current set of modules, like <code>extendModules</code>.</p> <p>However, the value returned from the type is just the <code>config</code>, like any submodule.</p> <p>If you're familiar with prototype inheritance, you can think of this <code>evalModules</code> invocation as the prototype, and usages of this type as the instances.</p> <p>This type is also available to the <code>modules</code> as the module argument <code>moduleType</code>.</p>"},{"location":"module-system/module-system.chapter.html#module-system-lib-evalModules-return-value-extendModules","title":"<code>extendModules</code>","text":"<p>A function similar to <code>evalModules</code> but building on top of the already passed <code>modules</code>. Its arguments, <code>modules</code> and <code>specialArgs</code> are added to the existing values.</p> <p>If you're familiar with prototype inheritance, you can think of the current, actual <code>evalModules</code> invocation as the prototype, and the return value of <code>extendModules</code> as the instance.</p> <p>This functionality is also available to modules as the <code>extendModules</code> module argument.</p> <p>::: {.note}</p> <p>Evaluation Performance</p> <p><code>extendModules</code> returns a configuration that shares very little with the original <code>evalModules</code> invocation, because the module arguments may be different.</p> <p>So if you have a configuration that has been (or will be) largely evaluated, almost none of the computation is shared with the configuration returned by <code>extendModules</code>.</p> <p>The real work of module evaluation happens while computing the values in <code>config</code> and <code>options</code>, so multiple invocations of <code>extendModules</code> have a particularly small cost, as long as only the final <code>config</code> and <code>options</code> are evaluated.</p> <p>If you do reference multiple <code>config</code> (or <code>options</code>) from before and after <code>extendModules</code>, evaluation performance is the same as with multiple <code>evalModules</code> invocations, because the new modules' ability to override existing configuration fundamentally requires constructing a new <code>config</code> and <code>options</code> fixpoint. :::</p>"},{"location":"module-system/module-system.chapter.html#module-system-lib-evalModules-return-value-_module","title":"<code>_module</code>","text":"<p>A portion of the configuration tree which is elided from <code>config</code>.</p>"},{"location":"module-system/module-system.chapter.html#module-system-lib-evalModules-return-value-_type","title":"<code>_type</code>","text":"<p>A nominal type marker, always <code>\"configuration\"</code>.</p>"},{"location":"module-system/module-system.chapter.html#module-system-lib-evalModules-return-value-_configurationClass","title":"<code>class</code>","text":"<p>The <code>class</code> argument.</p>"},{"location":"packages/index.html","title":"Packages","text":"<p>This chapter contains information about how to use and maintain the Nix expressions for a number of specific packages, such as the Linux kernel or X.org.</p> <p><code>{=include=} sections citrix.section.md darwin-builder.section.md dlib.section.md eclipse.section.md elm.section.md emacs.section.md firefox.section.md fish.section.md fuse.section.md ibus.section.md kakoune.section.md linux.section.md locales.section.md etc-files.section.md nginx.section.md opengl.section.md shell-helpers.section.md steam.section.md cataclysm-dda.section.md urxvt.section.md weechat.section.md xorg.section.md</code></p>"},{"location":"packages/cataclysm-dda.section.html","title":"Cataclysm: Dark Days Ahead","text":""},{"location":"packages/cataclysm-dda.section.html#how-to-install-cataclysm-dda","title":"How to install Cataclysm DDA","text":"<p>To install the latest stable release of Cataclysm DDA to your profile, execute <code>nix-env -f \"&lt;nixpkgs&gt;\" -iA cataclysm-dda</code>. For the curses build (build without tiles), install <code>cataclysmDDA.stable.curses</code>. Note: <code>cataclysm-dda</code> is an alias to <code>cataclysmDDA.stable.tiles</code>.</p> <p>If you like access to a development build of your favorite git revision, override <code>cataclysm-dda-git</code> (or <code>cataclysmDDA.git.curses</code> if you like curses build):</p> <pre><code>cataclysm-dda-git.override {\n  version = \"YYYY-MM-DD\";\n  rev = \"YOUR_FAVORITE_REVISION\";\n  sha256 = \"CHECKSUM_OF_THE_REVISION\";\n}\n</code></pre> <p>The sha256 checksum can be obtained by</p> <pre><code>nix-prefetch-url --unpack \"https://github.com/CleverRaven/Cataclysm-DDA/archive/${YOUR_FAVORITE_REVISION}.tar.gz\"\n</code></pre> <p>The default configuration directory is <code>~/.cataclysm-dda</code>. If you prefer <code>$XDG_CONFIG_HOME/cataclysm-dda</code>, override the derivation:</p> <pre><code>cataclysm-dda.override {\n  useXdgDir = true;\n}\n</code></pre>"},{"location":"packages/cataclysm-dda.section.html#important-note-for-overriding-packages","title":"Important note for overriding packages","text":"<p>After applying <code>overrideAttrs</code>, you need to fix <code>passthru.pkgs</code> and <code>passthru.withMods</code> attributes either manually or by using <code>attachPkgs</code>:</p> <pre><code>let\n  # You enabled parallel building.\n  myCDDA = cataclysm-dda-git.overrideAttrs (_: {\n    enableParallelBuilding = true;\n  });\n\n  # Unfortunately, this refers to the package before overriding and\n  # parallel building is still disabled.\n  badExample = myCDDA.withMods (_: []);\n\n  inherit (cataclysmDDA) attachPkgs pkgs wrapCDDA;\n\n  # You can fix it by hand\n  goodExample1 = myCDDA.overrideAttrs (old: {\n    passthru = old.passthru // {\n      pkgs = pkgs.override { build = goodExample1; };\n      withMods = wrapCDDA goodExample1;\n    };\n  });\n\n  # or by using a helper function `attachPkgs`.\n  goodExample2 = attachPkgs pkgs myCDDA;\nin\n\n# badExample                     # parallel building disabled\n# goodExample1.withMods (_: [])  # parallel building enabled\ngoodExample2.withMods (_: [])    # parallel building enabled\n</code></pre>"},{"location":"packages/cataclysm-dda.section.html#customizing-with-mods","title":"Customizing with mods","text":"<p>To install Cataclysm DDA with mods of your choice, you can use <code>withMods</code> attribute:</p> <pre><code>cataclysm-dda.withMods (mods: with mods; [\n  tileset.UndeadPeople\n])\n</code></pre> <p>All mods, soundpacks, and tilesets available in nixpkgs are found in <code>cataclysmDDA.pkgs</code>.</p> <p>Here is an example to modify existing mods and/or add more mods not available in nixpkgs:</p> <pre><code>let\n  customMods = self: super: lib.recursiveUpdate super {\n    # Modify existing mod\n    tileset.UndeadPeople = super.tileset.UndeadPeople.overrideAttrs (old: {\n      # If you like to apply a patch to the tileset for example\n      patches = [ ./path/to/your.patch ];\n    });\n\n    # Add another mod\n    mod.Awesome = cataclysmDDA.buildMod {\n      modName = \"Awesome\";\n      version = \"0.x\";\n      src = fetchFromGitHub {\n        owner = \"Someone\";\n        repo = \"AwesomeMod\";\n        rev = \"...\";\n        hash = \"...\";\n      };\n      # Path to be installed in the unpacked source (default: \".\")\n      modRoot = \"contents/under/this/path/will/be/installed\";\n    };\n\n    # Add another soundpack\n    soundpack.Fantastic = cataclysmDDA.buildSoundPack {\n      # ditto\n    };\n\n    # Add another tileset\n    tileset.SuperDuper = cataclysmDDA.buildTileSet {\n      # ditto\n    };\n  };\nin\ncataclysm-dda.withMods (mods: with mods.extend customMods; [\n  tileset.UndeadPeople\n  mod.Awesome\n  soundpack.Fantastic\n  tileset.SuperDuper\n])\n</code></pre>"},{"location":"packages/citrix.section.html","title":"Citrix Workspace","text":"<p>The Citrix Workspace App is a remote desktop viewer which provides access to XenDesktop installations.</p>"},{"location":"packages/citrix.section.html#sec-citrix-base","title":"Basic usage","text":"<p>The tarball archive needs to be downloaded manually, as the license agreements of the vendor for Citrix Workspace needs to be accepted first. Then run <code>nix-prefetch-url file://$PWD/linuxx64-$version.tar.gz</code>. With the archive available in the store, the package can be built and installed with Nix.</p>"},{"location":"packages/citrix.section.html#sec-citrix-selfservice","title":"Citrix Self-service","text":"<p>The self-service is an application managing Citrix desktops and applications. Please note that this feature only works with at least citrix_workspace_20_06_0 and later versions.</p> <p>In order to set this up, you first have to download the <code>.cr</code> file from the Netscaler Gateway. After that, you can configure the <code>selfservice</code> like this:</p> <pre><code>$ storebrowse -C ~/Downloads/receiverconfig.cr\n$ selfservice\n</code></pre>"},{"location":"packages/citrix.section.html#sec-citrix-custom-certs","title":"Custom certificates","text":"<p>The <code>Citrix Workspace App</code> in <code>nixpkgs</code> trusts several certificates from the Mozilla database by default. However, several companies using Citrix might require their own corporate certificate. On distros with imperative packaging, these certs can be stored easily in <code>$ICAROOT</code>, however this directory is a store path in <code>nixpkgs</code>. In order to work around this issue, the package provides a simple mechanism to add custom certificates without rebuilding the entire package using <code>symlinkJoin</code>:</p> <pre><code>with import &lt;nixpkgs&gt; { config.allowUnfree = true; };\nlet\n  extraCerts = [\n    ./custom-cert-1.pem\n    ./custom-cert-2.pem # ...\n  ];\nin citrix_workspace.override { inherit extraCerts; }\n</code></pre>"},{"location":"packages/darwin-builder.section.html","title":"darwin.linux-builder","text":"<p><code>darwin.linux-builder</code> provides a way to bootstrap a Linux remote builder on a macOS machine.</p> <p>This requires macOS version 12.4 or later.</p> <p>The remote builder runs on host port 31022 by default. You can change it by overriding <code>virtualisation.darwin-builder.hostPort</code>. See the example.</p> <p>You will also need to be a trusted user for your Nix installation.  In other words, your <code>/etc/nix/nix.conf</code> should have something like:</p> <pre><code>extra-trusted-users = &lt;your username goes here&gt;\n</code></pre> <p>To launch the remote builder, run the following flake:</p> <pre><code>$ nix run nixpkgs#darwin.linux-builder\n</code></pre> <p>That will prompt you to enter your <code>sudo</code> password:</p> <pre><code>+ sudo --reset-timestamp /nix/store/\u2026-install-credentials.sh ./keys\nPassword:\n</code></pre> <p>\u2026 so that it can install a private key used to <code>ssh</code> into the build server. After that the script will launch the virtual machine and automatically log you in as the <code>builder</code> user:</p> <pre><code>&lt;&lt;&lt; Welcome to NixOS 22.11.20220901.1bd8d11 (aarch64) - ttyAMA0 &gt;&gt;&gt;\n\nRun 'nixos-help' for the NixOS manual.\n\nnixos login: builder (automatic login)\n\n\n[builder@nixos:~]$\n</code></pre> <p>Note: When you need to stop the VM, run <code>shutdown now</code> as the <code>builder</code> user.</p> <p>To delegate builds to the remote builder, add the following options to your <code>nix.conf</code> file:</p> <pre><code># - Replace ${ARCH} with either aarch64 or x86_64 to match your host machine\n# - Replace ${MAX_JOBS} with the maximum number of builds (pick 4 if you're not sure)\nbuilders = ssh-ng://builder@linux-builder ${ARCH}-linux /etc/nix/builder_ed25519 ${MAX_JOBS} - - - c3NoLWVkMjU1MTkgQUFBQUMzTnphQzFsWkRJMU5URTVBQUFBSUpCV2N4Yi9CbGFxdDFhdU90RStGOFFVV3JVb3RpQzVxQkorVXVFV2RWQ2Igcm9vdEBuaXhvcwo=\n\n# Not strictly necessary, but this will reduce your disk utilization\nbuilders-use-substitutes = true\n</code></pre> <p>To allow Nix to connect to a remote builder not running on port 22, you will also need to create a new file at <code>/etc/ssh/ssh_config.d/100-linux-builder.conf</code>:</p> <pre><code>Host linux-builder\n  Hostname localhost\n  HostKeyAlias linux-builder\n  Port 31022\n</code></pre> <p>\u2026 and then restart your Nix daemon to apply the change:</p> <pre><code>$ sudo launchctl kickstart -k system/org.nixos.nix-daemon\n</code></pre>"},{"location":"packages/darwin-builder.section.html#sec-darwin-builder-example-flake","title":"Example flake usage","text":"<pre><code>{\n  inputs = {\n    nixpkgs.url = \"github:nixos/nixpkgs/nixpkgs-22.11-darwin\";\n    darwin.url = \"github:lnl7/nix-darwin/master\";\n    darwin.inputs.nixpkgs.follows = \"nixpkgs\";\n  };\n\n  outputs = { self, darwin, nixpkgs, ... }@inputs:\n  let\n\n    inherit (darwin.lib) darwinSystem;\n    system = \"aarch64-darwin\";\n    pkgs = nixpkgs.legacyPackages.\"${system}\";\n    linuxSystem = builtins.replaceStrings [ \"darwin\" ] [ \"linux\" ] system;\n\n    darwin-builder = nixpkgs.lib.nixosSystem {\n      system = linuxSystem;\n      modules = [\n        \"${nixpkgs}/nixos/modules/profiles/macos-builder.nix\"\n        { virtualisation = {\n            host.pkgs = pkgs;\n            darwin-builder.workingDirectory = \"/var/lib/darwin-builder\";\n          };\n        };\n      ];\n    };\n  in {\n\n    darwinConfigurations = {\n      machine1 = darwinSystem {\n        inherit system;\n        modules = [\n          {\n            nix.distributedBuilds = true;\n            nix.buildMachines = [{\n              hostName = \"ssh://builder@localhost\";\n              system = linuxSystem;\n              maxJobs = 4;\n              supportedFeatures = [ \"kvm\" \"benchmark\" \"big-parallel\" ];\n            }];\n\n            launchd.daemons.darwin-builder = {\n              command = \"${darwin-builder.config.system.build.macos-builder-installer}/bin/create-builder\";\n              serviceConfig = {\n                KeepAlive = true;\n                RunAtLoad = true;\n                StandardOutPath = \"/var/log/darwin-builder.log\";\n                StandardErrorPath = \"/var/log/darwin-builder.log\";\n              };\n            };\n          }\n        ];\n      };\n    };\n\n  };\n}\n</code></pre>"},{"location":"packages/darwin-builder.section.html#sec-darwin-builder-reconfiguring","title":"Reconfiguring the remote builder","text":"<p>Initially you should not change the remote builder configuration else you will not be able to use the binary cache. However, after you have the remote builder running locally you may use it to build a modified remote builder with additional storage or memory.</p> <p>To do this, you just need to set the <code>virtualisation.darwin-builder.*</code> parameters as in the example below and rebuild.</p> <pre><code>    darwin-builder = nixpkgs.lib.nixosSystem {\n      system = linuxSystem;\n      modules = [\n        \"${nixpkgs}/nixos/modules/profiles/macos-builder.nix\"\n        {\n          virtualisation.host.pkgs = pkgs;\n          virtualisation.darwin-builder.diskSize = 5120;\n          virtualisation.darwin-builder.memorySize = 1024;\n          virtualisation.darwin-builder.hostPort = 33022;\n          virtualisation.darwin-builder.workingDirectory = \"/var/lib/darwin-builder\";\n        }\n      ];\n</code></pre> <p>You may make any other changes to your VM in this attribute set. For example, you could enable Docker or X11 forwarding to your Darwin host.</p>"},{"location":"packages/darwin-builder.section.html#sec-darwin-builder-troubleshoot","title":"Troubleshooting the generated configuration","text":"<p>The <code>linux-builder</code> package exposes the attributes <code>nixosConfig</code> and <code>nixosOptions</code> that allow you to inspect the generated NixOS configuration in the <code>nix repl</code>. For example:</p> <pre><code>$ nix repl --file ~/src/nixpkgs --argstr system aarch64-darwin\n\nnix-repl&gt; darwin.linux-builder.nixosConfig.nix.package\n\u00abderivation /nix/store/...-nix-2.17.0.drv\u00bb\n\nnix-repl&gt; :p darwin.linux-builder.nixosOptions.virtualisation.memorySize.definitionsWithLocations\n[ { file = \"/home/user/src/nixpkgs/nixos/modules/profiles/macos-builder.nix\"; value = 3072; } ]\n</code></pre>"},{"location":"packages/dlib.section.html","title":"DLib","text":"<p>DLib is a modern, C++-based toolkit which provides several machine learning algorithms.</p>"},{"location":"packages/dlib.section.html#compiling-without-avx-support","title":"Compiling without AVX support","text":"<p>Especially older CPUs don't support AVX (Advanced Vector Extensions) instructions that are used by DLib to optimize their algorithms.</p> <p>On the affected hardware errors like <code>Illegal instruction</code> will occur. In those cases AVX support needs to be disabled:</p> <pre><code>self: super: { dlib = super.dlib.override { avxSupport = false; }; }\n</code></pre>"},{"location":"packages/eclipse.section.html","title":"Eclipse","text":"<p>The Nix expressions related to the Eclipse platform and IDE are in <code>pkgs/applications/editors/eclipse</code>.</p> <p>Nixpkgs provides a number of packages that will install Eclipse in its various forms. These range from the bare-bones Eclipse Platform to the more fully featured Eclipse SDK or Scala-IDE packages and multiple version are often available. It is possible to list available Eclipse packages by issuing the command:</p> <pre><code>$ nix-env -f '&lt;nixpkgs&gt;' -qaP -A eclipses --description\n</code></pre> <p>Once an Eclipse variant is installed, it can be run using the <code>eclipse</code> command, as expected. From within Eclipse, it is then possible to install plugins in the usual manner by either manually specifying an Eclipse update site or by installing the Marketplace Client plugin and using it to discover and install other plugins. This installation method provides an Eclipse installation that closely resemble a manually installed Eclipse.</p> <p>If you prefer to install plugins in a more declarative manner, then Nixpkgs also offer a number of Eclipse plugins that can be installed in an Eclipse environment. This type of environment is created using the function <code>eclipseWithPlugins</code> found inside the <code>nixpkgs.eclipses</code> attribute set. This function takes as argument <code>{ eclipse, plugins ? [], jvmArgs ? [] }</code> where <code>eclipse</code> is a one of the Eclipse packages described above, <code>plugins</code> is a list of plugin derivations, and <code>jvmArgs</code> is a list of arguments given to the JVM running the Eclipse. For example, say you wish to install the latest Eclipse Platform with the popular Eclipse Color Theme plugin and also allow Eclipse to use more RAM. You could then add:</p> <pre><code>packageOverrides = pkgs: {\n  myEclipse = with pkgs.eclipses; eclipseWithPlugins {\n    eclipse = eclipse-platform;\n    jvmArgs = [ \"-Xmx2048m\" ];\n    plugins = [ plugins.color-theme ];\n  };\n}\n</code></pre> <p>to your Nixpkgs configuration (<code>~/.config/nixpkgs/config.nix</code>) and install it by running <code>nix-env -f '&lt;nixpkgs&gt;' -iA myEclipse</code> and afterward run Eclipse as usual. It is possible to find out which plugins are available for installation using <code>eclipseWithPlugins</code> by running:</p> <pre><code>$ nix-env -f '&lt;nixpkgs&gt;' -qaP -A eclipses.plugins --description\n</code></pre> <p>If there is a need to install plugins that are not available in Nixpkgs then it may be possible to define these plugins outside Nixpkgs using the <code>buildEclipseUpdateSite</code> and <code>buildEclipsePlugin</code> functions found in the <code>nixpkgs.eclipses.plugins</code> attribute set. Use the <code>buildEclipseUpdateSite</code> function to install a plugin distributed as an Eclipse update site. This function takes <code>{ name, src }</code> as argument, where <code>src</code> indicates the Eclipse update site archive. All Eclipse features and plugins within the downloaded update site will be installed. When an update site archive is not available, then the <code>buildEclipsePlugin</code> function can be used to install a plugin that consists of a pair of feature and plugin JARs. This function takes an argument <code>{ name, srcFeature, srcPlugin }</code> where <code>srcFeature</code> and <code>srcPlugin</code> are the feature and plugin JARs, respectively.</p> <p>Expanding the previous example with two plugins using the above functions, we have:</p> <pre><code>packageOverrides = pkgs: {\n  myEclipse = with pkgs.eclipses; eclipseWithPlugins {\n    eclipse = eclipse-platform;\n    jvmArgs = [ \"-Xmx2048m\" ];\n    plugins = [\n      plugins.color-theme\n      (plugins.buildEclipsePlugin {\n        name = \"myplugin1-1.0\";\n        srcFeature = fetchurl {\n          url = \"http://\u2026/features/myplugin1.jar\";\n          hash = \"sha256-123\u2026\";\n        };\n        srcPlugin = fetchurl {\n          url = \"http://\u2026/plugins/myplugin1.jar\";\n          hash = \"sha256-123\u2026\";\n        };\n      });\n      (plugins.buildEclipseUpdateSite {\n        name = \"myplugin2-1.0\";\n        src = fetchurl {\n          stripRoot = false;\n          url = \"http://\u2026/myplugin2.zip\";\n          hash = \"sha256-123\u2026\";\n        };\n      });\n    ];\n  };\n}\n</code></pre>"},{"location":"packages/elm.section.html","title":"Elm","text":"<p>To start a development environment, run:</p> <pre><code>nix-shell -p elmPackages.elm elmPackages.elm-format\n</code></pre> <p>To update the Elm compiler, see <code>nixpkgs/pkgs/development/compilers/elm/README.md</code>.</p> <p>To package Elm applications, read about elm2nix.</p>"},{"location":"packages/emacs.section.html","title":"Emacs","text":""},{"location":"packages/emacs.section.html#sec-emacs-config","title":"Configuring Emacs","text":"<p>The Emacs package comes with some extra helpers to make it easier to configure. <code>emacs.pkgs.withPackages</code> allows you to manage packages from ELPA. This means that you will not have to install that packages from within Emacs. For instance, if you wanted to use <code>company</code> <code>counsel</code>, <code>flycheck</code>, <code>ivy</code>, <code>magit</code>, <code>projectile</code>, and <code>use-package</code> you could use this as a <code>~/.config/nixpkgs/config.nix</code> override:</p> <pre><code>{\n  packageOverrides = pkgs: with pkgs; {\n    myEmacs = emacs.pkgs.withPackages (epkgs: (with epkgs.melpaStablePackages; [\n      company\n      counsel\n      flycheck\n      ivy\n      magit\n      projectile\n      use-package\n    ]));\n  }\n}\n</code></pre> <p>You can install it like any other packages via <code>nix-env -iA myEmacs</code>. However, this will only install those packages. It will not <code>configure</code> them for us. To do this, we need to provide a configuration file. Luckily, it is possible to do this from within Nix! By modifying the above example, we can make Emacs load a custom config file. The key is to create a package that provides a <code>default.el</code> file in <code>/share/emacs/site-start/</code>. Emacs knows to load this file automatically when it starts.</p> <pre><code>{\n  packageOverrides = pkgs: with pkgs; rec {\n    myEmacsConfig = writeText \"default.el\" ''\n      (eval-when-compile\n        (require 'use-package))\n\n      ;; load some packages\n\n      (use-package company\n        :bind (\"&lt;C-tab&gt;\" . company-complete)\n        :diminish company-mode\n        :commands (company-mode global-company-mode)\n        :defer 1\n        :config\n        (global-company-mode))\n\n      (use-package counsel\n        :commands (counsel-descbinds)\n        :bind (([remap execute-extended-command] . counsel-M-x)\n               (\"C-x C-f\" . counsel-find-file)\n               (\"C-c g\" . counsel-git)\n               (\"C-c j\" . counsel-git-grep)\n               (\"C-c k\" . counsel-ag)\n               (\"C-x l\" . counsel-locate)\n               (\"M-y\" . counsel-yank-pop)))\n\n      (use-package flycheck\n        :defer 2\n        :config (global-flycheck-mode))\n\n      (use-package ivy\n        :defer 1\n        :bind ((\"C-c C-r\" . ivy-resume)\n               (\"C-x C-b\" . ivy-switch-buffer)\n               :map ivy-minibuffer-map\n               (\"C-j\" . ivy-call))\n        :diminish ivy-mode\n        :commands ivy-mode\n        :config\n        (ivy-mode 1))\n\n      (use-package magit\n        :defer\n        :if (executable-find \"git\")\n        :bind ((\"C-x g\" . magit-status)\n               (\"C-x G\" . magit-dispatch-popup))\n        :init\n        (setq magit-completing-read-function 'ivy-completing-read))\n\n      (use-package projectile\n        :commands projectile-mode\n        :bind-keymap (\"C-c p\" . projectile-command-map)\n        :defer 5\n        :config\n        (projectile-global-mode))\n    '';\n\n    myEmacs = emacs.pkgs.withPackages (epkgs: (with epkgs.melpaStablePackages; [\n      (runCommand \"default.el\" {} ''\n         mkdir -p $out/share/emacs/site-lisp\n         cp ${myEmacsConfig} $out/share/emacs/site-lisp/default.el\n       '')\n      company\n      counsel\n      flycheck\n      ivy\n      magit\n      projectile\n      use-package\n    ]));\n  };\n}\n</code></pre> <p>This provides a fairly full Emacs start file. It will load in addition to the user's personal config. You can always disable it by passing <code>-q</code> to the Emacs command.</p> <p>Sometimes <code>emacs.pkgs.withPackages</code> is not enough, as this package set has some priorities imposed on packages (with the lowest priority assigned to GNU-devel ELPA, and the highest for packages manually defined in <code>pkgs/applications/editors/emacs/elisp-packages/manual-packages</code>). But you can't control these priorities when some package is installed as a dependency. You can override it on a per-package-basis, providing all the required dependencies manually, but it's tedious and there is always a possibility that an unwanted dependency will sneak in through some other package. To completely override such a package, you can use <code>overrideScope</code>.</p> <pre><code>overrides = self: super: rec {\n  haskell-mode = self.melpaPackages.haskell-mode;\n  ...\n};\n((emacsPackagesFor emacs).overrideScope overrides).withPackages\n  (p: with p; [\n    # here both these package will use haskell-mode of our own choice\n    ghc-mod\n    dante\n  ])\n</code></pre>"},{"location":"packages/etc-files.section.html","title":"/etc files","text":"<p>Certain calls in glibc require access to runtime files found in <code>/etc</code> such as <code>/etc/protocols</code> or <code>/etc/services</code> -- getprotobyname is one such function.</p> <p>On non-NixOS distributions these files are typically provided by packages (i.e., netbase) if not already pre-installed in your distribution. This can cause non-reproducibility for code if they rely on these files being present.</p> <p>If iana-etc is part of your <code>buildInputs</code>, then it will set the environment variables <code>NIX_ETC_PROTOCOLS</code> and <code>NIX_ETC_SERVICES</code> to the corresponding files in the package through a setup hook.</p> <pre><code>&gt; nix-shell -p iana-etc\n\n[nix-shell:~]$ env | grep NIX_ETC\nNIX_ETC_SERVICES=/nix/store/aj866hr8fad8flnggwdhrldm0g799ccz-iana-etc-20210225/etc/services\nNIX_ETC_PROTOCOLS=/nix/store/aj866hr8fad8flnggwdhrldm0g799ccz-iana-etc-20210225/etc/protocols\n</code></pre> <p>Nixpkg's version of glibc has been patched to check for the existence of these environment variables. If the environment variables are not set, then it will attempt to find the files at the default location within <code>/etc</code>.</p>"},{"location":"packages/firefox.section.html","title":"Firefox","text":""},{"location":"packages/firefox.section.html#build-wrapped-firefox-with-extensions-and-policies","title":"Build wrapped Firefox with extensions and policies","text":"<p>The <code>wrapFirefox</code> function allows to pass policies, preferences and extensions that are available to Firefox. With the help of <code>fetchFirefoxAddon</code> this allows to build a Firefox version that already comes with add-ons pre-installed:</p> <pre><code>{\n  # Nix firefox addons only work with the firefox-esr package.\n  myFirefox = wrapFirefox firefox-esr-unwrapped {\n    nixExtensions = [\n      (fetchFirefoxAddon {\n        name = \"ublock\"; # Has to be unique!\n        url = \"https://addons.mozilla.org/firefox/downloads/file/3679754/ublock_origin-1.31.0-an+fx.xpi\";\n        hash = \"sha256-2e73AbmYZlZXCP5ptYVcFjQYdjDp4iPoEPEOSCVF5sA=\";\n      })\n    ];\n\n    extraPolicies = {\n      CaptivePortal = false;\n      DisableFirefoxStudies = true;\n      DisablePocket = true;\n      DisableTelemetry = true;\n      DisableFirefoxAccounts = true;\n      FirefoxHome = {\n        Pocket = false;\n        Snippets = false;\n      };\n      UserMessaging = {\n        ExtensionRecommendations = false;\n        SkipOnboarding = true;\n      };\n      SecurityDevices = {\n        # Use a proxy module rather than `nixpkgs.config.firefox.smartcardSupport = true`\n        \"PKCS#11 Proxy Module\" = \"${pkgs.p11-kit}/lib/p11-kit-proxy.so\";\n      };\n    };\n\n    extraPrefs = ''\n      // Show more ssl cert infos\n      lockPref(\"security.identityblock.show_extended_validation\", true);\n    '';\n  };\n}\n</code></pre> <p>If <code>nixExtensions != null</code>, then all manually installed add-ons will be uninstalled from your browser profile. To view available enterprise policies, visit enterprise policies or type into the Firefox URL bar: <code>about:policies#documentation</code>. Nix installed add-ons do not have a valid signature, which is why signature verification is disabled. This does not compromise security because downloaded add-ons are checksummed and manual add-ons can't be installed. Also, make sure that the <code>name</code> field of <code>fetchFirefoxAddon</code> is unique. If you remove an add-on from the <code>nixExtensions</code> array, rebuild and start Firefox: the removed add-on will be completely removed with all of its settings.</p>"},{"location":"packages/firefox.section.html#sec-firefox-troubleshooting","title":"Troubleshooting","text":"<p>If add-ons are marked as broken or the signature is invalid, make sure you have Firefox ESR installed. Normal Firefox does not provide the ability anymore to disable signature verification for add-ons thus nix add-ons get disabled by the normal Firefox binary.</p> <p>If add-ons do not appear installed despite being defined in your nix configuration file, reset the local add-on state of your Firefox profile by clicking <code>Help -&gt; More Troubleshooting Information -&gt; Refresh Firefox</code>. This can happen if you switch from manual add-on mode to nix add-on mode and then back to manual mode and then again to nix add-on mode.</p>"},{"location":"packages/fish.section.html","title":"Fish","text":"<p>Fish is a \"smart and user-friendly command line shell\" with support for plugins.</p>"},{"location":"packages/fish.section.html#sec-fish-vendor","title":"Vendor Fish scripts","text":"<p>Any package may ship its own Fish completions, configuration snippets, and functions. Those should be installed to <code>$out/share/fish/vendor_{completions,conf,functions}.d</code> respectively.</p> <p>When the <code>programs.fish.enable</code> and <code>programs.fish.vendor.{completions,config,functions}.enable</code> options from the NixOS Fish module are set to true, those paths are symlinked in the current system environment and automatically loaded by Fish.</p>"},{"location":"packages/fish.section.html#sec-fish-plugins-pkg","title":"Packaging Fish plugins","text":"<p>While packages providing standalone executables belong to the top level, packages which have the sole purpose of extending Fish belong to the <code>fishPlugins</code> scope and should be registered in <code>pkgs/shells/fish/plugins/default.nix</code>.</p> <p>The <code>buildFishPlugin</code> utility function can be used to automatically copy Fish scripts from <code>$src/{completions,conf,conf.d,functions}</code> to the standard vendor installation paths. It also sets up the test environment so that the optional <code>checkPhase</code> is executed in a Fish shell with other already packaged plugins and package-local Fish functions specified in <code>checkPlugins</code> and <code>checkFunctionDirs</code> respectively.</p> <p>See <code>pkgs/shells/fish/plugins/pure.nix</code> for an example of Fish plugin package using <code>buildFishPlugin</code> and running unit tests with the <code>fishtape</code> test runner.</p>"},{"location":"packages/fish.section.html#sec-fish-wrapper","title":"Fish wrapper","text":"<p>The <code>wrapFish</code> package is a wrapper around Fish which can be used to create Fish shells initialized with some plugins as well as completions, configuration snippets and functions sourced from the given paths. This provides a convenient way to test Fish plugins and scripts without having to alter the environment.</p> <pre><code>wrapFish {\n  pluginPkgs = with fishPlugins; [ pure foreign-env ];\n  completionDirs = [];\n  functionDirs = [];\n  confDirs = [ \"/path/to/some/fish/init/dir/\" ];\n}\n</code></pre>"},{"location":"packages/fuse.section.html","title":"FUSE","text":"<p>Some packages rely on FUSE to provide support for additional filesystems not supported by the kernel.</p> <p>In general, FUSE software are primarily developed for Linux but many of them can also run on macOS. Nixpkgs supports FUSE packages on macOS, but it requires macFUSE to be installed outside of Nix. macFUSE currently isn't packaged in Nixpkgs mainly because it includes a kernel extension, which isn't supported by Nix outside of NixOS.</p> <p>If a package fails to run on macOS with an error message similar to the following, it's a likely sign that you need to have macFUSE installed.</p> <pre><code>dyld: Library not loaded: /usr/local/lib/libfuse.2.dylib\nReferenced from: /nix/store/w8bi72bssv0bnxhwfw3xr1mvn7myf37x-sshfs-fuse-2.10/bin/sshfs\nReason: image not found\n[1]    92299 abort      /nix/store/w8bi72bssv0bnxhwfw3xr1mvn7myf37x-sshfs-fuse-2.10/bin/sshfs\n</code></pre> <p>Package maintainers may often encounter the following error when building FUSE packages on macOS:</p> <pre><code>checking for fuse.h... no\nconfigure: error: No fuse.h found.\n</code></pre> <p>This happens on autoconf based projects that use <code>AC_CHECK_HEADERS</code> or <code>AC_CHECK_LIBS</code> to detect libfuse, and will occur even when the <code>fuse</code> package is included in <code>buildInputs</code>. It happens because libfuse headers throw an error on macOS if the <code>FUSE_USE_VERSION</code> macro is undefined. Many projects do define <code>FUSE_USE_VERSION</code>, but only inside C source files. This results in the above error at configure time because the configure script would attempt to compile sample FUSE programs without defining <code>FUSE_USE_VERSION</code>.</p> <p>There are two possible solutions for this problem in Nixpkgs:</p> <ol> <li>Pass <code>FUSE_USE_VERSION</code> to the configure script by adding    <code>CFLAGS=-DFUSE_USE_VERSION=25</code> in <code>configureFlags</code>. The actual value would    have to match the definition used in the upstream source code.</li> <li>Remove <code>AC_CHECK_HEADERS</code> / <code>AC_CHECK_LIBS</code> for libfuse.</li> </ol> <p>However, a better solution might be to fix the build script upstream to use <code>PKG_CHECK_MODULES</code> instead. This approach wouldn't suffer from the problem that <code>AC_CHECK_HEADERS</code>/<code>AC_CHECK_LIBS</code> has at the price of introducing a dependency on pkg-config.</p>"},{"location":"packages/ibus.section.html","title":"ibus-engines.typing-booster","text":"<p>This package is an ibus-based completion method to speed up typing.</p>"},{"location":"packages/ibus.section.html#sec-ibus-typing-booster-activate","title":"Activating the engine","text":"<p>IBus needs to be configured accordingly to activate <code>typing-booster</code>. The configuration depends on the desktop manager in use. For detailed instructions, please refer to the upstream docs.</p> <p>On NixOS, you need to explicitly enable <code>ibus</code> with given engines before customizing your desktop to use <code>typing-booster</code>. This can be achieved using the <code>ibus</code> module:</p> <pre><code>{ pkgs, ... }: {\n  i18n.inputMethod = {\n    enabled = \"ibus\";\n    ibus.engines = with pkgs.ibus-engines; [ typing-booster ];\n  };\n}\n</code></pre>"},{"location":"packages/ibus.section.html#sec-ibus-typing-booster-customize-hunspell","title":"Using custom hunspell dictionaries","text":"<p>The IBus engine is based on <code>hunspell</code> to support completion in many languages. By default, the dictionaries <code>de-de</code>, <code>en-us</code>, <code>fr-moderne</code> <code>es-es</code>, <code>it-it</code>, <code>sv-se</code> and <code>sv-fi</code> are in use. To add another dictionary, the package can be overridden like this:</p> <pre><code>ibus-engines.typing-booster.override { langs = [ \"de-at\" \"en-gb\" ]; }\n</code></pre> <p>Note: each language passed to <code>langs</code> must be an attribute name in <code>pkgs.hunspellDicts</code>.</p>"},{"location":"packages/ibus.section.html#sec-ibus-typing-booster-emoji-picker","title":"Built-in emoji picker","text":"<p>The <code>ibus-engines.typing-booster</code> package contains a program named <code>emoji-picker</code>. To display all emojis correctly, a special font such as <code>noto-fonts-color-emoji</code> is needed:</p> <p>On NixOS, it can be installed using the following expression:</p> <pre><code>{ pkgs, ... }: {\n  fonts.packages = with pkgs; [ noto-fonts-color-emoji ];\n}\n</code></pre>"},{"location":"packages/kakoune.section.html","title":"Kakoune","text":"<p>Kakoune can be built to autoload plugins:</p> <pre><code>(kakoune.override {\n  plugins = with pkgs.kakounePlugins; [ parinfer-rust ];\n})\n</code></pre>"},{"location":"packages/linux.section.html","title":"Linux kernel","text":"<p>The Nix expressions to build the Linux kernel are in <code>pkgs/os-specific/linux/kernel</code>.</p> <p>The function <code>pkgs.buildLinux</code> builds a kernel with common configuration values. This is the preferred option unless you have a very specific use case. Most kernels packaged in Nixpkgs are built that way, and it will also generate kernels suitable for NixOS. <code>pkgs.linuxManualConfig</code> requires a complete configuration to be passed. It has fewer additional features than <code>pkgs.buildLinux</code>, which provides common configuration values and exposes the <code>features</code> attribute, as explained below.</p> <p>Both functions have an argument <code>kernelPatches</code> which should be a list of <code>{name, patch, extraConfig}</code> attribute sets, where <code>name</code> is the name of the patch (which is included in the kernel\u2019s <code>meta.description</code> attribute), <code>patch</code> is the patch itself (possibly compressed), and <code>extraConfig</code> (optional) is a string specifying extra options to be concatenated to the kernel configuration file (<code>.config</code>).</p> <p>The kernel derivation created with <code>pkgs.buildLinux</code> exports an attribute <code>features</code> specifying whether optional functionality is or isn\u2019t enabled. This is used in NixOS to implement kernel-specific behaviour.</p> <p>If you are using a kernel packaged in Nixpkgs, you can customize it by overriding its arguments. For details on how each argument affects the generated kernel, refer to the <code>pkgs.buildLinux</code> source code.</p> <p>:::{.example #ex-overriding-kernel-derivation}</p>"},{"location":"packages/linux.section.html#overriding-the-kernel-derivation","title":"Overriding the kernel derivation","text":"<p>Assuming you are using the kernel from <code>pkgs.linux_latest</code>:</p> <pre><code>pkgs.linux_latest.override {\n  ignoreConfigErrors = true;\n  autoModules = false;\n  kernelPreferBuiltin = true;\n  extraStructuredConfig = with lib.kernel; {\n    DEBUG_KERNEL = yes;\n    FRAME_POINTER = yes;\n    KGDB = yes;\n    KGDB_SERIAL_CONSOLE = yes;\n    DEBUG_INFO = yes;\n  };\n}\n</code></pre> <p>:::</p>"},{"location":"packages/linux.section.html#sec-manual-kernel-configuration","title":"Manual kernel configuration","text":"<p>Sometimes it may not be desirable to use kernels built with <code>pkgs.buildLinux</code>, especially if most of the common configuration has to be altered or disabled to achieve a kernel as expected by the target use case. An example of this is building a kernel for use in a VM or micro VM. You can use <code>pkgs.linuxManualConfig</code> in these cases. It requires the <code>src</code>, <code>version</code>, and <code>configfile</code> attributes to be specified.</p> <p>:::{.example #ex-using-linux-manual-config}</p>"},{"location":"packages/linux.section.html#using-pkgslinuxmanualconfig-with-a-specific-source-version-and-config-file","title":"Using <code>pkgs.linuxManualConfig</code> with a specific source, version, and config file","text":"<pre><code>{ pkgs, ... }: {\n  version = \"6.1.55\";\n  src = pkgs.fetchurl {\n    url = \"https://cdn.kernel.org/pub/linux/kernel/v6.x/linux-${version}.tar.xz\";\n    hash = \"sha256:1h0mzx52q9pvdv7rhnvb8g68i7bnlc9rf8gy9qn4alsxq4g28zm8\";\n  };\n  configfile = ./path_to_config_file;\n  linux = pkgs.linuxManualConfig {\n    inherit version src configfile;\n    allowImportFromDerivation = true;\n  };\n}\n</code></pre> <p>If necessary, the version string can be slightly modified to explicitly mark it as a custom version. If you do so, ensure the <code>modDirVersion</code> attribute matches the source's version, otherwise the build will fail.</p> <pre><code>{ pkgs, ... }: {\n  version = \"6.1.55-custom\";\n  modDirVersion = \"6.1.55\";\n  src = pkgs.fetchurl {\n    url = \"https://cdn.kernel.org/pub/linux/kernel/v6.x/linux-${modDirVersion}.tar.xz\";\n    hash = \"sha256:1h0mzx52q9pvdv7rhnvb8g68i7bnlc9rf8gy9qn4alsxq4g28zm8\";\n  };\n  configfile = ./path_to_config_file;\n  linux = pkgs.linuxManualConfig {\n    inherit version modDirVersion src configfile;\n    allowImportFromDerivation = true;\n  };\n}\n</code></pre> <p>:::</p> <p>Additional attributes can be used with <code>linuxManualConfig</code> for further customisation. You're encouraged to read the <code>pkgs.linuxManualConfig</code> source code to understand how to use them.</p> <p>To edit the <code>.config</code> file for Linux X.Y from within Nix, proceed as follows:</p> <pre><code>$ nix-shell '&lt;nixpkgs&gt;' -A linuxKernel.kernels.linux_X_Y.configEnv\n$ unpackPhase\n$ cd linux-*\n$ make nconfig\n</code></pre>"},{"location":"packages/linux.section.html#sec-linux-kernel-developing-modules","title":"Developing kernel modules","text":"<p>When developing kernel modules it's often convenient to run the edit-compile-run loop as quickly as possible. See the snippet below as an example.</p> <p>:::{.example #ex-edit-compile-run-kernel-modules}</p>"},{"location":"packages/linux.section.html#edit-compile-run-loop-when-developing-mellanox-drivers","title":"Edit-compile-run loop when developing <code>mellanox</code> drivers","text":"<pre><code>$ nix-build '&lt;nixpkgs&gt;' -A linuxPackages.kernel.dev\n$ nix-shell '&lt;nixpkgs&gt;' -A linuxPackages.kernel\n$ unpackPhase\n$ cd linux-*\n$ make -C $dev/lib/modules/*/build M=$(pwd)/drivers/net/ethernet/mellanox modules\n# insmod ./drivers/net/ethernet/mellanox/mlx5/core/mlx5_core.ko\n</code></pre> <p>:::</p>"},{"location":"packages/locales.section.html","title":"Locales","text":"<p>To allow simultaneous use of packages linked against different versions of <code>glibc</code> with different locale archive formats, Nixpkgs patches <code>glibc</code> to rely on <code>LOCALE_ARCHIVE</code> environment variable.</p> <p>On non-NixOS distributions, this variable is obviously not set. This can cause regressions in language support or even crashes in some Nixpkgs-provided programs. The simplest way to mitigate this problem is exporting the <code>LOCALE_ARCHIVE</code> variable pointing to <code>${glibcLocales}/lib/locale/locale-archive</code>. The drawback (and the reason this is not the default) is the relatively large (a hundred MiB) size of the full set of locales. It is possible to build a custom set of locales by overriding parameters <code>allLocales</code> and <code>locales</code> of the package.</p>"},{"location":"packages/nginx.section.html","title":"Nginx","text":"<p>Nginx is a reverse proxy and lightweight webserver.</p>"},{"location":"packages/nginx.section.html#sec-nginx-etag","title":"ETags on static files served from the Nix store","text":"<p>HTTP has a couple of different mechanisms for caching to prevent clients from having to download the same content repeatedly if a resource has not changed since the last time it was requested. When nginx is used as a server for static files, it implements the caching mechanism based on the <code>Last-Modified</code> response header automatically; unfortunately, it works by using filesystem timestamps to determine the value of the <code>Last-Modified</code> header. This doesn't give the desired behavior when the file is in the Nix store because all file timestamps are set to 0 (for reasons related to build reproducibility).</p> <p>Fortunately, HTTP supports an alternative (and more effective) caching mechanism: the <code>ETag</code> response header. The value of the <code>ETag</code> header specifies some identifier for the particular content that the server is sending (e.g., a hash). When a client makes a second request for the same resource, it sends that value back in an <code>If-None-Match</code> header. If the ETag value is unchanged, then the server does not need to resend the content.</p> <p>As of NixOS 19.09, the nginx package in Nixpkgs is patched such that when nginx serves a file out of <code>/nix/store</code>, the hash in the store path is used as the <code>ETag</code> header in the HTTP response, thus providing proper caching functionality. With NixOS 24.05 and later, the <code>ETag</code> additionally includes the response content length, to ensure files served with static compression do not share <code>ETag</code>s with their uncompressed version. This <code>ETag</code> functionality is enabled automatically; you do not need to do modify any configuration to get this behavior.</p>"},{"location":"packages/opengl.section.html","title":"OpenGL","text":"<p>OpenGL support varies depending on which hardware is used and which drivers are available and loaded.</p> <p>Broadly, we support both GL vendors: Mesa and NVIDIA.</p>"},{"location":"packages/opengl.section.html#nixos-desktop","title":"NixOS Desktop","text":"<p>The NixOS desktop or other non-headless configurations are the primary target for OpenGL libraries and applications. The current solution for discovering which drivers are available is based on libglvnd. <code>libglvnd</code> performs \"vendor-neutral dispatch\", trying a variety of techniques to find the system's GL implementation. In practice, this will be either via standard GLX for X11 users or EGL for Wayland users, and supporting either NVIDIA or Mesa extensions.</p>"},{"location":"packages/opengl.section.html#nix-on-gnulinux","title":"Nix on GNU/Linux","text":"<p>If you are using a non-NixOS GNU/Linux/X11 desktop with free software video drivers, consider launching OpenGL-dependent programs from Nixpkgs with Nixpkgs versions of <code>libglvnd</code> and <code>mesa.drivers</code> in <code>LD_LIBRARY_PATH</code>. For Mesa drivers, the Linux kernel version doesn't have to match nixpkgs.</p> <p>For proprietary video drivers, you might have luck with also adding the corresponding video driver package.</p>"},{"location":"packages/shell-helpers.section.html","title":"Interactive shell helpers","text":"<p>Some packages provide the shell integration to be more useful. But unlike other systems, nix doesn't have a standard <code>share</code> directory location. This is why a bunch <code>PACKAGE-share</code> scripts are shipped that print the location of the corresponding shared folder. Current list of such packages is as following:</p> <ul> <li><code>fzf</code> : <code>fzf-share</code></li> </ul> <p>E.g. <code>fzf</code> can then be used in the <code>.bashrc</code> like this:</p> <pre><code>source \"$(fzf-share)/completion.bash\"\nsource \"$(fzf-share)/key-bindings.bash\"\n</code></pre>"},{"location":"packages/steam.section.html","title":"Steam","text":""},{"location":"packages/steam.section.html#sec-steam-nix","title":"Steam in Nix","text":"<p>Steam is distributed as a <code>.deb</code> file, for now only as an i686 package (the amd64 package only has documentation). When unpacked, it has a script called <code>steam</code> that in Ubuntu (their target distro) would go to <code>/usr/bin</code>. When run for the first time, this script copies some files to the user's home, which include another script that is the ultimate responsible for launching the steam binary, which is also in <code>$HOME</code>.</p> <p>Nix problems and constraints:</p> <ul> <li>We don't have <code>/bin/bash</code> and many scripts point there. Same thing for <code>/usr/bin/python</code>.</li> <li>We don't have the dynamic loader in <code>/lib</code>.</li> <li>The <code>steam.sh</code> script in <code>$HOME</code> cannot be patched, as it is checked and rewritten by steam.</li> <li>The steam binary cannot be patched, it's also checked.</li> </ul> <p>The current approach to deploy Steam in NixOS is composing a FHS-compatible chroot environment, as documented here. This allows us to have binaries in the expected paths without disrupting the system, and to avoid patching them to work in a non FHS environment.</p>"},{"location":"packages/steam.section.html#sec-steam-play","title":"How to play","text":"<p>Use <code>programs.steam.enable = true;</code> if you want to add steam to <code>systemPackages</code> and also enable a few workarounds as well as Steam controller support or other Steam supported controllers such as the DualShock 4 or Nintendo Switch Pro Controller.</p>"},{"location":"packages/steam.section.html#sec-steam-troub","title":"Troubleshooting","text":"<ul> <li>Steam fails to start. What do I do?</li> </ul> <p>Try to run</p> <pre><code>strace steam\n</code></pre> <p>to see what is causing steam to fail.</p> <ul> <li> <p>Using the FOSS Radeon or nouveau (nvidia) drivers</p> </li> <li> <p>The <code>newStdcpp</code> parameter was removed since NixOS 17.09 and should not be needed anymore.</p> </li> <li> <p>Steam ships statically linked with a version of <code>libcrypto</code> that conflicts with the one dynamically loaded by radeonsi_dri.so. If you get the error:</p> <pre><code>steam.sh: line 713: 7842 Segmentation fault (core dumped)\n</code></pre> <p>have a look at this pull request.</p> </li> <li> <p>Java</p> </li> <li> <p>There is no java in steam chrootenv by default. If you get a message like:</p> <pre><code>/home/foo/.local/share/Steam/SteamApps/common/towns/towns.sh: line 1: java: command not found\n</code></pre> <p>you need to add:</p> <pre><code>steam.override { withJava = true; };\n</code></pre> </li> </ul>"},{"location":"packages/steam.section.html#sec-steam-run","title":"steam-run","text":"<p>The FHS-compatible chroot used for Steam can also be used to run other Linux games that expect a FHS environment. To use it, install the <code>steam-run</code> package and run the game with:</p> <pre><code>steam-run ./foo\n</code></pre>"},{"location":"packages/urxvt.section.html","title":"Urxvt","text":"<p>Urxvt, also known as rxvt-unicode, is a highly customizable terminal emulator.</p>"},{"location":"packages/urxvt.section.html#sec-urxvt-conf","title":"Configuring urxvt","text":"<p>In <code>nixpkgs</code>, urxvt is provided by the package <code>rxvt-unicode</code>. It can be configured to include your choice of plugins, reducing its closure size from the default configuration which includes all available plugins. To make use of this functionality, use an overlay or directly install an expression that overrides its configuration, such as:</p> <pre><code>rxvt-unicode.override {\n  configure = { availablePlugins, ... }: {\n    plugins = with availablePlugins; [ perls resize-font vtwheel ];\n  };\n}\n</code></pre> <p>If the <code>configure</code> function returns an attrset without the <code>plugins</code> attribute, <code>availablePlugins</code> will be used automatically.</p> <p>In order to add plugins but also keep all default plugins installed, it is possible to use the following method:</p> <pre><code>rxvt-unicode.override {\n  configure = { availablePlugins, ... }: {\n    plugins = (builtins.attrValues availablePlugins) ++ [ custom-plugin ];\n  };\n}\n</code></pre> <p>To get a list of all the plugins available, open the Nix REPL and run</p> <pre><code>$ nix repl\n:l &lt;nixpkgs&gt;\nmap (p: p.name) pkgs.rxvt-unicode.plugins\n</code></pre> <p>Alternatively, if your shell is bash or zsh and have completion enabled, type <code>nixpkgs.rxvt-unicode.plugins.&lt;tab&gt;</code>.</p> <p>In addition to <code>plugins</code> the options <code>extraDeps</code> and <code>perlDeps</code> can be used to install extra packages. <code>extraDeps</code> can be used, for example, to provide <code>xsel</code> (a clipboard manager) to the clipboard plugin, without installing it globally:</p> <pre><code>rxvt-unicode.override {\n  configure = { availablePlugins, ... }: {\n    pluginsDeps = [ xsel ];\n  };\n}\n</code></pre> <p><code>perlDeps</code> is a handy way to provide Perl packages to your custom plugins (in <code>$HOME/.urxvt/ext</code>). For example, if you need <code>AnyEvent</code> you can do:</p> <pre><code>rxvt-unicode.override {\n  configure = { availablePlugins, ... }: {\n    perlDeps = with perlPackages; [ AnyEvent ];\n  };\n}\n</code></pre>"},{"location":"packages/urxvt.section.html#sec-urxvt-pkg","title":"Packaging urxvt plugins","text":"<p>Urxvt plugins resides in <code>pkgs/applications/misc/rxvt-unicode-plugins</code>. To add a new plugin, create an expression in a subdirectory and add the package to the set in <code>pkgs/applications/misc/rxvt-unicode-plugins/default.nix</code>.</p> <p>A plugin can be any kind of derivation, the only requirement is that it should always install perl scripts in <code>$out/lib/urxvt/perl</code>. Look for existing plugins for examples.</p> <p>If the plugin is itself a Perl package that needs to be imported from other plugins or scripts, add the following passthrough:</p> <pre><code>passthru.perlPackages = [ \"self\" ];\n</code></pre> <p>This will make the urxvt wrapper pick up the dependency and set up the Perl path accordingly.</p>"},{"location":"packages/weechat.section.html","title":"WeeChat","text":"<p>WeeChat can be configured to include your choice of plugins, reducing its closure size from the default configuration which includes all available plugins. To make use of this functionality, install an expression that overrides its configuration, such as:</p> <pre><code>weechat.override {configure = {availablePlugins, ...}: {\n    plugins = with availablePlugins; [ python perl ];\n  }\n}\n</code></pre> <p>If the <code>configure</code> function returns an attrset without the <code>plugins</code> attribute, <code>availablePlugins</code> will be used automatically.</p> <p>The plugins currently available are <code>python</code>, <code>perl</code>, <code>ruby</code>, <code>guile</code>, <code>tcl</code> and <code>lua</code>.</p> <p>The Python and Perl plugins allows the addition of extra libraries. For instance, the <code>inotify.py</code> script in <code>weechat-scripts</code> requires D-Bus or libnotify, and the <code>fish.py</code> script requires <code>pycrypto</code>. To use these scripts, use the plugin's <code>withPackages</code> attribute:</p> <pre><code>weechat.override { configure = {availablePlugins, ...}: {\n    plugins = with availablePlugins; [\n            (python.withPackages (ps: with ps; [ pycrypto python-dbus ]))\n        ];\n    };\n}\n</code></pre> <p>In order to also keep all default plugins installed, it is possible to use the following method:</p> <pre><code>weechat.override { configure = { availablePlugins, ... }: {\n  plugins = builtins.attrValues (availablePlugins // {\n    python = availablePlugins.python.withPackages (ps: with ps; [ pycrypto python-dbus ]);\n  });\n}; }\n</code></pre> <p>WeeChat allows to set defaults on startup using the <code>--run-command</code>. The <code>configure</code> method can be used to pass commands to the program:</p> <pre><code>weechat.override {\n  configure = { availablePlugins, ... }: {\n    init = ''\n      /set foo bar\n      /server add libera irc.libera.chat\n    '';\n  };\n}\n</code></pre> <p>Further values can be added to the list of commands when running <code>weechat --run-command \"your-commands\"</code>.</p> <p>Additionally, it's possible to specify scripts to be loaded when starting <code>weechat</code>. These will be loaded before the commands from <code>init</code>:</p> <pre><code>weechat.override {\n  configure = { availablePlugins, ... }: {\n    scripts = with pkgs.weechatScripts; [\n      weechat-xmpp weechat-matrix-bridge wee-slack\n    ];\n    init = ''\n      /set plugins.var.python.jabber.key \"val\"\n    '':\n  };\n}\n</code></pre> <p>In <code>nixpkgs</code> there's a subpackage which contains derivations for WeeChat scripts. Such derivations expect a <code>passthru.scripts</code> attribute, which contains a list of all scripts inside the store path. Furthermore, all scripts have to live in <code>$out/share</code>. An exemplary derivation looks like this:</p> <pre><code>{ stdenv, fetchurl }:\n\nstdenv.mkDerivation {\n  name = \"exemplary-weechat-script\";\n  src = fetchurl {\n    url = \"https://scripts.tld/your-scripts.tar.gz\";\n    hash = \"...\";\n  };\n  passthru.scripts = [ \"foo.py\" \"bar.lua\" ];\n  installPhase = ''\n    mkdir $out/share\n    cp foo.py $out/share\n    cp bar.lua $out/share\n  '';\n}\n</code></pre>"},{"location":"packages/xorg.section.html","title":"X.org","text":"<p>The Nix expressions for the X.org packages reside in <code>pkgs/servers/x11/xorg/default.nix</code>. This file is automatically generated from lists of tarballs in an X.org release. As such it should not be modified directly; rather, you should modify the lists, the generator script or the file <code>pkgs/servers/x11/xorg/overrides.nix</code>, in which you can override or add to the derivations produced by the generator.</p>"},{"location":"packages/xorg.section.html#katamari-tarballs","title":"Katamari Tarballs","text":"<p>X.org upstream releases used to include katamari releases, which included a holistic recommended version for each tarball, up until 7.7. To create a list of tarballs in a katamari release:</p> <pre><code>export release=\"X11R7.7\"\nexport url=\"mirror://xorg/$release/src/everything/\"\ncat $(PRINT_PATH=1 nix-prefetch-url $url | tail -n 1) \\\n  | perl -e 'while (&lt;&gt;) { if (/(href|HREF)=\"([^\"]*.bz2)\"/) { print \"$ENV{'url'}$2\\n\"; }; }' \\\n  | sort &gt; \"tarballs-$release.list\"\n</code></pre>"},{"location":"packages/xorg.section.html#individual-tarballs","title":"Individual Tarballs","text":"<p>The upstream release process for X11R7.8 does not include a planned katamari. Instead, each component of X.org is released as its own tarball. We maintain <code>pkgs/servers/x11/xorg/tarballs.list</code> as a list of tarballs for each individual package. This list includes X.org core libraries and protocol descriptions, extra newer X11 interface libraries, like <code>xorg.libxcb</code>, and classic utilities which are largely unused but still available if needed, like <code>xorg.imake</code>.</p>"},{"location":"packages/xorg.section.html#generating-nix-expressions","title":"Generating Nix Expressions","text":"<p>The generator is invoked as follows:</p> <pre><code>cd pkgs/servers/x11/xorg\n&lt;tarballs.list perl ./generate-expr-from-tarballs.pl\n</code></pre> <p>For each of the tarballs in the <code>.list</code> files, the script downloads it, unpacks it, and searches its <code>configure.ac</code> and <code>*.pc.in</code> files for dependencies. This information is used to generate <code>default.nix</code>. The generator caches downloaded tarballs between runs. Pay close attention to the <code>NOT FOUND: $NAME</code> messages at the end of the run, since they may indicate missing dependencies. (Some might be optional dependencies, however.)</p>"},{"location":"packages/xorg.section.html#overriding-the-generator","title":"Overriding the Generator","text":"<p>If the expression for a package requires derivation attributes that the generator cannot figure out automatically (say, <code>patches</code> or a <code>postInstall</code> hook), you should modify <code>pkgs/servers/x11/xorg/overrides.nix</code>.</p>"},{"location":"stdenv/cross-compilation.chapter.html","title":"Cross-compilation","text":""},{"location":"stdenv/cross-compilation.chapter.html#sec-cross-intro","title":"Introduction","text":"<p>\"Cross-compilation\" means compiling a program on one machine for another type of machine. For example, a typical use of cross-compilation is to compile programs for embedded devices. These devices often don't have the computing power and memory to compile their own programs. One might think that cross-compilation is a fairly niche concern. However, there are significant advantages to rigorously distinguishing between build-time and run-time environments! Significant, because the benefits apply even when one is developing and deploying on the same machine. Nixpkgs is increasingly adopting the opinion that packages should be written with cross-compilation in mind, and Nixpkgs should evaluate in a similar way (by minimizing cross-compilation-specific special cases) whether or not one is cross-compiling.</p> <p>This chapter will be organized in three parts. First, it will describe the basics of how to package software in a way that supports cross-compilation. Second, it will describe how to use Nixpkgs when cross-compiling. Third, it will describe the internal infrastructure supporting cross-compilation.</p>"},{"location":"stdenv/cross-compilation.chapter.html#sec-cross-packaging","title":"Packaging in a cross-friendly manner","text":""},{"location":"stdenv/cross-compilation.chapter.html#ssec-cross-platform-parameters","title":"Platform parameters","text":"<p>Nixpkgs follows the conventions of GNU autoconf. We distinguish between 3 types of platforms when building a derivation: build, host, and target. In summary, build is the platform on which a package is being built, host is the platform on which it will run. The third attribute, target, is relevant only for certain specific compilers and build tools.</p> <p>In Nixpkgs, these three platforms are defined as attribute sets under the names <code>buildPlatform</code>, <code>hostPlatform</code>, and <code>targetPlatform</code>. They are always defined as attributes in the standard environment. That means one can access them like:</p> <pre><code>{ stdenv, fooDep, barDep, ... }: ...stdenv.buildPlatform...\n</code></pre> <p><code>buildPlatform</code></p> <p>: The \"build platform\" is the platform on which a package is built. Once someone has a built package, or pre-built binary package, the build platform should not matter and can be ignored.</p> <p><code>hostPlatform</code></p> <p>: The \"host platform\" is the platform on which a package will be run. This is the simplest platform to understand, but also the one with the worst name.</p> <p><code>targetPlatform</code></p> <p>: The \"target platform\" attribute is, unlike the other two attributes, not actually fundamental to the process of building software. Instead, it is only relevant for compatibility with building certain specific compilers and build tools. It can be safely ignored for all other packages.</p> <p>: The build process of certain compilers is written in such a way that the compiler resulting from a single build can itself only produce binaries for a single platform. The task of specifying this single \"target platform\" is thus pushed to build time of the compiler. The root cause of this is that the compiler (which will be run on the host) and the standard library/runtime (which will be run on the target) are built by a single build process.</p> <p>: There is no fundamental need to think about a single target ahead of time like this. If the tool supports modular or pluggable backends, both the need to specify the target at build time and the constraint of having only a single target disappear. An example of such a tool is LLVM.</p> <p>: Although the existence of a \"target platform\" is arguably a historical mistake, it is a common one: examples of tools that suffer from it are GCC, Binutils, GHC and Autoconf. Nixpkgs tries to avoid sharing in the mistake where possible. Still, because the concept of a target platform is so ingrained, it is best to support it as is.</p> <p>The exact schema these fields follow is a bit ill-defined due to a long and convoluted evolution, but this is slowly being cleaned up. You can see examples of ones used in practice in <code>lib.systems.examples</code>; note how they are not all very consistent. For now, here are few fields can count on them containing:</p> <p><code>system</code></p> <p>: This is a two-component shorthand for the platform. Examples of this would be \"x86_64-darwin\" and \"i686-linux\"; see <code>lib.systems.doubles</code> for more. The first component corresponds to the CPU architecture of the platform and the second to the operating system of the platform (<code>[cpu]-[os]</code>). This format has built-in support in Nix, such as the <code>builtins.currentSystem</code> impure string.</p> <p><code>config</code></p> <p>: This is a 3- or 4- component shorthand for the platform. Examples of this would be <code>x86_64-unknown-linux-gnu</code> and <code>aarch64-apple-darwin14</code>. This is a standard format called the \"LLVM target triple\", as they are pioneered by LLVM. In the 4-part form, this corresponds to <code>[cpu]-[vendor]-[os]-[abi]</code>. This format is strictly more informative than the \"Nix host double\", as the previous format could analogously be termed. This needs a better name than <code>config</code>!</p> <p><code>parsed</code></p> <p>: This is a Nix representation of a parsed LLVM target triple with white-listed components. This can be specified directly, or actually parsed from the <code>config</code>. See <code>lib.systems.parse</code> for the exact representation.</p> <p><code>libc</code></p> <p>: This is a string identifying the standard C library used. Valid identifiers include \"glibc\" for GNU libc, \"libSystem\" for Darwin's Libsystem, and \"uclibc\" for \u00b5Clibc. It should probably be refactored to use the module system, like <code>parse</code>.</p> <p><code>is*</code></p> <p>: These predicates are defined in <code>lib.systems.inspect</code>, and slapped onto every platform. They are superior to the ones in <code>stdenv</code> as they force the user to be explicit about which platform they are inspecting. Please use these instead of those.</p> <p><code>platform</code></p> <p>: This is, quite frankly, a dumping ground of ad-hoc settings (it's an attribute set). See <code>lib.systems.platforms</code> for examples\u2014there's hopefully one in there that will work verbatim for each platform that is working. Please help us triage these flags and give them better homes!</p>"},{"location":"stdenv/cross-compilation.chapter.html#ssec-cross-dependency-categorization","title":"Theory of dependency categorization","text":"<p>::: {.note} This is a rather philosophical description that isn't very Nixpkgs-specific. For an overview of all the relevant attributes given to <code>mkDerivation</code>, see . For a description of how everything is implemented, see . :::</p> <p>In this section we explore the relationship between both runtime and build-time dependencies and the 3 Autoconf platforms.</p> <p>A run time dependency between two packages requires that their host platforms match. This is directly implied by the meaning of \"host platform\" and \"runtime dependency\": The package dependency exists while both packages are running on a single host platform.</p> <p>A build time dependency, however, has a shift in platforms between the depending package and the depended-on package. \"build time dependency\" means that to build the depending package we need to be able to run the depended-on's package. The depending package's build platform is therefore equal to the depended-on package's host platform.</p> <p>If both the dependency and depending packages aren't compilers or other machine-code-producing tools, we're done. And indeed <code>buildInputs</code> and <code>nativeBuildInputs</code> have covered these simpler cases for many years. But if the dependency does produce machine code, we might need to worry about its target platform too. In principle, that target platform might be any of the depending package's build, host, or target platforms, but we prohibit dependencies from a \"later\" platform to an earlier platform to limit confusion because we've never seen a legitimate use for them.</p> <p>Finally, if the depending package is a compiler or other machine-code-producing tool, it might need dependencies that run at \"emit time\". This is for compilers that (regrettably) insist on being built together with their source languages' standard libraries. Assuming build != host != target, a run-time dependency of the standard library cannot be run at the compiler's build time or run time, but only at the run time of code emitted by the compiler.</p> <p>Putting this all together, that means that we have dependency types of the form \"X\u2192 E\", which means that the dependency executes on X and emits code for E; each of X and E can be <code>build</code>, <code>host</code>, or <code>target</code>, and E can be <code>*</code> to indicate that the dependency is not a compiler-like package.</p> <p>Dependency types describe the relationships that a package has with each of its transitive dependencies.  You could think of attaching one or more dependency types to each of the formal parameters at the top of a package's <code>.nix</code> file, as well as to all of their formal parameters, and so on.   Triples like <code>(foo, bar, baz)</code>, on the other hand, are a property of an instantiated derivation -- you could would attach a triple <code>(mips-linux, mips-linux, sparc-solaris)</code> to a <code>.drv</code> file in <code>/nix/store</code>.</p> <p>Only nine dependency types matter in practice:</p>"},{"location":"stdenv/cross-compilation.chapter.html#possible-dependency-types","title":"Possible dependency types","text":"Dependency type Dependency\u2019s host platform Dependency\u2019s target platform build \u2192 * build (none) build \u2192 build build build build \u2192 host build host build \u2192 target build target host \u2192 * host (none) host \u2192 host host host host \u2192 target host target target \u2192 * target (none) target \u2192 target target target <p>Let's use <code>g++</code> as an example to make this table clearer.  <code>g++</code> is a C++ compiler written in C.  Suppose we are building <code>g++</code> with a <code>(build, host, target)</code> platform triple of <code>(foo, bar, baz)</code>.  This means we are using a <code>foo</code>-machine to build a copy of <code>g++</code> which will run on a <code>bar</code>-machine and emit binaries for the <code>baz</code>-machine.</p> <ul> <li> <p><code>g++</code> links against the host platform's <code>glibc</code> C library, which is a \"host\u2192 *\" dependency with a triple of <code>(bar, bar, *)</code>.  Since it is a library, not a compiler, it has no \"target\".</p> </li> <li> <p>Since <code>g++</code> is written in C, the <code>gcc</code> compiler used to compile it is a \"build\u2192 host\" dependency of <code>g++</code> with a triple of <code>(foo, foo, bar)</code>.  This compiler runs on the build platform and emits code for the host platform.</p> </li> <li> <p><code>gcc</code> links against the build platform's <code>glibc</code> C library, which is a \"build\u2192 *\" dependency with a triple of <code>(foo, foo, *)</code>.  Since it is a library, not a compiler, it has no \"target\".</p> </li> <li> <p>This <code>gcc</code> is itself compiled by an earlier copy of <code>gcc</code>.  This earlier copy of <code>gcc</code> is a \"build\u2192 build\" dependency of <code>g++</code> with a triple of <code>(foo, foo, foo)</code>.  This \"early <code>gcc</code>\" runs on the build platform and emits code for the build platform.</p> </li> <li> <p><code>g++</code> is bundled with <code>libgcc</code>, which includes a collection of target-machine routines for exception handling and software floating point emulation.  <code>libgcc</code> would be a \"target\u2192 *\" dependency with triple <code>(foo, baz, *)</code>, because it consists of machine code which gets linked against the output of the compiler that we are building.  It is a library, not a compiler, so it has no target of its own.</p> </li> <li> <p><code>libgcc</code> is written in C and compiled with <code>gcc</code>.  The <code>gcc</code> that compiles it will be a \"build\u2192 target\" dependency with triple <code>(foo, foo, baz)</code>.  It gets compiled and run at <code>g++</code>-build-time (on platform <code>foo</code>), but must emit code for the <code>baz</code>-platform.</p> </li> <li> <p><code>g++</code> allows inline assembler code, so it depends on access to a copy of the <code>gas</code> assembler.  This would be a \"host\u2192 target\" dependency with triple <code>(foo, bar, baz)</code>.</p> </li> <li> <p><code>g++</code> (and <code>gcc</code>) include a library <code>libgccjit.so</code>, which wrap the compiler in a library to create a just-in-time compiler.  In nixpkgs, this library is in the <code>libgccjit</code> package; if C++ required that programs have access to a JIT, <code>g++</code> would need to add a \"target\u2192 target\" dependency for <code>libgccjit</code> with triple <code>(foo, baz, baz)</code>.  This would ensure that the compiler ships with a copy of <code>libgccjit</code> which both executes on and generates code for the <code>baz</code>-platform.</p> </li> <li> <p>If <code>g++</code> itself linked against <code>libgccjit.so</code> (for example, to allow compile-time-evaluated C++ expressions), then the <code>libgccjit</code> package used to provide this functionality would be a \"host\u2192 host\" dependency of <code>g++</code>: it is code which runs on the <code>host</code> and emits code for execution on the <code>host</code>.</p> </li> </ul>"},{"location":"stdenv/cross-compilation.chapter.html#ssec-cross-cookbook","title":"Cross packaging cookbook","text":"<p>Some frequently encountered problems when packaging for cross-compilation should be answered here. Ideally, the information above is exhaustive, so this section cannot provide any new information, but it is ludicrous and cruel to expect everyone to spend effort working through the interaction of many features just to figure out the same answer to the same common problem. Feel free to add to this list!</p>"},{"location":"stdenv/cross-compilation.chapter.html#cross-qa-fails-to-find-binutils","title":"My package fails to find a binutils command (<code>cc</code>/<code>ar</code>/<code>ld</code> etc.)","text":"<p>Many packages assume that an unprefixed binutils (<code>cc</code>/<code>ar</code>/<code>ld</code> etc.) is available, but Nix doesn't provide one. It only provides a prefixed one, just as it only does for all the other binutils programs. It may be necessary to patch the package to fix the build system to use a prefix. For instance, instead of <code>cc</code>, use <code>${stdenv.cc.targetPrefix}cc</code>.</p> <pre><code>makeFlags = [ \"CC=${stdenv.cc.targetPrefix}cc\" ];\n</code></pre>"},{"location":"stdenv/cross-compilation.chapter.html#cross-qa-avoid-compiling-gcc-cross-compiler","title":"How do I avoid compiling a GCC cross-compiler from source?","text":"<p>On less powerful machines, it can be inconvenient to cross-compile a package only to find out that GCC has to be compiled from source, which could take up to several hours. Nixpkgs maintains a limited cross-related jobset on Hydra, which tests cross-compilation to various platforms from build platforms \"x86_64-darwin\", \"x86_64-linux\", and \"aarch64-linux\".  See <code>pkgs/top-level/release-cross.nix</code> for the full list of target platforms and packages.  For instance, the following invocation fetches the pre-built cross-compiled GCC for <code>armv6l-unknown-linux-gnueabihf</code> and builds GNU Hello from source.</p> <pre><code>$ nix-build '&lt;nixpkgs&gt;' -A pkgsCross.raspberryPi.hello\n</code></pre>"},{"location":"stdenv/cross-compilation.chapter.html#cross-qa-build-c-program-in-build-environment","title":"What if my package\u2019s build system needs to build a C program to be run under the build environment?","text":"<p>Add the following to your <code>mkDerivation</code> invocation.</p> <pre><code>depsBuildBuild = [ buildPackages.stdenv.cc ];\n</code></pre>"},{"location":"stdenv/cross-compilation.chapter.html#cross-testsuite-runs-host-code","title":"My package\u2019s testsuite needs to run host platform code.","text":"<p>Add the following to your <code>mkDerivation</code> invocation.</p> <pre><code>doCheck = stdenv.buildPlatform.canExecute stdenv.hostPlatform;\n</code></pre>"},{"location":"stdenv/cross-compilation.chapter.html#cross-meson-runs-host-code","title":"Package using Meson needs to run binaries for the host platform during build.","text":"<p>Add <code>mesonEmulatorHook</code> to <code>nativeBuildInputs</code> conditionally on if the target binaries can be executed.</p> <p>e.g.</p> <pre><code>nativeBuildInputs = [\n  meson\n] ++ lib.optionals (!stdenv.buildPlatform.canExecute stdenv.hostPlatform) [\n  mesonEmulatorHook\n];\n</code></pre> <p>Example of an error which this fixes.</p> <p><code>[Errno 8] Exec format error: './gdk3-scan'</code></p>"},{"location":"stdenv/cross-compilation.chapter.html#sec-cross-usage","title":"Cross-building packages","text":"<p>Nixpkgs can be instantiated with <code>localSystem</code> alone, in which case there is no cross-compiling and everything is built by and for that system, or also with <code>crossSystem</code>, in which case packages run on the latter, but all building happens on the former. Both parameters take the same schema as the 3 (build, host, and target) platforms defined in the previous section. As mentioned above, <code>lib.systems.examples</code> has some platforms which are used as arguments for these parameters in practice. You can use them programmatically, or on the command line:</p> <pre><code>$ nix-build '&lt;nixpkgs&gt;' --arg crossSystem '(import &lt;nixpkgs/lib&gt;).systems.examples.fooBarBaz' -A whatever\n</code></pre> <p>::: {.note} Eventually we would like to make these platform examples an unnecessary convenience so that</p> <pre><code>$ nix-build '&lt;nixpkgs&gt;' --arg crossSystem '{ config = \"&lt;arch&gt;-&lt;os&gt;-&lt;vendor&gt;-&lt;abi&gt;\"; }' -A whatever\n</code></pre> <p>works in the vast majority of cases. The problem today is dependencies on other sorts of configuration which aren't given proper defaults. We rely on the examples to crudely to set those configuration parameters in some vaguely sane manner on the users behalf. Issue #34274 tracks this inconvenience along with its root cause in crufty configuration options. :::</p> <p>While one is free to pass both parameters in full, there's a lot of logic to fill in missing fields. As discussed in the previous section, only one of <code>system</code>, <code>config</code>, and <code>parsed</code> is needed to infer the other two. Additionally, <code>libc</code> will be inferred from <code>parse</code>. Finally, <code>localSystem.system</code> is also impurely inferred based on the platform evaluation occurs. This means it is often not necessary to pass <code>localSystem</code> at all, as in the command-line example in the previous paragraph.</p> <p>::: {.note} Many sources (manual, wiki, etc) probably mention passing <code>system</code>, <code>platform</code>, along with the optional <code>crossSystem</code> to Nixpkgs: <code>import &lt;nixpkgs&gt; { system = ..; platform = ..; crossSystem = ..; }</code>. Passing those two instead of <code>localSystem</code> is still supported for compatibility, but is discouraged. Indeed, much of the inference we do for these parameters is motivated by compatibility as much as convenience. :::</p> <p>One would think that <code>localSystem</code> and <code>crossSystem</code> overlap horribly with the three <code>*Platforms</code> (<code>buildPlatform</code>, <code>hostPlatform,</code> and <code>targetPlatform</code>; see <code>stage.nix</code> or the manual). Actually, those identifiers are purposefully not used here to draw a subtle but important distinction: While the granularity of having 3 platforms is necessary to properly build packages, it is overkill for specifying the user's intent when making a build plan or package set. A simple \"build vs deploy\" dichotomy is adequate: the sliding window principle described in the previous section shows how to interpolate between the these two \"end points\" to get the 3 platform triple for each bootstrapping stage. That means for any package a given package set, even those not bound on the top level but only reachable via dependencies or <code>buildPackages</code>, the three platforms will be defined as one of <code>localSystem</code> or <code>crossSystem</code>, with the former replacing the latter as one traverses build-time dependencies. A last simple difference is that <code>crossSystem</code> should be null when one doesn't want to cross-compile, while the <code>*Platform</code>s are always non-null. <code>localSystem</code> is always non-null.</p>"},{"location":"stdenv/cross-compilation.chapter.html#sec-cross-infra","title":"Cross-compilation infrastructure","text":""},{"location":"stdenv/cross-compilation.chapter.html#ssec-cross-dependency-implementation","title":"Implementation of dependencies","text":"<p>The categories of dependencies developed in  are specified as lists of derivations given to <code>mkDerivation</code>, as documented in . In short, each list of dependencies for \"host \u2192 target\" is called <code>deps&lt;host&gt;&lt;target&gt;</code> (where <code>host</code>, and <code>target</code> values are either <code>build</code>, <code>host</code>, or <code>target</code>), with exceptions for backwards compatibility that <code>depsBuildHost</code> is instead called <code>nativeBuildInputs</code> and <code>depsHostTarget</code> is instead called <code>buildInputs</code>. Nixpkgs is now structured so that each <code>deps&lt;host&gt;&lt;target&gt;</code> is automatically taken from <code>pkgs&lt;host&gt;&lt;target&gt;</code>. (These <code>pkgs&lt;host&gt;&lt;target&gt;</code>s are quite new, so there is no special case for <code>nativeBuildInputs</code> and <code>buildInputs</code>.) For example, <code>pkgsBuildHost.gcc</code> should be used at build-time, while <code>pkgsHostTarget.gcc</code> should be used at run-time.</p> <p>Now, for most of Nixpkgs's history, there were no <code>pkgs&lt;host&gt;&lt;target&gt;</code> attributes, and most packages have not been refactored to use it explicitly. Prior to those, there were just <code>buildPackages</code>, <code>pkgs</code>, and <code>targetPackages</code>. Those are now redefined as aliases to <code>pkgsBuildHost</code>, <code>pkgsHostTarget</code>, and <code>pkgsTargetTarget</code>. It is acceptable, even recommended, to use them for libraries to show that the host platform is irrelevant.</p> <p>But before that, there was just <code>pkgs</code>, even though both <code>buildInputs</code> and <code>nativeBuildInputs</code> existed. [Cross barely worked, and those were implemented with some hacks on <code>mkDerivation</code> to override dependencies.] What this means is the vast majority of packages do not use any explicit package set to populate their dependencies, just using whatever <code>callPackage</code> gives them even if they do correctly sort their dependencies into the multiple lists described above. And indeed, asking that users both sort their dependencies, and take them from the right attribute set, is both too onerous and redundant, so the recommended approach (for now) is to continue just categorizing by list and not using an explicit package set.</p> <p>To make this work, we \"splice\" together the six <code>pkgsFooBar</code> package sets and have <code>callPackage</code> actually take its arguments from that. This is currently implemented in <code>pkgs/top-level/splice.nix</code>. <code>mkDerivation</code> then, for each dependency attribute, pulls the right derivation out from the splice. This splicing can be skipped when not cross-compiling as the package sets are the same, but still is a bit slow for cross-compiling. We'd like to do something better, but haven't come up with anything yet.</p>"},{"location":"stdenv/cross-compilation.chapter.html#ssec-bootstrapping","title":"Bootstrapping","text":"<p>Each of the package sets described above come from a single bootstrapping stage. While <code>pkgs/top-level/default.nix</code>, coordinates the composition of stages at a high level, <code>pkgs/top-level/stage.nix</code> \"ties the knot\" (creates the fixed point) of each stage. The package sets are defined per-stage however, so they can be thought of as edges between stages (the nodes) in a graph. Compositions like <code>pkgsBuildTarget.targetPackages</code> can be thought of as paths to this graph.</p> <p>While there are many package sets, and thus many edges, the stages can also be arranged in a linear chain. In other words, many of the edges are redundant as far as connectivity is concerned. This hinges on the type of bootstrapping we do. Currently for cross it is:</p> <ol> <li> <p><code>(native, native, native)</code></p> </li> <li> <p><code>(native, native, foreign)</code></p> </li> <li> <p><code>(native, foreign, foreign)</code></p> </li> </ol> <p>In each stage, <code>pkgsBuildHost</code> refers to the previous stage, <code>pkgsBuildBuild</code> refers to the one before that, and <code>pkgsHostTarget</code> refers to the current one, and <code>pkgsTargetTarget</code> refers to the next one. When there is no previous or next stage, they instead refer to the current stage. Note how all the invariants regarding the mapping between dependency and depending packages' build host and target platforms are preserved. <code>pkgsBuildTarget</code> and <code>pkgsHostHost</code> are more complex in that the stage fitting the requirements isn't always a fixed chain of \"prevs\" and \"nexts\" away (modulo the \"saturating\" self-references at the ends). We just special case each instead. All the primary edges are implemented is in <code>pkgs/stdenv/booter.nix</code>, and secondarily aliases in <code>pkgs/top-level/stage.nix</code>.</p> <p>::: {.note} The native stages are bootstrapped in legacy ways that predate the current cross implementation. This is why the bootstrapping stages leading up to the final stages are ignored in the previous paragraph. :::</p> <p>If one looks at the 3 platform triples, one can see that they overlap such that one could put them together into a chain like: <pre><code>(native, native, native, foreign, foreign)\n</code></pre></p> <p>If one imagines the saturating self references at the end being replaced with infinite stages, and then overlays those platform triples, one ends up with the infinite tuple: <pre><code>(native..., native, native, native, foreign, foreign, foreign...)\n</code></pre> One can then imagine any sequence of platforms such that there are bootstrap stages with their 3 platforms determined by \"sliding a window\" that is the 3 tuple through the sequence. This was the original model for bootstrapping. Without a target platform (assume a better world where all compilers are multi-target and all standard libraries are built in their own derivation), this is sufficient. Conversely if one wishes to cross compile \"faster\", with a \"Canadian Cross\" bootstrapping stage where <code>build != host != target</code>, more bootstrapping stages are needed since no sliding window provides the pesky <code>pkgsBuildTarget</code> package set since it skips the Canadian cross stage's \"host\".</p> <p>::: {.note} It is much better to refer to <code>buildPackages</code> than <code>targetPackages</code>, or more broadly package sets that do not mention \u201ctarget\u201d. There are three reasons for this.</p> <p>First, it is because bootstrapping stages do not have a unique <code>targetPackages</code>. For example a <code>(x86-linux, x86-linux, arm-linux)</code> and <code>(x86-linux, x86-linux, x86-windows)</code> package set both have a <code>(x86-linux, x86-linux, x86-linux)</code> package set. Because there is no canonical <code>targetPackages</code> for such a native (<code>build == host == target</code>) package set, we set their <code>targetPackages</code></p> <p>Second, it is because this is a frequent source of hard-to-follow \"infinite recursions\" / cycles. When only package sets that don't mention target are used, the package set forms a directed acyclic graph. This means that all cycles that exist are confined to one stage. This means they are a lot smaller, and easier to follow in the code or a backtrace. It also means they are present in native and cross builds alike, and so more likely to be caught by CI and other users.</p> <p>Thirdly, it is because everything target-mentioning only exists to accommodate compilers with lousy build systems that insist on the compiler itself and standard library being built together. Of course that is bad because bigger derivations means longer rebuilds. It is also problematic because it tends to make the standard libraries less like other libraries than they could be, complicating code and build systems alike. Because of the other problems, and because of these innate disadvantages, compilers ought to be packaged another way where possible. :::</p> <p>::: {.note} If one explores Nixpkgs, they will see derivations with names like <code>gccCross</code>. Such <code>*Cross</code> derivations is a holdover from before we properly distinguished between the host and target platforms\u2014the derivation with \u201cCross\u201d in the name covered the <code>build = host != target</code> case, while the other covered the <code>host = target</code>, with build platform the same or not based on whether one was using its <code>.__spliced.buildHost</code> or <code>.__spliced.hostTarget</code>. :::</p>"},{"location":"stdenv/meta.chapter.html","title":"Meta-attributes","text":"<p>Nix packages can declare meta-attributes that contain information about a package such as a description, its homepage, its license, and so on. For instance, the GNU Hello package has a <code>meta</code> declaration like this:</p> <pre><code>meta = with lib; {\n  description = \"A program that produces a familiar, friendly greeting\";\n  longDescription = ''\n    GNU Hello is a program that prints \"Hello, world!\" when you run it.\n    It is fully customizable.\n  '';\n  homepage = \"https://www.gnu.org/software/hello/manual/\";\n  license = licenses.gpl3Plus;\n  maintainers = with maintainers; [ eelco ];\n  platforms = platforms.all;\n};\n</code></pre> <p>Meta-attributes are not passed to the builder of the package. Thus, a change to a meta-attribute doesn\u2019t trigger a recompilation of the package.</p>"},{"location":"stdenv/meta.chapter.html#sec-standard-meta-attributes","title":"Standard meta-attributes","text":"<p>It is expected that each meta-attribute is one of the following:</p>"},{"location":"stdenv/meta.chapter.html#var-meta-description","title":"<code>description</code>","text":"<p>A short (one-line) description of the package. This is displayed on search.nixos.org.</p> <p>Don\u2019t include a period at the end. Don\u2019t include newline characters. Capitalise the first character. For brevity, don\u2019t repeat the name of package --- just describe what it does.</p> <p>Wrong: <code>\"libpng is a library that allows you to decode PNG images.\"</code></p> <p>Right: <code>\"A library for decoding PNG images\"</code></p>"},{"location":"stdenv/meta.chapter.html#var-meta-longDescription","title":"<code>longDescription</code>","text":"<p>An arbitrarily long description of the package in CommonMark Markdown.</p>"},{"location":"stdenv/meta.chapter.html#var-meta-branch","title":"<code>branch</code>","text":"<p>Release branch. Used to specify that a package is not going to receive updates that are not in this branch; for example, Linux kernel 3.0 is supposed to be updated to 3.0.X, not 3.1.</p>"},{"location":"stdenv/meta.chapter.html#var-meta-homepage","title":"<code>homepage</code>","text":"<p>The package\u2019s homepage. Example: <code>https://www.gnu.org/software/hello/manual/</code></p>"},{"location":"stdenv/meta.chapter.html#var-meta-downloadPage","title":"<code>downloadPage</code>","text":"<p>The page where a link to the current version can be found. Example: <code>https://ftp.gnu.org/gnu/hello/</code></p>"},{"location":"stdenv/meta.chapter.html#var-meta-changelog","title":"<code>changelog</code>","text":"<p>A link or a list of links to the location of Changelog for a package. A link may use expansion to refer to the correct changelog version. Example: <code>\"https://git.savannah.gnu.org/cgit/hello.git/plain/NEWS?h=v${version}\"</code></p>"},{"location":"stdenv/meta.chapter.html#var-meta-license","title":"<code>license</code>","text":"<p>The license, or licenses, for the package. One from the attribute set defined in <code>nixpkgs/lib/licenses.nix</code>. At this moment using both a list of licenses and a single license is valid. If the license field is in the form of a list representation, then it means that parts of the package are licensed differently. Each license should preferably be referenced by their attribute. The non-list attribute value can also be a space delimited string representation of the contained attribute <code>shortNames</code> or <code>spdxIds</code>. The following are all valid examples:</p> <ul> <li>Single license referenced by attribute (preferred) <code>lib.licenses.gpl3Only</code>.</li> <li>Single license referenced by its attribute shortName (frowned upon) <code>\"gpl3Only\"</code>.</li> <li>Single license referenced by its attribute spdxId (frowned upon) <code>\"GPL-3.0-only\"</code>.</li> <li>Multiple licenses referenced by attribute (preferred) <code>with lib.licenses; [ asl20 free ofl ]</code>.</li> <li>Multiple licenses referenced as a space delimited string of attribute shortNames (frowned upon) <code>\"asl20 free ofl\"</code>.</li> </ul> <p>For details, see Licenses.</p>"},{"location":"stdenv/meta.chapter.html#var-meta-maintainers","title":"<code>maintainers</code>","text":"<p>A list of the maintainers of this Nix expression. Maintainers are defined in <code>nixpkgs/maintainers/maintainer-list.nix</code>. There is no restriction to becoming a maintainer, just add yourself to that list in a separate commit titled \u201cmaintainers: add alice\u201d in the same pull request, and reference maintainers with <code>maintainers = with lib.maintainers; [ alice bob ]</code>.</p>"},{"location":"stdenv/meta.chapter.html#var-meta-mainProgram","title":"<code>mainProgram</code>","text":"<p>The name of the main binary for the package. This affects the binary <code>nix run</code> executes. Example: <code>\"rg\"</code></p>"},{"location":"stdenv/meta.chapter.html#var-meta-priority","title":"<code>priority</code>","text":"<p>The priority of the package, used by <code>nix-env</code> to resolve file name conflicts between packages. See the manual page for <code>nix-env</code> for details. Example: <code>\"10\"</code> (a low-priority package).</p>"},{"location":"stdenv/meta.chapter.html#var-meta-platforms","title":"<code>platforms</code>","text":"<p>The list of Nix platform types on which the package is supported. Hydra builds packages according to the platform specified. If no platform is specified, the package does not have prebuilt binaries. An example is:</p> <pre><code>meta.platforms = lib.platforms.linux;\n</code></pre> <p>Attribute Set <code>lib.platforms</code> defines various common lists of platforms types.</p>"},{"location":"stdenv/meta.chapter.html#var-meta-badPlatforms","title":"<code>badPlatforms</code>","text":"<p>The list of Nix platform types on which the package is known not to be buildable. Hydra will never create prebuilt binaries for these platform types, even if they are in <code>meta.platforms</code>. In general it is preferable to set <code>meta.platforms = lib.platforms.all</code> and then exclude any platforms on which the package is known not to build. For example, a package which requires dynamic linking and cannot be linked statically could use this:</p> <pre><code>meta.platforms = lib.platforms.all;\nmeta.badPlatforms = [ lib.systems.inspect.patterns.isStatic ];\n</code></pre> <p>The <code>lib.meta.availableOn</code> function can be used to test whether or not a package is available (i.e. buildable) on a given platform. Some packages use this to automatically detect the maximum set of features with which they can be built. For example, <code>systemd</code> requires dynamic linking, and has a <code>meta.badPlatforms</code> setting similar to the one above. Packages which can be built with or without <code>systemd</code> support will use <code>lib.meta.availableOn</code> to detect whether or not <code>systemd</code> is available on the <code>hostPlatform</code> for which they are being built; if it is not available (e.g. due to a statically-linked host platform like <code>pkgsStatic</code>) this support will be disabled by default.</p>"},{"location":"stdenv/meta.chapter.html#var-meta-tests","title":"<code>tests</code>","text":"<p>::: {.warning} This attribute is special in that it is not actually under the <code>meta</code> attribute set but rather under the <code>passthru</code> attribute set. This is due to how <code>meta</code> attributes work, and the fact that they are supposed to contain only metadata, not derivations. :::</p> <p>An attribute set with tests as values. A test is a derivation that builds when the test passes and fails to build otherwise.</p> <p>You can run these tests with:</p> <pre><code>$ cd path/to/nixpkgs\n$ nix-build -A your-package.tests\n</code></pre>"},{"location":"stdenv/meta.chapter.html#var-meta-tests-packages","title":"Package tests","text":"<p>Tests that are part of the source package are often executed in the <code>installCheckPhase</code>.</p> <p>Prefer <code>passthru.tests</code> for tests that are introduced in nixpkgs because:</p> <ul> <li><code>passthru.tests</code> tests the 'real' package, independently from the environment in which it was built</li> <li>we can run <code>passthru.tests</code> independently</li> <li><code>installCheckPhase</code> adds overhead to each build</li> </ul> <p>For more on how to write and run package tests, see .</p>"},{"location":"stdenv/meta.chapter.html#var-meta-tests-nixos","title":"NixOS tests","text":"<p>The NixOS tests are available as <code>nixosTests</code> in parameters of derivations. For instance, the OpenSMTPD derivation includes lines similar to:</p> <pre><code>{ /* ... */, nixosTests }:\n{\n  # ...\n  passthru.tests = {\n    basic-functionality-and-dovecot-integration = nixosTests.opensmtpd;\n  };\n}\n</code></pre> <p>NixOS tests run in a VM, so they are slower than regular package tests. For more information see NixOS module tests.</p> <p>Alternatively, you can specify other derivations as tests. You can make use of the optional parameter to inject the correct package without relying on non-local definitions, even in the presence of <code>overrideAttrs</code>. Here that's <code>finalAttrs.finalPackage</code>, but you could choose a different name if <code>finalAttrs</code> already exists in your scope.</p> <p><code>(mypkg.overrideAttrs f).passthru.tests</code> will be as expected, as long as the definition of <code>tests</code> does not rely on the original <code>mypkg</code> or overrides it in all places.</p> <pre><code># my-package/default.nix\n{ stdenv, callPackage }:\nstdenv.mkDerivation (finalAttrs: {\n  # ...\n  passthru.tests.example = callPackage ./example.nix { my-package = finalAttrs.finalPackage; };\n})\n</code></pre> <pre><code># my-package/example.nix\n{ runCommand, lib, my-package, ... }:\nrunCommand \"my-package-test\" {\n  nativeBuildInputs = [ my-package ];\n  src = lib.sources.sourcesByRegex ./. [ \".*.in\" \".*.expected\" ];\n} ''\n  my-package --help\n  my-package &lt;example.in &gt;example.actual\n  diff -U3 --color=auto example.expected example.actual\n  mkdir $out\n''\n</code></pre>"},{"location":"stdenv/meta.chapter.html#var-meta-timeout","title":"<code>timeout</code>","text":"<p>A timeout (in seconds) for building the derivation. If the derivation takes longer than this time to build, Hydra will fail it due to breaking the timeout. However, all computers do not have the same computing power, hence some builders may decide to apply a multiplicative factor to this value. When filling this value in, try to keep it approximately consistent with other values already present in <code>nixpkgs</code>.</p> <p><code>meta</code> attributes are not stored in the instantiated derivation. Therefore, this setting may be lost when the package is used as a dependency. To be effective, it must be presented directly to an evaluation process that handles the <code>meta.timeout</code> attribute.</p>"},{"location":"stdenv/meta.chapter.html#var-meta-hydraPlatforms","title":"<code>hydraPlatforms</code>","text":"<p>The list of Nix platform types for which the Hydra instance at <code>hydra.nixos.org</code> will build the package. (Hydra is the Nix-based continuous build system.) It defaults to the value of <code>meta.platforms</code>. Thus, the only reason to set <code>meta.hydraPlatforms</code> is if you want <code>hydra.nixos.org</code> to build the package on a subset of <code>meta.platforms</code>, or not at all, e.g.</p> <pre><code>meta.platforms = lib.platforms.linux;\nmeta.hydraPlatforms = [];\n</code></pre>"},{"location":"stdenv/meta.chapter.html#var-meta-broken","title":"<code>broken</code>","text":"<p>If set to <code>true</code>, the package is marked as \"broken\", meaning that it won\u2019t show up in search.nixos.org, and cannot be built or installed unless the environment variable <code>NIXPKGS_ALLOW_BROKEN</code> is set. Such unconditionally-broken packages should be removed from Nixpkgs eventually unless they are fixed.</p> <p>The value of this attribute can depend on a package's arguments, including <code>stdenv</code>. This means that <code>broken</code> can be used to express constraints, for example:</p> <ul> <li>Does not cross compile</li> </ul> <pre><code> meta.broken = !(stdenv.buildPlatform.canExecute stdenv.hostPlatform)\n</code></pre> <ul> <li>Broken if all of a certain set of its dependencies are broken</li> </ul> <pre><code>meta.broken = lib.all (map (p: p.meta.broken) [ glibc musl ])\n</code></pre> <p>This makes <code>broken</code> strictly more powerful than <code>meta.badPlatforms</code>. However <code>meta.availableOn</code> currently examines only <code>meta.platforms</code> and <code>meta.badPlatforms</code>, so <code>meta.broken</code> does not influence the default values for optional dependencies.</p>"},{"location":"stdenv/meta.chapter.html#sec-meta-license","title":"Licenses","text":"<p>The <code>meta.license</code> attribute should preferably contain a value from <code>lib.licenses</code> defined in <code>nixpkgs/lib/licenses.nix</code>, or in-place license description of the same format if the license is unlikely to be useful in another expression.</p> <p>Although it\u2019s typically better to indicate the specific license, a few generic options are available:</p>"},{"location":"stdenv/meta.chapter.html#lib.licenses.free-free","title":"<code>lib.licenses.free</code>, <code>\"free\"</code>","text":"<p>Catch-all for free software licenses not listed above.</p>"},{"location":"stdenv/meta.chapter.html#lib.licenses.unfreeredistributable-unfree-redistributable","title":"<code>lib.licenses.unfreeRedistributable</code>, <code>\"unfree-redistributable\"</code>","text":"<p>Unfree package that can be redistributed in binary form. That is, it\u2019s legal to redistribute the output of the derivation. This means that the package can be included in the Nixpkgs channel.</p> <p>Sometimes proprietary software can only be redistributed unmodified. Make sure the builder doesn\u2019t actually modify the original binaries; otherwise we\u2019re breaking the license. For instance, the NVIDIA X11 drivers can be redistributed unmodified, but our builder applies <code>patchelf</code> to make them work. Thus, its license is <code>\"unfree\"</code> and it cannot be included in the Nixpkgs channel.</p>"},{"location":"stdenv/meta.chapter.html#lib.licenses.unfree-unfree","title":"<code>lib.licenses.unfree</code>, <code>\"unfree\"</code>","text":"<p>Unfree package that cannot be redistributed. You can build it yourself, but you cannot redistribute the output of the derivation. Thus it cannot be included in the Nixpkgs channel.</p>"},{"location":"stdenv/meta.chapter.html#lib.licenses.unfreeredistributablefirmware-unfree-redistributable-firmware","title":"<code>lib.licenses.unfreeRedistributableFirmware</code>, <code>\"unfree-redistributable-firmware\"</code>","text":"<p>This package supplies unfree, redistributable firmware. This is a separate value from <code>unfree-redistributable</code> because not everybody cares whether firmware is free.</p>"},{"location":"stdenv/meta.chapter.html#sec-meta-sourceProvenance","title":"Source provenance","text":"<p>The value of a package's <code>meta.sourceProvenance</code> attribute specifies the provenance of the package's derivation outputs.</p> <p>If a package contains elements that are not built from the original source by a nixpkgs derivation, the <code>meta.sourceProvenance</code> attribute should be a list containing one or more value from <code>lib.sourceTypes</code> defined in <code>nixpkgs/lib/source-types.nix</code>.</p> <p>Adding this information helps users who have needs related to build transparency and supply-chain security to gain some visibility into their installed software or set policy to allow or disallow installation based on source provenance.</p> <p>The presence of a particular <code>sourceType</code> in a package's <code>meta.sourceProvenance</code> list indicates that the package contains some components falling into that category, though the absence of that <code>sourceType</code> does not guarantee the absence of that category of <code>sourceType</code> in the package's contents. A package with no <code>meta.sourceProvenance</code> set implies it has no known <code>sourceType</code>s other than <code>fromSource</code>.</p> <p>The meaning of the <code>meta.sourceProvenance</code> attribute does not depend on the value of the <code>meta.license</code> attribute.</p>"},{"location":"stdenv/meta.chapter.html#lib.sourceTypes.fromSource","title":"<code>lib.sourceTypes.fromSource</code>","text":"<p>Package elements which are produced by a nixpkgs derivation which builds them from source code.</p>"},{"location":"stdenv/meta.chapter.html#lib.sourceTypes.binaryNativeCode","title":"<code>lib.sourceTypes.binaryNativeCode</code>","text":"<p>Native code to be executed on the target system's CPU, built by a third party. This includes packages which wrap a downloaded AppImage or Debian package.</p>"},{"location":"stdenv/meta.chapter.html#lib.sourceTypes.binaryFirmware","title":"<code>lib.sourceTypes.binaryFirmware</code>","text":"<p>Code to be executed on a peripheral device or embedded controller, built by a third party.</p>"},{"location":"stdenv/meta.chapter.html#lib.sourceTypes.binaryBytecode","title":"<code>lib.sourceTypes.binaryBytecode</code>","text":"<p>Code to run on a VM interpreter or JIT compiled into bytecode by a third party. This includes packages which download Java <code>.jar</code> files from another source.</p>"},{"location":"stdenv/multiple-output.chapter.html","title":"Multiple-output packages","text":"<p>The Nix language allows a derivation to produce multiple outputs, which is similar to what is utilized by other Linux distribution packaging systems. The outputs reside in separate Nix store paths, so they can be mostly handled independently of each other, including passing to build inputs, garbage collection or binary substitution. The exception is that building from source always produces all the outputs.</p> <p>The main motivation is to save disk space by reducing runtime closure sizes; consequently also sizes of substituted binaries get reduced. Splitting can be used to have more granular runtime dependencies, for example the typical reduction is to split away development-only files, as those are typically not needed during runtime. As a result, closure sizes of many packages can get reduced to a half or even much less.</p> <p>::: {.note} The reduction effects could be instead achieved by building the parts in completely separate derivations. That would often additionally reduce build-time closures, but it tends to be much harder to write such derivations, as build systems typically assume all parts are being built at once. This compromise approach of single source package producing multiple binary packages is also utilized often by rpm and deb. :::</p> <p>A number of attributes can be used to work with a derivation with multiple outputs. The attribute <code>outputs</code> is a list of strings, which are the names of the outputs. For each of these names, an identically named attribute is created, corresponding to that output.</p> <p>The attribute <code>meta.outputsToInstall</code> is used to determine the default set of outputs to install when using the derivation name unqualified: <code>bin</code>, or <code>out</code>, or the first specified output; as well as <code>man</code> if that is specified.</p>"},{"location":"stdenv/multiple-output.chapter.html#sec-multiple-outputs-using-split-packages","title":"Using a split package","text":"<p>In the Nix language the individual outputs can be reached explicitly as attributes, e.g. <code>coreutils.info</code>, but the typical case is just using packages as build inputs.</p> <p>When a multiple-output derivation gets into a build input of another derivation, the <code>dev</code> output is added if it exists, otherwise the first output is added. In addition to that, <code>propagatedBuildOutputs</code> of that package which by default contain <code>$outputBin</code> and <code>$outputLib</code> are also added. (See .)</p> <p>In some cases it may be desirable to combine different outputs under a single store path. A function <code>symlinkJoin</code> can be used to do this. (Note that it may negate some closure size benefits of using a multiple-output package.)</p>"},{"location":"stdenv/multiple-output.chapter.html#sec-multiple-outputs-","title":"Writing a split derivation","text":"<p>Here you find how to write a derivation that produces multiple outputs.</p> <p>In nixpkgs there is a framework supporting multiple-output derivations. It tries to cover most cases by default behavior. You can find the source separated in <code>&lt;nixpkgs/pkgs/build-support/setup-hooks/multiple-outputs.sh&gt;</code>; it\u2019s relatively well-readable. The whole machinery is triggered by defining the <code>outputs</code> attribute to contain the list of desired output names (strings).</p> <pre><code>outputs = [ \"bin\" \"dev\" \"out\" \"doc\" ];\n</code></pre> <p>Often such a single line is enough. For each output an equally named environment variable is passed to the builder and contains the path in nix store for that output. Typically you also want to have the main <code>out</code> output, as it catches any files that didn\u2019t get elsewhere.</p> <p>::: {.note} There is a special handling of the <code>debug</code> output, described at . :::</p>"},{"location":"stdenv/multiple-output.chapter.html#multiple-output-file-binaries-first-convention","title":"\u201cBinaries first\u201d","text":"<p>A commonly adopted convention in <code>nixpkgs</code> is that executables provided by the package are contained within its first output. This convention allows the dependent packages to reference the executables provided by packages in a uniform manner. For instance, provided with the knowledge that the <code>perl</code> package contains a <code>perl</code> executable it can be referenced as <code>${pkgs.perl}/bin/perl</code> within a Nix derivation that needs to execute a Perl script.</p> <p>The <code>glibc</code> package is a deliberate single exception to the \u201cbinaries first\u201d convention. The <code>glibc</code> has <code>libs</code> as its first output allowing the libraries provided by <code>glibc</code> to be referenced directly (e.g. <code>${glibc}/lib/ld-linux-x86-64.so.2</code>). The executables provided by <code>glibc</code> can be accessed via its <code>bin</code> attribute (e.g. <code>${lib.getBin stdenv.cc.libc}/bin/ldd</code>).</p> <p>The reason for why <code>glibc</code> deviates from the convention is because referencing a library provided by <code>glibc</code> is a very common operation among Nix packages. For instance, third-party executables packaged by Nix are typically patched and relinked with the relevant version of <code>glibc</code> libraries from Nix packages (please see the documentation on patchelf for more details).</p>"},{"location":"stdenv/multiple-output.chapter.html#multiple-output-file-type-groups","title":"File type groups","text":"<p>The support code currently recognizes some particular kinds of outputs and either instructs the build system of the package to put files into their desired outputs or it moves the files during the fixup phase. Each group of file types has an <code>outputFoo</code> variable specifying the output name where they should go. If that variable isn\u2019t defined by the derivation writer, it is guessed \u2013 a default output name is defined, falling back to other possibilities if the output isn\u2019t defined.</p>"},{"location":"stdenv/multiple-output.chapter.html#outputdev","title":"<code>$outputDev</code>","text":"<p>is for development-only files. These include C(++) headers (<code>include/</code>), pkg-config (<code>lib/pkgconfig/</code>), cmake (<code>lib/cmake/</code>) and aclocal files (<code>share/aclocal/</code>). They go to <code>dev</code> or <code>out</code> by default.</p>"},{"location":"stdenv/multiple-output.chapter.html#outputbin","title":"<code>$outputBin</code>","text":"<p>is meant for user-facing binaries, typically residing in <code>bin/</code>. They go to <code>bin</code> or <code>out</code> by default.</p>"},{"location":"stdenv/multiple-output.chapter.html#outputlib","title":"<code>$outputLib</code>","text":"<p>is meant for libraries, typically residing in <code>lib/</code> and <code>libexec/</code>. They go to <code>lib</code> or <code>out</code> by default.</p>"},{"location":"stdenv/multiple-output.chapter.html#outputdoc","title":"<code>$outputDoc</code>","text":"<p>is for user documentation, typically residing in <code>share/doc/</code>. It goes to <code>doc</code> or <code>out</code> by default.</p>"},{"location":"stdenv/multiple-output.chapter.html#outputdevdoc","title":"<code>$outputDevdoc</code>","text":"<p>is for developer documentation. Currently we count gtk-doc and devhelp books, typically residing in <code>share/gtk-doc/</code> and <code>share/devhelp/</code>, in there. It goes to <code>devdoc</code> or is removed (!) by default. This is because e.g. gtk-doc tends to be rather large and completely unused by nixpkgs users.</p>"},{"location":"stdenv/multiple-output.chapter.html#outputman","title":"<code>$outputMan</code>","text":"<p>is for man pages (except for section 3), typically residing in <code>share/man/man[0-9]/</code>. They go to <code>man</code> or <code>$outputBin</code> by default.</p>"},{"location":"stdenv/multiple-output.chapter.html#outputdevman","title":"<code>$outputDevman</code>","text":"<p>is for section 3 man pages, typically residing in <code>share/man/man[0-9]/</code>. They go to <code>devman</code> or <code>$outputMan</code> by default.</p>"},{"location":"stdenv/multiple-output.chapter.html#outputinfo","title":"<code>$outputInfo</code>","text":"<p>is for info pages, typically residing in <code>share/info/</code>. They go to <code>info</code> or <code>$outputBin</code> by default.</p>"},{"location":"stdenv/multiple-output.chapter.html#sec-multiple-outputs-caveats","title":"Common caveats","text":"<ul> <li> <p>Some configure scripts don\u2019t like some of the parameters passed by default by the framework, e.g. <code>--docdir=/foo/bar</code>. You can disable this by setting <code>setOutputFlags = false;</code>.</p> </li> <li> <p>The outputs of a single derivation can retain references to each other, but note that circular references are not allowed. (And each strongly-connected component would act as a single output anyway.)</p> </li> <li> <p>Most of split packages contain their core functionality in libraries. These libraries tend to refer to various kind of data that typically gets into <code>out</code>, e.g. locale strings, so there is often no advantage in separating the libraries into <code>lib</code>, as keeping them in <code>out</code> is easier.</p> </li> <li> <p>Some packages have hidden assumptions on install paths, which complicates splitting.</p> </li> </ul>"},{"location":"stdenv/platform-notes.chapter.html","title":"Platform Notes","text":""},{"location":"stdenv/platform-notes.chapter.html#sec-darwin","title":"Darwin (macOS)","text":"<p>Some common issues when packaging software for Darwin:</p> <ul> <li>The Darwin <code>stdenv</code> uses clang instead of gcc. When referring to the compiler <code>$CC</code> or <code>cc</code> will work in both cases. Some builds hardcode gcc/g++ in their build scripts, that can usually be fixed with using something like <code>makeFlags = [ \"CC=cc\" ];</code> or by patching the build scripts.</li> </ul> <pre><code>stdenv.mkDerivation {\n  name = \"libfoo-1.2.3\";\n  # ...\n  buildPhase = ''\n    $CC -o hello hello.c\n  '';\n}\n</code></pre> <ul> <li>On Darwin, libraries are linked using absolute paths, libraries are resolved by their <code>install_name</code> at link time. Sometimes packages won\u2019t set this correctly causing the library lookups to fail at runtime. This can be fixed by adding extra linker flags or by running <code>install_name_tool -id</code> during the <code>fixupPhase</code>.</li> </ul> <pre><code>stdenv.mkDerivation {\n  name = \"libfoo-1.2.3\";\n  # ...\n  makeFlags = lib.optional stdenv.isDarwin \"LDFLAGS=-Wl,-install_name,$(out)/lib/libfoo.dylib\";\n}\n</code></pre> <ul> <li>Even if the libraries are linked using absolute paths and resolved via their <code>install_name</code> correctly, tests can sometimes fail to run binaries. This happens because the <code>checkPhase</code> runs before the libraries are installed.</li> </ul> <p>This can usually be solved by running the tests after the <code>installPhase</code> or alternatively by using <code>DYLD_LIBRARY_PATH</code>. More information about this variable can be found in the dyld(1) manpage.</p> <pre><code>dyld: Library not loaded: /nix/store/7hnmbscpayxzxrixrgxvvlifzlxdsdir-jq-1.5-lib/lib/libjq.1.dylib\nReferenced from: /private/tmp/nix-build-jq-1.5.drv-0/jq-1.5/tests/../jq\nReason: image not found\n./tests/jqtest: line 5: 75779 Abort trap: 6\n</code></pre> <pre><code>stdenv.mkDerivation {\n  name = \"libfoo-1.2.3\";\n  # ...\n  doInstallCheck = true;\n  installCheckTarget = \"check\";\n}\n</code></pre> <ul> <li>Some packages assume xcode is available and use <code>xcrun</code> to resolve build tools like <code>clang</code>, etc. This causes errors like <code>xcode-select: error: no developer tools were found at '/Applications/Xcode.app'</code> while the build doesn\u2019t actually depend on xcode.</li> </ul> <pre><code>stdenv.mkDerivation {\n  name = \"libfoo-1.2.3\";\n  # ...\n  prePatch = ''\n    substituteInPlace Makefile \\\n        --replace-fail '/usr/bin/xcrun clang' clang\n  '';\n}\n</code></pre> <p>The package <code>xcbuild</code> can be used to build projects that really depend on Xcode. However, this replacement is not 100% compatible with Xcode and can occasionally cause issues.</p> <ul> <li>x86_64-darwin uses the 10.12 SDK by default, but some software is not compatible with that version of the SDK. In that case,   the 11.0 SDK used by aarch64-darwin is available for use on x86_64-darwin. To use it, reference <code>apple_sdk_11_0</code> instead of   <code>apple_sdk</code> in your derivation and use <code>pkgs.darwin.apple_sdk_11_0.callPackage</code> instead of <code>pkgs.callPackage</code>. On Linux, this will   have the same effect as <code>pkgs.callPackage</code>, so you can use <code>pkgs.darwin.apple_sdk_11_0.callPackage</code> regardless of platform.</li> </ul>"},{"location":"stdenv/stdenv.chapter.html","title":"The Standard Environment","text":"<p>The standard build environment in the Nix Packages collection provides an environment for building Unix packages that does a lot of common build tasks automatically. In fact, for Unix packages that use the standard <code>./configure; make; make install</code> build interface, you don\u2019t need to write a build script at all; the standard environment does everything automatically. If <code>stdenv</code> doesn\u2019t do what you need automatically, you can easily customise or override the various build phases.</p>"},{"location":"stdenv/stdenv.chapter.html#sec-using-stdenv","title":"Using <code>stdenv</code>","text":"<p>To build a package with the standard environment, you use the function <code>stdenv.mkDerivation</code>, instead of the primitive built-in function <code>derivation</code>, e.g.</p> <pre><code>stdenv.mkDerivation {\n  name = \"libfoo-1.2.3\";\n  src = fetchurl {\n    url = \"http://example.org/libfoo-1.2.3.tar.bz2\";\n    hash = \"sha256-tWxU/LANbQE32my+9AXyt3nCT7NBVfJ45CX757EMT3Q=\";\n  };\n}\n</code></pre> <p>(<code>stdenv</code> needs to be in scope, so if you write this in a separate Nix expression from <code>pkgs/all-packages.nix</code>, you need to pass it as a function argument.) Specifying a <code>name</code> and a <code>src</code> is the absolute minimum Nix requires. For convenience, you can also use <code>pname</code> and <code>version</code> attributes and <code>mkDerivation</code> will automatically set <code>name</code> to <code>\"${pname}-${version}\"</code> by default. Since RFC 0035, this is preferred for packages in Nixpkgs, as it allows us to reuse the version easily:</p> <pre><code>stdenv.mkDerivation rec {\n  pname = \"libfoo\";\n  version = \"1.2.3\";\n  src = fetchurl {\n    url = \"http://example.org/libfoo-source-${version}.tar.bz2\";\n    hash = \"sha256-tWxU/LANbQE32my+9AXyt3nCT7NBVfJ45CX757EMT3Q=\";\n  };\n}\n</code></pre> <p>Many packages have dependencies that are not provided in the standard environment. It\u2019s usually sufficient to specify those dependencies in the <code>buildInputs</code> attribute:</p> <pre><code>stdenv.mkDerivation {\n  pname = \"libfoo\";\n  version = \"1.2.3\";\n  ...\n  buildInputs = [libbar perl ncurses];\n}\n</code></pre> <p>This attribute ensures that the <code>bin</code> subdirectories of these packages appear in the <code>PATH</code> environment variable during the build, that their <code>include</code> subdirectories are searched by the C compiler, and so on. (See  for details.)</p> <p>Often it is necessary to override or modify some aspect of the build. To make this easier, the standard environment breaks the package build into a number of phases, all of which can be overridden or modified individually: unpacking the sources, applying patches, configuring, building, and installing. (There are some others; see .) For instance, a package that doesn\u2019t supply a makefile but instead has to be compiled \"manually\" could be handled like this:</p> <pre><code>stdenv.mkDerivation {\n  pname = \"fnord\";\n  version = \"4.5\";\n  ...\n  buildPhase = ''\n    gcc foo.c -o foo\n  '';\n  installPhase = ''\n    mkdir -p $out/bin\n    cp foo $out/bin\n  '';\n}\n</code></pre> <p>(Note the use of <code>''</code>-style string literals, which are very convenient for large multi-line script fragments because they don\u2019t need escaping of <code>\"</code> and <code>\\</code>, and because indentation is intelligently removed.)</p> <p>There are many other attributes to customise the build. These are listed in .</p> <p>While the standard environment provides a generic builder, you can still supply your own build script:</p> <pre><code>stdenv.mkDerivation {\n  pname = \"libfoo\";\n  version = \"1.2.3\";\n  ...\n  builder = ./builder.sh;\n}\n</code></pre> <p>where the builder can do anything it wants, but typically starts with</p> <pre><code>source $stdenv/setup\n</code></pre> <p>to let <code>stdenv</code> set up the environment (e.g. by resetting <code>PATH</code> and populating it from build inputs). If you want, you can still use <code>stdenv</code>\u2019s generic builder:</p> <pre><code>source $stdenv/setup\n\nbuildPhase() {\n  echo \"... this is my custom build phase ...\"\n  gcc foo.c -o foo\n}\n\ninstallPhase() {\n  mkdir -p $out/bin\n  cp foo $out/bin\n}\n\ngenericBuild\n</code></pre>"},{"location":"stdenv/stdenv.chapter.html#sec-building-stdenv-package-in-nix-shell","title":"Building a <code>stdenv</code> package in <code>nix-shell</code>","text":"<p>To build a <code>stdenv</code> package in a <code>nix-shell</code>, enter a shell, find the phases you wish to build, then invoke <code>genericBuild</code> manually:</p> <p>Go to an empty directory, invoke <code>nix-shell</code> with the desired package, and from inside the shell, set the output variables to a writable directory:</p> <pre><code>cd \"$(mktemp -d)\"\nnix-shell '&lt;nixpkgs&gt;' -A some_package\nexport out=$(pwd)/out\n</code></pre> <p>Next, invoke the desired parts of the build. First, run the phases that generate a working copy of the sources, which will change directory to the sources for you:</p> <pre><code>phases=\"${prePhases[*]:-} unpackPhase patchPhase\" genericBuild\n</code></pre> <p>Then, run more phases up until the failure is reached. If the failure is in the build or check phase, the following phases would be required:</p> <pre><code>phases=\"${preConfigurePhases[*]:-} configurePhase ${preBuildPhases[*]:-} buildPhase checkPhase\" genericBuild\n</code></pre> <p>Use this command to run all install phases: <pre><code>phases=\"${preInstallPhases[*]:-} installPhase ${preFixupPhases[*]:-} fixupPhase installCheckPhase\" genericBuild\n</code></pre></p> <p>Single phase can be re-run as many times as necessary to examine the failure like so:</p> <pre><code>phases=\"buildPhase\" genericBuild\n</code></pre> <p>To modify a phase, first print it with</p> <pre><code>echo \"$buildPhase\"\n</code></pre> <p>Or, if that is empty, for instance, if it is using a function:</p> <pre><code>type buildPhase\n</code></pre> <p>then change it in a text editor, and paste it back to the terminal.</p> <p>::: {.note} This method may have some inconsistencies in environment variables and behaviour compared to a normal build within the Nix build sandbox. The following is a non-exhaustive list of such differences:</p> <ul> <li><code>TMP</code>, <code>TMPDIR</code>, and similar variables likely point to non-empty directories that the build might conflict with files in.</li> <li>Output store paths are not writable, so the variables for outputs need to be overridden to writable paths.</li> <li>Other environment variables may be inconsistent with a <code>nix-build</code> either due to <code>nix-shell</code>'s initialization script or due to the use of <code>nix-shell</code> without the <code>--pure</code> option.</li> </ul> <p>If the build fails differently inside the shell than in the sandbox, consider using <code>breakpointHook</code> and invoking <code>nix-build</code> instead. The <code>--keep-failed</code> option for <code>nix-build</code> may also be useful to examine the build directory of a failed build. :::</p>"},{"location":"stdenv/stdenv.chapter.html#sec-tools-of-stdenv","title":"Tools provided by <code>stdenv</code>","text":"<p>The standard environment provides the following packages:</p> <ul> <li>The GNU C Compiler, configured with C and C++ support.</li> <li>GNU coreutils (contains a few dozen standard Unix commands).</li> <li>GNU findutils (contains <code>find</code>).</li> <li>GNU diffutils (contains <code>diff</code>, <code>cmp</code>).</li> <li>GNU <code>sed</code>.</li> <li>GNU <code>grep</code>.</li> <li>GNU <code>awk</code>.</li> <li>GNU <code>tar</code>.</li> <li><code>gzip</code>, <code>bzip2</code> and <code>xz</code>.</li> <li>GNU Make.</li> <li>Bash. This is the shell used for all builders in the Nix Packages collection. Not using <code>/bin/sh</code> removes a large source of portability problems.</li> <li>The <code>patch</code> command.</li> </ul> <p>On Linux, <code>stdenv</code> also includes the <code>patchelf</code> utility.</p>"},{"location":"stdenv/stdenv.chapter.html#ssec-stdenv-dependencies","title":"Specifying dependencies","text":"<p>Build systems often require more dependencies than just what <code>stdenv</code> provides. This section describes attributes accepted by <code>stdenv.mkDerivation</code> that can be used to make these dependencies available to the build system.</p>"},{"location":"stdenv/stdenv.chapter.html#ssec-stdenv-dependencies-overview","title":"Overview","text":"<p>A full reference of the different kinds of dependencies is provided in , but here is an overview of the most common ones. It should cover most use cases.</p> <p>Add dependencies to <code>nativeBuildInputs</code> if they are executed during the build: - those which are needed on <code>$PATH</code> during the build, for example <code>cmake</code> and <code>pkg-config</code> - setup hooks, for example <code>makeWrapper</code> - interpreters needed by <code>patchShebangs</code> for build scripts (with the <code>--build</code> flag), which can be the case for e.g. <code>perl</code></p> <p>Add dependencies to <code>buildInputs</code> if they will end up copied or linked into the final output or otherwise used at runtime: - libraries used by compilers, for example <code>zlib</code>, - interpreters needed by <code>patchShebangs</code> for scripts which are installed, which can be the case for e.g. <code>perl</code></p> <p>::: {.note} These criteria are independent.</p> <p>For example, software using Wayland usually needs the <code>wayland</code> library at runtime, so <code>wayland</code> should be added to <code>buildInputs</code>. But it also executes the <code>wayland-scanner</code> program as part of the build to generate code, so <code>wayland</code> should also be added to <code>nativeBuildInputs</code>. :::</p> <p>Dependencies needed only to run tests are similarly classified between native (executed during build) and non-native (executed at runtime): - <code>nativeCheckInputs</code> for test tools needed on <code>$PATH</code> (such as <code>ctest</code>) and setup hooks (for example <code>pytestCheckHook</code>) - <code>checkInputs</code> for libraries linked into test executables (for example the <code>qcheck</code> OCaml package)</p> <p>These dependencies are only injected when <code>doCheck</code> is set to <code>true</code>.</p>"},{"location":"stdenv/stdenv.chapter.html#ssec-stdenv-dependencies-overview-example","title":"Example","text":"<p>Consider for example this simplified derivation for <code>solo5</code>, a sandboxing tool: <pre><code>stdenv.mkDerivation rec {\n  pname = \"solo5\";\n  version = \"0.7.5\";\n\n  src = fetchurl {\n    url = \"https://github.com/Solo5/solo5/releases/download/v${version}/solo5-v${version}.tar.gz\";\n    hash = \"sha256-viwrS9lnaU8sTGuzK/+L/PlMM/xRRtgVuK5pixVeDEw=\";\n  };\n\n  nativeBuildInputs = [ makeWrapper pkg-config ];\n  buildInputs = [ libseccomp ];\n\n  postInstall = ''\n    substituteInPlace $out/bin/solo5-virtio-mkimage \\\n      --replace-fail \"/usr/lib/syslinux\" \"${syslinux}/share/syslinux\" \\\n      --replace-fail \"/usr/share/syslinux\" \"${syslinux}/share/syslinux\" \\\n      --replace-fail \"cp \" \"cp --no-preserve=mode \"\n\n    wrapProgram $out/bin/solo5-virtio-mkimage \\\n      --prefix PATH : ${lib.makeBinPath [ dosfstools mtools parted syslinux ]}\n  '';\n\n  doCheck = true;\n  nativeCheckInputs = [ util-linux qemu ];\n  checkPhase = '' [elided] '';\n}\n</code></pre></p> <ul> <li><code>makeWrapper</code> is a setup hook, i.e., a shell script sourced by the generic builder of <code>stdenv</code>.   It is thus executed during the build and must be added to <code>nativeBuildInputs</code>.</li> <li><code>pkg-config</code> is a build tool which the configure script of <code>solo5</code> expects to be on <code>$PATH</code> during the build:   therefore, it must be added to <code>nativeBuildInputs</code>.</li> <li><code>libseccomp</code> is a library linked into <code>$out/bin/solo5-elftool</code>.   As it is used at runtime, it must be added to <code>buildInputs</code>.</li> <li>Tests need <code>qemu</code> and <code>getopt</code> (from <code>util-linux</code>) on <code>$PATH</code>, these must be added to <code>nativeCheckInputs</code>.</li> <li>Some dependencies are injected directly in the shell code of phases: <code>syslinux</code>, <code>dosfstools</code>, <code>mtools</code>, and <code>parted</code>. In this specific case, they will end up in the output of the derivation (<code>$out</code> here). As Nix marks dependencies whose absolute path is present in the output as runtime dependencies, adding them to <code>buildInputs</code> is not required.</li> </ul> <p>For more complex cases, like libraries linked into an executable which is then executed as part of the build system, see .</p>"},{"location":"stdenv/stdenv.chapter.html#ssec-stdenv-dependencies-reference","title":"Reference","text":"<p>As described in the Nix manual, almost any <code>*.drv</code> store path in a derivation\u2019s attribute set will induce a dependency on that derivation. <code>mkDerivation</code>, however, takes a few attributes intended to include all the dependencies of a package. This is done both for structure and consistency, but also so that certain other setup can take place. For example, certain dependencies need their bin directories added to the <code>PATH</code>. That is built-in, but other setup is done via a pluggable mechanism that works in conjunction with these dependency attributes. See  for details.</p> <p>Dependencies can be broken down along these axes: their host and target platforms relative to the new derivation\u2019s. The platform distinctions are motivated by cross compilation; see  for exactly what each platform means. [^footnote-stdenv-ignored-build-platform] But even if one is not cross compiling, the platforms imply whether a dependency is needed at run-time or build-time.</p> <p>The extension of <code>PATH</code> with dependencies, alluded to above, proceeds according to the relative platforms alone. The process is carried out only for dependencies whose host platform matches the new derivation\u2019s build platform i.e. dependencies which run on the platform where the new derivation will be built. [^footnote-stdenv-native-dependencies-in-path] For each dependency \\&lt;dep&gt; of those dependencies, <code>dep/bin</code>, if present, is added to the <code>PATH</code> environment variable.</p>"},{"location":"stdenv/stdenv.chapter.html#ssec-stdenv-dependencies-propagated","title":"Dependency propagation","text":"<p>Propagated dependencies are made available to all downstream dependencies. This is particularly useful for interpreted languages, where all transitive dependencies have to be present in the same environment. Therefore it is used for the Python infrastructure in Nixpkgs.</p> <p>:::{.note} Propagated dependencies should be used with care, because they obscure the actual build inputs of dependent derivations and cause side effects through setup hooks. This can lead to conflicting dependencies that cannot easily be resolved. :::</p> <p>:::{.example}</p>"},{"location":"stdenv/stdenv.chapter.html#a-propagated-dependency","title":"A propagated dependency","text":"<p><pre><code>with import &lt;nixpkgs&gt; {};\nlet\n  bar = stdenv.mkDerivation {\n    name = \"bar\";\n    dontUnpack = true;\n    # `hello` is also made available to dependents, such as `foo`\n    propagatedBuildInputs = [ hello ];\n    postInstall = \"mkdir $out\";\n  };\n  foo = stdenv.mkDerivation {\n    name = \"foo\";\n    dontUnpack = true;\n    # `bar` is a direct dependency, which implicitly includes the propagated `hello`\n    buildInputs = [ bar ];\n    # The `hello` binary is available!\n    postInstall = \"hello &gt; $out\";\n  };\nin\nfoo\n</code></pre> :::</p> <p>Dependency propagation takes cross compilation into account, meaning that dependencies that cross platform boundaries are properly adjusted.</p> <p>To determine the exact rules for dependency propagation, we start by assigning to each dependency a couple of ternary numbers (<code>-1</code> for <code>build</code>, <code>0</code> for <code>host</code>, and <code>1</code> for <code>target</code>) representing its dependency type, which captures how its host and target platforms are each \"offset\" from the depending derivation\u2019s host and target platforms. The following table summarize the different combinations that can be obtained:</p> <code>host \u2192 target</code> attribute name offset <code>build --&gt; build</code> <code>depsBuildBuild</code> <code>-1, -1</code> <code>build --&gt; host</code> <code>nativeBuildInputs</code> <code>-1, 0</code> <code>build --&gt; target</code> <code>depsBuildTarget</code> <code>-1, 1</code> <code>host --&gt; host</code> <code>depsHostHost</code> <code>0, 0</code> <code>host --&gt; target</code> <code>buildInputs</code> <code>0, 1</code> <code>target --&gt; target</code> <code>depsTargetTarget</code> <code>1, 1</code> <p>Algorithmically, we traverse propagated inputs, accumulating every propagated dependency\u2019s propagated dependencies and adjusting them to account for the \u201cshift in perspective\u201d described by the current dependency\u2019s platform offsets. This results is sort of a transitive closure of the dependency relation, with the offsets being approximately summed when two dependency links are combined. We also prune transitive dependencies whose combined offsets go out-of-bounds, which can be viewed as a filter over that transitive closure removing dependencies that are blatantly absurd.</p> <p>We can define the process precisely with Natural Deduction using the inference rules. This probably seems a bit obtuse, but so is the bash code that actually implements it! [^footnote-stdenv-find-inputs-location] They\u2019re confusing in very different ways so\u2026 hopefully if something doesn\u2019t make sense in one presentation, it will in the other!</p> <pre><code>let mapOffset(h, t, i) = i + (if i &lt;= 0 then h else t - 1)\n\npropagated-dep(h0, t0, A, B)\npropagated-dep(h1, t1, B, C)\nh0 + h1 in {-1, 0, 1}\nh0 + t1 in {-1, 0, 1}\n-------------------------------------- Transitive property\npropagated-dep(mapOffset(h0, t0, h1),\n               mapOffset(h0, t0, t1),\n               A, C)\n</code></pre> <pre><code>let mapOffset(h, t, i) = i + (if i &lt;= 0 then h else t - 1)\n\ndep(h0, t0, A, B)\npropagated-dep(h1, t1, B, C)\nh0 + h1 in {-1, 0, 1}\nh0 + t1 in {-1, 0, -1}\n----------------------------- Take immediate dependencies' propagated dependencies\npropagated-dep(mapOffset(h0, t0, h1),\n               mapOffset(h0, t0, t1),\n               A, C)\n</code></pre> <pre><code>propagated-dep(h, t, A, B)\n----------------------------- Propagated dependencies count as dependencies\ndep(h, t, A, B)\n</code></pre> <p>Some explanation of this monstrosity is in order. In the common case, the target offset of a dependency is the successor to the target offset: <code>t = h + 1</code>. That means that:</p> <pre><code>let f(h, t, i) = i + (if i &lt;= 0 then h else t - 1)\nlet f(h, h + 1, i) = i + (if i &lt;= 0 then h else (h + 1) - 1)\nlet f(h, h + 1, i) = i + (if i &lt;= 0 then h else h)\nlet f(h, h + 1, i) = i + h\n</code></pre> <p>This is where \u201csum-like\u201d comes in from above: We can just sum all of the host offsets to get the host offset of the transitive dependency. The target offset is the transitive dependency is the host offset + 1, just as it was with the dependencies composed to make this transitive one; it can be ignored as it doesn\u2019t add any new information.</p> <p>Because of the bounds checks, the uncommon cases are <code>h = t</code> and <code>h + 2 = t</code>. In the former case, the motivation for <code>mapOffset</code> is that since its host and target platforms are the same, no transitive dependency of it should be able to \u201cdiscover\u201d an offset greater than its reduced target offsets. <code>mapOffset</code> effectively \u201csquashes\u201d all its transitive dependencies\u2019 offsets so that none will ever be greater than the target offset of the original <code>h = t</code> package. In the other case, <code>h + 1</code> is skipped over between the host and target offsets. Instead of squashing the offsets, we need to \u201crip\u201d them apart so no transitive dependencies\u2019 offset is that one.</p> <p>Overall, the unifying theme here is that propagation shouldn\u2019t be introducing transitive dependencies involving platforms the depending package is unaware of. [One can imagine the depending package asking for dependencies with the platforms it knows about; other platforms it doesn\u2019t know how to ask for. The platform description in that scenario is a kind of unforgeable capability.] The offset bounds checking and definition of <code>mapOffset</code> together ensure that this is the case. Discovering a new offset is discovering a new platform, and since those platforms weren\u2019t in the derivation \u201cspec\u201d of the needing package, they cannot be relevant. From a capability perspective, we can imagine that the host and target platforms of a package are the capabilities a package requires, and the depending package must provide the capability to the dependency.</p>"},{"location":"stdenv/stdenv.chapter.html#variables-specifying-dependencies","title":"Variables specifying dependencies","text":""},{"location":"stdenv/stdenv.chapter.html#var-stdenv-depsBuildBuild","title":"<code>depsBuildBuild</code>","text":"<p>A list of dependencies whose host and target platforms are the new derivation\u2019s build platform. These are programs and libraries used at build time that produce programs and libraries also used at build time. If the dependency doesn\u2019t care about the target platform (i.e. isn\u2019t a compiler or similar tool), put it in <code>nativeBuildInputs</code> instead. The most common use of this <code>buildPackages.stdenv.cc</code>, the default C compiler for this role. That example crops up more than one might think in old commonly used C libraries.</p> <p>Since these packages are able to be run at build-time, they are always added to the <code>PATH</code>, as described above. But since these packages are only guaranteed to be able to run then, they shouldn\u2019t persist as run-time dependencies. This isn\u2019t currently enforced, but could be in the future.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-nativeBuildInputs","title":"<code>nativeBuildInputs</code>","text":"<p>A list of dependencies whose host platform is the new derivation\u2019s build platform, and target platform is the new derivation\u2019s host platform. These are programs and libraries used at build-time that, if they are a compiler or similar tool, produce code to run at run-time\u2014i.e. tools used to build the new derivation. If the dependency doesn\u2019t care about the target platform (i.e. isn\u2019t a compiler or similar tool), put it here, rather than in <code>depsBuildBuild</code> or <code>depsBuildTarget</code>. This could be called <code>depsBuildHost</code> but <code>nativeBuildInputs</code> is used for historical continuity.</p> <p>Since these packages are able to be run at build-time, they are added to the <code>PATH</code>, as described above. But since these packages are only guaranteed to be able to run then, they shouldn\u2019t persist as run-time dependencies. This isn\u2019t currently enforced, but could be in the future.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-depsBuildTarget","title":"<code>depsBuildTarget</code>","text":"<p>A list of dependencies whose host platform is the new derivation\u2019s build platform, and target platform is the new derivation\u2019s target platform. These are programs used at build time that produce code to run with code produced by the depending package. Most commonly, these are tools used to build the runtime or standard library that the currently-being-built compiler will inject into any code it compiles. In many cases, the currently-being-built-compiler is itself employed for that task, but when that compiler won\u2019t run (i.e. its build and host platform differ) this is not possible. Other times, the compiler relies on some other tool, like binutils, that is always built separately so that the dependency is unconditional.</p> <p>This is a somewhat confusing concept to wrap one\u2019s head around, and for good reason. As the only dependency type where the platform offsets, <code>-1</code> and <code>1</code>, are not adjacent integers, it requires thinking of a bootstrapping stage two away from the current one. It and its use-case go hand in hand and are both considered poor form: try to not need this sort of dependency, and try to avoid building standard libraries and runtimes in the same derivation as the compiler produces code using them. Instead strive to build those like a normal library, using the newly-built compiler just as a normal library would. In short, do not use this attribute unless you are packaging a compiler and are sure it is needed.</p> <p>Since these packages are able to run at build time, they are added to the <code>PATH</code>, as described above. But since these packages are only guaranteed to be able to run then, they shouldn\u2019t persist as run-time dependencies. This isn\u2019t currently enforced, but could be in the future.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-depsHostHost","title":"<code>depsHostHost</code>","text":"<p>A list of dependencies whose host and target platforms match the new derivation\u2019s host platform. In practice, this would usually be tools used by compilers for macros or a metaprogramming system, or libraries used by the macros or metaprogramming code itself. It\u2019s always preferable to use a <code>depsBuildBuild</code> dependency in the derivation being built over a <code>depsHostHost</code> on the tool doing the building for this purpose.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-buildInputs","title":"<code>buildInputs</code>","text":"<p>A list of dependencies whose host platform and target platform match the new derivation\u2019s. This would be called <code>depsHostTarget</code> but for historical continuity. If the dependency doesn\u2019t care about the target platform (i.e. isn\u2019t a compiler or similar tool), put it here, rather than in <code>depsBuildBuild</code>.</p> <p>These are often programs and libraries used by the new derivation at run-time, but that isn\u2019t always the case. For example, the machine code in a statically-linked library is only used at run-time, but the derivation containing the library is only needed at build-time. Even in the dynamic case, the library may also be needed at build-time to appease the linker.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-depsTargetTarget","title":"<code>depsTargetTarget</code>","text":"<p>A list of dependencies whose host platform matches the new derivation\u2019s target platform. These are packages that run on the target platform, e.g. the standard library or run-time deps of standard library that a compiler insists on knowing about. It\u2019s poor form in almost all cases for a package to depend on another from a future stage [future stage corresponding to positive offset]. Do not use this attribute unless you are packaging a compiler and are sure it is needed.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-depsBuildBuildPropagated","title":"<code>depsBuildBuildPropagated</code>","text":"<p>The propagated equivalent of <code>depsBuildBuild</code>. This perhaps never ought to be used, but it is included for consistency [see below for the others].</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-propagatedNativeBuildInputs","title":"<code>propagatedNativeBuildInputs</code>","text":"<p>The propagated equivalent of <code>nativeBuildInputs</code>. This would be called <code>depsBuildHostPropagated</code> but for historical continuity. For example, if package <code>Y</code> has <code>propagatedNativeBuildInputs = [X]</code>, and package <code>Z</code> has <code>buildInputs = [Y]</code>, then package <code>Z</code> will be built as if it included package <code>X</code> in its <code>nativeBuildInputs</code>. If instead, package <code>Z</code> has <code>nativeBuildInputs = [Y]</code>, then <code>Z</code> will be built as if it included <code>X</code> in the <code>depsBuildBuild</code> of package <code>Z</code>, because of the sum of the two <code>-1</code> host offsets.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-depsBuildTargetPropagated","title":"<code>depsBuildTargetPropagated</code>","text":"<p>The propagated equivalent of <code>depsBuildTarget</code>. This is prefixed for the same reason of alerting potential users.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-depsHostHostPropagated","title":"<code>depsHostHostPropagated</code>","text":"<p>The propagated equivalent of <code>depsHostHost</code>.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-propagatedBuildInputs","title":"<code>propagatedBuildInputs</code>","text":"<p>The propagated equivalent of <code>buildInputs</code>. This would be called <code>depsHostTargetPropagated</code> but for historical continuity.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-depsTargetTargetPropagated","title":"<code>depsTargetTargetPropagated</code>","text":"<p>The propagated equivalent of <code>depsTargetTarget</code>. This is prefixed for the same reason of alerting potential users.</p>"},{"location":"stdenv/stdenv.chapter.html#ssec-stdenv-attributes","title":"Attributes","text":""},{"location":"stdenv/stdenv.chapter.html#variables-affecting-stdenv-initialisation","title":"Variables affecting <code>stdenv</code> initialisation","text":""},{"location":"stdenv/stdenv.chapter.html#var-stdenv-NIX_DEBUG","title":"<code>NIX_DEBUG</code>","text":"<p>A number between 0 and 7 indicating how much information to log. If set to 1 or higher, <code>stdenv</code> will print moderate debugging information during the build. In particular, the <code>gcc</code> and <code>ld</code> wrapper scripts will print out the complete command line passed to the wrapped tools. If set to 6 or higher, the <code>stdenv</code> setup script will be run with <code>set -x</code> tracing. If set to 7 or higher, the <code>gcc</code> and <code>ld</code> wrapper scripts will also be run with <code>set -x</code> tracing.</p>"},{"location":"stdenv/stdenv.chapter.html#attributes-affecting-build-properties","title":"Attributes affecting build properties","text":""},{"location":"stdenv/stdenv.chapter.html#var-stdenv-enableParallelBuilding","title":"<code>enableParallelBuilding</code>","text":"<p>If set to <code>true</code>, <code>stdenv</code> will pass specific flags to <code>make</code> and other build tools to enable parallel building with up to <code>build-cores</code> workers.</p> <p>Unless set to <code>false</code>, some build systems with good support for parallel building including <code>cmake</code>, <code>meson</code>, and <code>qmake</code> will set it to <code>true</code>.</p>"},{"location":"stdenv/stdenv.chapter.html#special-variables","title":"Special variables","text":""},{"location":"stdenv/stdenv.chapter.html#var-stdenv-passthru","title":"<code>passthru</code>","text":"<p>This is an attribute set which can be filled with arbitrary values. For example:</p> <pre><code>passthru = {\n  foo = \"bar\";\n  baz = {\n    value1 = 4;\n    value2 = 5;\n  };\n}\n</code></pre> <p>Values inside it are not passed to the builder, so you can change them without triggering a rebuild. However, they can be accessed outside of a derivation directly, as if they were set inside a derivation itself, e.g. <code>hello.baz.value1</code>. We don\u2019t specify any usage or schema of <code>passthru</code> - it is meant for values that would be useful outside the derivation in other parts of a Nix expression (e.g. in other derivations). An example would be to convey some specific dependency of your derivation which contains a program with plugins support. Later, others who make derivations with plugins can use passed-through dependency to ensure that their plugin would be binary-compatible with built program.</p>"},{"location":"stdenv/stdenv.chapter.html#var-passthru-updateScript","title":"<code>passthru.updateScript</code>","text":"<p>A script to be run by <code>maintainers/scripts/update.nix</code> when the package is matched. The attribute can contain one of the following:</p> <ul> <li>[]{#var-passthru-updateScript-command} an executable file, either on the file system:</li> </ul> <pre><code>passthru.updateScript = ./update.sh;\n</code></pre> <p>or inside the expression itself:</p> <pre><code>passthru.updateScript = writeScript \"update-zoom-us\" ''\n  #!/usr/bin/env nix-shell\n  #!nix-shell -i bash -p curl pcre2 common-updater-scripts\n\n  set -eu -o pipefail\n\n  version=\"$(curl -sI https://zoom.us/client/latest/zoom_x86_64.tar.xz | grep -Fi 'Location:' | pcre2grep -o1 '/(([0-9]\\.?)+)/')\"\n  update-source-version zoom-us \"$version\"\n'';\n</code></pre> <ul> <li>a list, a script followed by arguments to be passed to it:</li> </ul> <pre><code>passthru.updateScript = [ ../../update.sh pname \"--requested-release=unstable\" ];\n</code></pre> <ul> <li>an attribute set containing:</li> <li>[<code>command</code>]{#var-passthru-updateScript-set-command} \u2013 a string or list in the format expected by <code>passthru.updateScript</code>.</li> <li>[<code>attrPath</code>]{#var-passthru-updateScript-set-attrPath} (optional) \u2013 a string containing the canonical attribute path for the package. If present, it will be passed to the update script instead of the attribute path on which the package was discovered during Nixpkgs traversal.</li> <li>[<code>supportedFeatures</code>]{#var-passthru-updateScript-set-supportedFeatures} (optional) \u2013 a list of the extra features the script supports.</li> </ul> <pre><code>passthru.updateScript = {\n  command = [ ../../update.sh pname ];\n  attrPath = pname;\n  supportedFeatures = [ \u2026 ];\n};\n</code></pre> <p>::: {.tip} A common pattern is to use the <code>nix-update-script</code> attribute provided in Nixpkgs, which runs <code>nix-update</code>:</p> <pre><code>passthru.updateScript = nix-update-script { };\n</code></pre> <p>For simple packages, this is often enough, and will ensure that the package is updated automatically by <code>nixpkgs-update</code> when a new version is released. The update bot runs periodically to attempt to automatically update packages, and will run <code>passthru.updateScript</code> if set. While not strictly necessary if the project is listed on Repology, using <code>nix-update-script</code> allows the package to update via many more sources (e.g. GitHub releases). :::</p>"},{"location":"stdenv/stdenv.chapter.html#var-passthru-updateScript-execution","title":"How update scripts are executed?","text":"<p>Update scripts are to be invoked by <code>maintainers/scripts/update.nix</code> script. You can run <code>nix-shell maintainers/scripts/update.nix</code> in the root of Nixpkgs repository for information on how to use it. <code>update.nix</code> offers several modes for selecting packages to update (e.g. select by attribute path, traverse Nixpkgs and filter by maintainer, etc.), and it will execute update scripts for all matched packages that have an <code>updateScript</code> attribute.</p> <p>Each update script will be passed the following environment variables:</p> <ul> <li>[<code>UPDATE_NIX_NAME</code>]{#var-passthru-updateScript-env-UPDATE_NIX_NAME} \u2013 content of the <code>name</code> attribute of the updated package.</li> <li>[<code>UPDATE_NIX_PNAME</code>]{#var-passthru-updateScript-env-UPDATE_NIX_PNAME} \u2013 content of the <code>pname</code> attribute of the updated package.</li> <li>[<code>UPDATE_NIX_OLD_VERSION</code>]{#var-passthru-updateScript-env-UPDATE_NIX_OLD_VERSION} \u2013 content of the <code>version</code> attribute of the updated package.</li> <li>[<code>UPDATE_NIX_ATTR_PATH</code>]{#var-passthru-updateScript-env-UPDATE_NIX_ATTR_PATH} \u2013 attribute path the <code>update.nix</code> discovered the package on (or the canonical <code>attrPath</code> when available). Example: <code>pantheon.elementary-terminal</code></li> </ul> <p>::: {.note} An update script will be usually run from the root of the Nixpkgs repository but you should not rely on that. Also note that <code>update.nix</code> executes update scripts in parallel by default so you should avoid running <code>git commit</code> or any other commands that cannot handle that. :::</p> <p>::: {.tip} While update scripts should not create commits themselves, <code>maintainers/scripts/update.nix</code> supports automatically creating commits when running it with <code>--argstr commit true</code>. If you need to customize commit message, you can have the update script implement <code>commit</code> feature. :::</p>"},{"location":"stdenv/stdenv.chapter.html#var-passthru-updateScript-supported-features","title":"Supported features","text":""},{"location":"stdenv/stdenv.chapter.html#var-passthru-updateScript-commit","title":"<code>commit</code>","text":"<p>This feature allows update scripts to ask <code>update.nix</code> to create Git commits.</p> <p>When support of this feature is declared, whenever the update script exits with <code>0</code> return status, it is expected to print a JSON list containing an object described below for each updated attribute to standard output.</p> <p>When <code>update.nix</code> is run with <code>--argstr commit true</code> arguments, it will create a separate commit for each of the objects. An empty list can be returned when the script did not update any files, for example, when the package is already at the latest version.</p> <p>The commit object contains the following values:</p> <ul> <li>[<code>attrPath</code>]{#var-passthru-updateScript-commit-attrPath} \u2013 a string containing attribute path.</li> <li>[<code>oldVersion</code>]{#var-passthru-updateScript-commit-oldVersion} \u2013 a string containing old version.</li> <li>[<code>newVersion</code>]{#var-passthru-updateScript-commit-newVersion} \u2013 a string containing new version.</li> <li>[<code>files</code>]{#var-passthru-updateScript-commit-files} \u2013 a non-empty list of file paths (as strings) to add to the commit.</li> <li>[<code>commitBody</code>]{#var-passthru-updateScript-commit-commitBody} (optional) \u2013 a string with extra content to be appended to the default commit message (useful for adding changelog links).</li> <li>[<code>commitMessage</code>]{#var-passthru-updateScript-commit-commitMessage} (optional) \u2013 a string to use instead of the default commit message.</li> </ul> <p>If the returned array contains exactly one object (e.g. <code>[{}]</code>), all values are optional and will be determined automatically.</p> <p>::: {.example #var-passthru-updateScript-example-commit}</p>"},{"location":"stdenv/stdenv.chapter.html#standard-output-of-an-update-script-using-commit-feature","title":"Standard output of an update script using commit feature","text":"<p><pre><code>[\n  {\n    \"attrPath\": \"volume_key\",\n    \"oldVersion\": \"0.3.11\",\n    \"newVersion\": \"0.3.12\",\n    \"files\": [\n      \"/path/to/nixpkgs/pkgs/development/libraries/volume-key/default.nix\"\n    ]\n  }\n]\n</code></pre> :::</p>"},{"location":"stdenv/stdenv.chapter.html#mkderivation-recursive-attributes","title":"Fixed-point arguments of <code>mkDerivation</code>","text":"<p>If you pass a function to <code>mkDerivation</code>, it will receive as its argument the final arguments, including the overrides when reinvoked via <code>overrideAttrs</code>. For example:</p> <pre><code>mkDerivation (finalAttrs: {\n  pname = \"hello\";\n  withFeature = true;\n  configureFlags =\n    lib.optionals finalAttrs.withFeature [\"--with-feature\"];\n})\n</code></pre> <p>Note that this does not use the <code>rec</code> keyword to reuse <code>withFeature</code> in <code>configureFlags</code>. The <code>rec</code> keyword works at the syntax level and is unaware of overriding.</p> <p>Instead, the definition references <code>finalAttrs</code>, allowing users to change <code>withFeature</code> consistently with <code>overrideAttrs</code>.</p> <p><code>finalAttrs</code> also contains the attribute <code>finalPackage</code>, which includes the output paths, etc.</p> <p>Let's look at a more elaborate example to understand the differences between various bindings:</p> <pre><code># `pkg` is the _original_ definition (for illustration purposes)\nlet pkg =\n  mkDerivation (finalAttrs: {\n    # ...\n\n    # An example attribute\n    packages = [];\n\n    # `passthru.tests` is a commonly defined attribute.\n    passthru.tests.simple = f finalAttrs.finalPackage;\n\n    # An example of an attribute containing a function\n    passthru.appendPackages = packages':\n      finalAttrs.finalPackage.overrideAttrs (newSelf: super: {\n        packages = super.packages ++ packages';\n      });\n\n    # For illustration purposes; referenced as\n    # `(pkg.overrideAttrs(x)).finalAttrs` etc in the text below.\n    passthru.finalAttrs = finalAttrs;\n    passthru.original = pkg;\n  });\nin pkg\n</code></pre> <p>Unlike the <code>pkg</code> binding in the above example, the <code>finalAttrs</code> parameter always references the final attributes. For instance <code>(pkg.overrideAttrs(x)).finalAttrs.finalPackage</code> is identical to <code>pkg.overrideAttrs(x)</code>, whereas <code>(pkg.overrideAttrs(x)).original</code> is the same as the original <code>pkg</code>.</p> <p>See also the section about <code>passthru.tests</code>.</p>"},{"location":"stdenv/stdenv.chapter.html#sec-stdenv-phases","title":"Phases","text":"<p><code>stdenv.mkDerivation</code> sets the Nix derivation's builder to a script that loads the stdenv <code>setup.sh</code> bash library and calls <code>genericBuild</code>. Most packaging functions rely on this default builder.</p> <p>This generic command either invokes a script at buildCommandPath, or a buildCommand, or a number of phases. Package builds are split into phases to make it easier to override specific parts of the build (e.g., unpacking the sources or installing the binaries).</p> <p>Each phase can be overridden in its entirety either by setting the environment variable <code>namePhase</code> to a string containing some shell commands to be executed, or by redefining the shell function <code>namePhase</code>. The former is convenient to override a phase from the derivation, while the latter is convenient from a build script. However, typically one only wants to add some commands to a phase, e.g. by defining <code>postInstall</code> or <code>preFixup</code>, as skipping some of the default actions may have unexpected consequences. The default script for each phase is defined in the file <code>pkgs/stdenv/generic/setup.sh</code>.</p> <p>When overriding a phase, for example <code>installPhase</code>, it is important to start with <code>runHook preInstall</code> and end it with <code>runHook postInstall</code>, otherwise <code>preInstall</code> and <code>postInstall</code> will not be run. Even if you don't use them directly, it is good practice to do so anyways for downstream users who would want to add a <code>postInstall</code> by overriding your derivation.</p> <p>While inside an interactive <code>nix-shell</code>, if you wanted to run all phases in the order they would be run in an actual build, you can invoke <code>genericBuild</code> yourself.</p>"},{"location":"stdenv/stdenv.chapter.html#ssec-controlling-phases","title":"Controlling phases","text":"<p>There are a number of variables that control what phases are executed and in what order:</p>"},{"location":"stdenv/stdenv.chapter.html#variables-affecting-phase-control","title":"Variables affecting phase control","text":""},{"location":"stdenv/stdenv.chapter.html#var-stdenv-phases","title":"<code>phases</code>","text":"<p>Specifies the phases. You can change the order in which phases are executed, or add new phases, by setting this variable. If it\u2019s not set, the default value is used, which is <code>$prePhases unpackPhase patchPhase $preConfigurePhases configurePhase $preBuildPhases buildPhase checkPhase $preInstallPhases installPhase fixupPhase installCheckPhase $preDistPhases distPhase $postPhases</code>.</p> <p>It is discouraged to set this variable, as it is easy to miss some important functionality hidden in some of the less obviously needed phases (like <code>fixupPhase</code> which patches the shebang of scripts). Usually, if you just want to add a few phases, it\u2019s more convenient to set one of the variables below (such as <code>preInstallPhases</code>).</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-prePhases","title":"<code>prePhases</code>","text":"<p>Additional phases executed before any of the default phases.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-preConfigurePhases","title":"<code>preConfigurePhases</code>","text":"<p>Additional phases executed just before the configure phase.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-preBuildPhases","title":"<code>preBuildPhases</code>","text":"<p>Additional phases executed just before the build phase.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-preInstallPhases","title":"<code>preInstallPhases</code>","text":"<p>Additional phases executed just before the install phase.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-preFixupPhases","title":"<code>preFixupPhases</code>","text":"<p>Additional phases executed just before the fixup phase.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-preDistPhases","title":"<code>preDistPhases</code>","text":"<p>Additional phases executed just before the distribution phase.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-postPhases","title":"<code>postPhases</code>","text":"<p>Additional phases executed after any of the default phases.</p>"},{"location":"stdenv/stdenv.chapter.html#ssec-unpack-phase","title":"The unpack phase","text":"<p>The unpack phase is responsible for unpacking the source code of the package. The default implementation of <code>unpackPhase</code> unpacks the source files listed in the <code>src</code> environment variable to the current directory. It supports the following files by default:</p>"},{"location":"stdenv/stdenv.chapter.html#tar-files","title":"Tar files","text":"<p>These can optionally be compressed using <code>gzip</code> (<code>.tar.gz</code>, <code>.tgz</code> or <code>.tar.Z</code>), <code>bzip2</code> (<code>.tar.bz2</code>, <code>.tbz2</code> or <code>.tbz</code>) or <code>xz</code> (<code>.tar.xz</code>, <code>.tar.lzma</code> or <code>.txz</code>).</p>"},{"location":"stdenv/stdenv.chapter.html#zip-files","title":"Zip files","text":"<p>Zip files are unpacked using <code>unzip</code>. However, <code>unzip</code> is not in the standard environment, so you should add it to <code>nativeBuildInputs</code> yourself.</p>"},{"location":"stdenv/stdenv.chapter.html#directories-in-the-nix-store","title":"Directories in the Nix store","text":"<p>These are copied to the current directory. The hash part of the file name is stripped, e.g. <code>/nix/store/1wydxgby13cz...-my-sources</code> would be copied to <code>my-sources</code>.</p> <p>Additional file types can be supported by setting the <code>unpackCmd</code> variable (see below).</p>"},{"location":"stdenv/stdenv.chapter.html#variables-controlling-the-unpack-phase","title":"Variables controlling the unpack phase","text":""},{"location":"stdenv/stdenv.chapter.html#var-stdenv-src","title":"<code>srcs</code> / <code>src</code>","text":"<p>The list of source files or directories to be unpacked or copied. One of these must be set. Note that if you use <code>srcs</code>, you should also set <code>sourceRoot</code> or <code>setSourceRoot</code>.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-sourceRoot","title":"<code>sourceRoot</code>","text":"<p>After unpacking all of <code>src</code> and <code>srcs</code>, if neither of <code>sourceRoot</code> and <code>setSourceRoot</code> are set, <code>unpackPhase</code> of the generic builder checks that the unpacking produced a single directory and moves the current working directory into it.</p> <p>If <code>unpackPhase</code> produces multiple source directories, you should set <code>sourceRoot</code> to the name of the intended directory. You can also set <code>sourceRoot = \".\";</code> if you want to control it yourself in a later phase.</p> <p>For example, if your want your build to start in a sub-directory inside your sources, and you are using <code>fetchzip</code>-derived <code>src</code> (like <code>fetchFromGitHub</code> or similar), you need to set <code>sourceRoot = \"${src.name}/my-sub-directory\"</code>.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-setSourceRoot","title":"<code>setSourceRoot</code>","text":"<p>Alternatively to setting <code>sourceRoot</code>, you can set <code>setSourceRoot</code> to a shell command to be evaluated by the unpack phase after the sources have been unpacked. This command must set <code>sourceRoot</code>.</p> <p>For example, if you are using <code>fetchurl</code> on an archive file that gets unpacked into a single directory the name of which changes between package versions, and you want your build to start in its sub-directory, you need to set <code>setSourceRoot = \"sourceRoot=$(echo */my-sub-directory)\";</code>, or in the case of multiple sources, you could use something more specific, like <code>setSourceRoot = \"sourceRoot=$(echo ${pname}-*/my-sub-directory)\";</code>.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-preUnpack","title":"<code>preUnpack</code>","text":"<p>Hook executed at the start of the unpack phase.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-postUnpack","title":"<code>postUnpack</code>","text":"<p>Hook executed at the end of the unpack phase.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-dontUnpack","title":"<code>dontUnpack</code>","text":"<p>Set to true to skip the unpack phase.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-dontMakeSourcesWritable","title":"<code>dontMakeSourcesWritable</code>","text":"<p>If set to <code>1</code>, the unpacked sources are not made writable. By default, they are made writable to prevent problems with read-only sources. For example, copied store directories would be read-only without this.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-unpackCmd","title":"<code>unpackCmd</code>","text":"<p>The unpack phase evaluates the string <code>$unpackCmd</code> for any unrecognised file. The path to the current source file is contained in the <code>curSrc</code> variable.</p>"},{"location":"stdenv/stdenv.chapter.html#ssec-patch-phase","title":"The patch phase","text":"<p>The patch phase applies the list of patches defined in the <code>patches</code> variable.</p>"},{"location":"stdenv/stdenv.chapter.html#variables-controlling-the-patch-phase","title":"Variables controlling the patch phase","text":""},{"location":"stdenv/stdenv.chapter.html#var-stdenv-dontPatch","title":"<code>dontPatch</code>","text":"<p>Set to true to skip the patch phase.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-patches","title":"<code>patches</code>","text":"<p>The list of patches. They must be in the format accepted by the <code>patch</code> command, and may optionally be compressed using <code>gzip</code> (<code>.gz</code>), <code>bzip2</code> (<code>.bz2</code>) or <code>xz</code> (<code>.xz</code>).</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-patchFlags","title":"<code>patchFlags</code>","text":"<p>Flags to be passed to <code>patch</code>. If not set, the argument <code>-p1</code> is used, which causes the leading directory component to be stripped from the file names in each patch.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-prePatch","title":"<code>prePatch</code>","text":"<p>Hook executed at the start of the patch phase.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-postPatch","title":"<code>postPatch</code>","text":"<p>Hook executed at the end of the patch phase.</p>"},{"location":"stdenv/stdenv.chapter.html#ssec-configure-phase","title":"The configure phase","text":"<p>The configure phase prepares the source tree for building. The default <code>configurePhase</code> runs <code>./configure</code> (typically an Autoconf-generated script) if it exists.</p>"},{"location":"stdenv/stdenv.chapter.html#variables-controlling-the-configure-phase","title":"Variables controlling the configure phase","text":""},{"location":"stdenv/stdenv.chapter.html#var-stdenv-configureScript","title":"<code>configureScript</code>","text":"<p>The name of the configure script. It defaults to <code>./configure</code> if it exists; otherwise, the configure phase is skipped. This can actually be a command (like <code>perl ./Configure.pl</code>).</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-configureFlags","title":"<code>configureFlags</code>","text":"<p>A list of strings passed as additional arguments to the configure script.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-dontConfigure","title":"<code>dontConfigure</code>","text":"<p>Set to true to skip the configure phase.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-configureFlagsArray","title":"<code>configureFlagsArray</code>","text":"<p>A shell array containing additional arguments passed to the configure script. You must use this instead of <code>configureFlags</code> if the arguments contain spaces.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-dontAddPrefix","title":"<code>dontAddPrefix</code>","text":"<p>By default, the flag <code>--prefix=$prefix</code> is added to the configure flags. If this is undesirable, set this variable to true.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-prefix","title":"<code>prefix</code>","text":"<p>The prefix under which the package must be installed, passed via the <code>--prefix</code> option to the configure script. It defaults to <code>$out</code>.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-prefixKey","title":"<code>prefixKey</code>","text":"<p>The key to use when specifying the prefix. By default, this is set to <code>--prefix=</code> as that is used by the majority of packages.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-dontAddStaticConfigureFlags","title":"<code>dontAddStaticConfigureFlags</code>","text":"<p>By default, when building statically, stdenv will try to add build system appropriate configure flags to try to enable static builds.</p> <p>If this is undesirable, set this variable to true.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-dontAddDisableDepTrack","title":"<code>dontAddDisableDepTrack</code>","text":"<p>By default, the flag <code>--disable-dependency-tracking</code> is added to the configure flags to speed up Automake-based builds. If this is undesirable, set this variable to true.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-dontFixLibtool","title":"<code>dontFixLibtool</code>","text":"<p>By default, the configure phase applies some special hackery to all files called <code>ltmain.sh</code> before running the configure script in order to improve the purity of Libtool-based packages [^footnote-stdenv-sys-lib-search-path] . If this is undesirable, set this variable to true.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-dontDisableStatic","title":"<code>dontDisableStatic</code>","text":"<p>By default, when the configure script has <code>--enable-static</code>, the option <code>--disable-static</code> is added to the configure flags.</p> <p>If this is undesirable, set this variable to true.  It is automatically set to true when building statically, for example through <code>pkgsStatic</code>.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-configurePlatforms","title":"<code>configurePlatforms</code>","text":"<p>By default, when cross compiling, the configure script has <code>--build=...</code> and <code>--host=...</code> passed. Packages can instead pass <code>[ \"build\" \"host\" \"target\" ]</code> or a subset to control exactly which platform flags are passed. Compilers and other tools can use this to also pass the target platform. [^footnote-stdenv-build-time-guessing-impurity]</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-preConfigure","title":"<code>preConfigure</code>","text":"<p>Hook executed at the start of the configure phase.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-postConfigure","title":"<code>postConfigure</code>","text":"<p>Hook executed at the end of the configure phase.</p>"},{"location":"stdenv/stdenv.chapter.html#build-phase","title":"The build phase","text":"<p>The build phase is responsible for actually building the package (e.g. compiling it). The default <code>buildPhase</code> calls <code>make</code> if a file named <code>Makefile</code>, <code>makefile</code> or <code>GNUmakefile</code> exists in the current directory (or the <code>makefile</code> is explicitly set); otherwise it does nothing.</p>"},{"location":"stdenv/stdenv.chapter.html#variables-controlling-the-build-phase","title":"Variables controlling the build phase","text":""},{"location":"stdenv/stdenv.chapter.html#var-stdenv-dontBuild","title":"<code>dontBuild</code>","text":"<p>Set to true to skip the build phase.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-makefile","title":"<code>makefile</code>","text":"<p>The file name of the Makefile.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-makeFlags","title":"<code>makeFlags</code>","text":"<p>A list of strings passed as additional flags to <code>make</code>. These flags are also used by the default install and check phase. For setting make flags specific to the build phase, use <code>buildFlags</code> (see below).</p> <pre><code>makeFlags = [ \"PREFIX=$(out)\" ];\n</code></pre> <p>::: {.note} The flags are quoted in bash, but environment variables can be specified by using the make syntax. :::</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-makeFlagsArray","title":"<code>makeFlagsArray</code>","text":"<p>A shell array containing additional arguments passed to <code>make</code>. You must use this instead of <code>makeFlags</code> if the arguments contain spaces, e.g.</p> <pre><code>preBuild = ''\n  makeFlagsArray+=(CFLAGS=\"-O0 -g\" LDFLAGS=\"-lfoo -lbar\")\n'';\n</code></pre> <p>Note that shell arrays cannot be passed through environment variables, so you cannot set <code>makeFlagsArray</code> in a derivation attribute (because those are passed through environment variables): you have to define them in shell code.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-buildFlags","title":"<code>buildFlags</code> / <code>buildFlagsArray</code>","text":"<p>A list of strings passed as additional flags to <code>make</code>. Like <code>makeFlags</code> and <code>makeFlagsArray</code>, but only used by the build phase. Any build targets should be specified as part of the <code>buildFlags</code>.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-preBuild","title":"<code>preBuild</code>","text":"<p>Hook executed at the start of the build phase.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-postBuild","title":"<code>postBuild</code>","text":"<p>Hook executed at the end of the build phase.</p> <p>You can set flags for <code>make</code> through the <code>makeFlags</code> variable.</p> <p>Before and after running <code>make</code>, the hooks <code>preBuild</code> and <code>postBuild</code> are called, respectively.</p>"},{"location":"stdenv/stdenv.chapter.html#ssec-check-phase","title":"The check phase","text":"<p>The check phase checks whether the package was built correctly by running its test suite. The default <code>checkPhase</code> calls <code>make $checkTarget</code>, but only if the <code>doCheck</code> variable is enabled.</p>"},{"location":"stdenv/stdenv.chapter.html#variables-controlling-the-check-phase","title":"Variables controlling the check phase","text":""},{"location":"stdenv/stdenv.chapter.html#var-stdenv-doCheck","title":"<code>doCheck</code>","text":"<p>Controls whether the check phase is executed. By default it is skipped, but if <code>doCheck</code> is set to true, the check phase is usually executed. Thus you should set</p> <pre><code>doCheck = true;\n</code></pre> <p>in the derivation to enable checks. The exception is cross compilation. Cross compiled builds never run tests, no matter how <code>doCheck</code> is set, as the newly-built program won\u2019t run on the platform used to build it.</p>"},{"location":"stdenv/stdenv.chapter.html#makeflags-makeflagsarray-makefile","title":"<code>makeFlags</code> / <code>makeFlagsArray</code> / <code>makefile</code>","text":"<p>See the build phase for details.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-checkTarget","title":"<code>checkTarget</code>","text":"<p>The <code>make</code> target that runs the tests. If unset, use <code>check</code> if it exists, otherwise <code>test</code>; if neither is found, do nothing.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-checkFlags","title":"<code>checkFlags</code> / <code>checkFlagsArray</code>","text":"<p>A list of strings passed as additional flags to <code>make</code>. Like <code>makeFlags</code> and <code>makeFlagsArray</code>, but only used by the check phase. Unlike with <code>buildFlags</code>, the <code>checkTarget</code> is automatically added to the <code>make</code> invocation in addition to any <code>checkFlags</code> specified.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-checkInputs","title":"<code>checkInputs</code>","text":"<p>A list of host dependencies used by the phase, usually libraries linked into executables built during tests. This gets included in <code>buildInputs</code> when <code>doCheck</code> is set.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-nativeCheckInputs","title":"<code>nativeCheckInputs</code>","text":"<p>A list of native dependencies used by the phase, notably tools needed on <code>$PATH</code>. This gets included in <code>nativeBuildInputs</code> when <code>doCheck</code> is set.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-preCheck","title":"<code>preCheck</code>","text":"<p>Hook executed at the start of the check phase.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-postCheck","title":"<code>postCheck</code>","text":"<p>Hook executed at the end of the check phase.</p>"},{"location":"stdenv/stdenv.chapter.html#ssec-install-phase","title":"The install phase","text":"<p>The install phase is responsible for installing the package in the Nix store under <code>out</code>. The default <code>installPhase</code> creates the directory <code>$out</code> and calls <code>make install</code>.</p>"},{"location":"stdenv/stdenv.chapter.html#variables-controlling-the-install-phase","title":"Variables controlling the install phase","text":""},{"location":"stdenv/stdenv.chapter.html#var-stdenv-dontInstall","title":"<code>dontInstall</code>","text":"<p>Set to true to skip the install phase.</p>"},{"location":"stdenv/stdenv.chapter.html#makeflags-makeflagsarray-makefile-1","title":"<code>makeFlags</code> / <code>makeFlagsArray</code> / <code>makefile</code>","text":"<p>See the build phase for details.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-installTargets","title":"<code>installTargets</code>","text":"<p>The make targets that perform the installation. Defaults to <code>install</code>. Example:</p> <pre><code>installTargets = \"install-bin install-doc\";\n</code></pre>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-installFlags","title":"<code>installFlags</code> / <code>installFlagsArray</code>","text":"<p>A list of strings passed as additional flags to <code>make</code>. Like <code>makeFlags</code> and <code>makeFlagsArray</code>, but only used by the install phase. Unlike with <code>buildFlags</code>, the <code>installTargets</code> are automatically added to the <code>make</code> invocation in addition to any <code>installFlags</code> specified.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-preInstall","title":"<code>preInstall</code>","text":"<p>Hook executed at the start of the install phase.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-postInstall","title":"<code>postInstall</code>","text":"<p>Hook executed at the end of the install phase.</p>"},{"location":"stdenv/stdenv.chapter.html#ssec-fixup-phase","title":"The fixup phase","text":"<p>The fixup phase performs (Nix-specific) post-processing actions on the files installed under <code>$out</code> by the install phase. The default <code>fixupPhase</code> does the following:</p> <ul> <li>It moves the <code>man/</code>, <code>doc/</code> and <code>info/</code> subdirectories of <code>$out</code> to <code>share/</code>.</li> <li>It strips libraries and executables of debug information.</li> <li>On Linux, it applies the <code>patchelf</code> command to ELF executables and libraries to remove unused directories from the <code>RPATH</code> in order to prevent unnecessary runtime dependencies.</li> <li>It rewrites the interpreter paths of shell scripts to paths found in <code>PATH</code>. E.g., <code>/usr/bin/perl</code> will be rewritten to <code>/nix/store/some-perl/bin/perl</code> found in <code>PATH</code>. See  for details.</li> </ul>"},{"location":"stdenv/stdenv.chapter.html#variables-controlling-the-fixup-phase","title":"Variables controlling the fixup phase","text":""},{"location":"stdenv/stdenv.chapter.html#var-stdenv-dontFixup","title":"<code>dontFixup</code>","text":"<p>Set to true to skip the fixup phase.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-dontStrip","title":"<code>dontStrip</code>","text":"<p>If set, libraries and executables are not stripped. By default, they are.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-dontStripHost","title":"<code>dontStripHost</code>","text":"<p>Like <code>dontStrip</code>, but only affects the <code>strip</code> command targeting the package\u2019s host platform. Useful when supporting cross compilation, but otherwise feel free to ignore.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-dontStripTarget","title":"<code>dontStripTarget</code>","text":"<p>Like <code>dontStrip</code>, but only affects the <code>strip</code> command targeting the packages\u2019 target platform. Useful when supporting cross compilation, but otherwise feel free to ignore.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-dontMoveSbin","title":"<code>dontMoveSbin</code>","text":"<p>If set, files in <code>$out/sbin</code> are not moved to <code>$out/bin</code>. By default, they are.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-stripAllList","title":"<code>stripAllList</code>","text":"<p>List of directories to search for libraries and executables from which all symbols should be stripped. By default, it\u2019s empty. Stripping all symbols is risky, since it may remove not just debug symbols but also ELF information necessary for normal execution.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-stripAllListTarget","title":"<code>stripAllListTarget</code>","text":"<p>Like <code>stripAllList</code>, but only applies to packages\u2019 target platform. By default, it\u2019s empty. Useful when supporting cross compilation.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-stripAllFlags","title":"<code>stripAllFlags</code>","text":"<p>Flags passed to the <code>strip</code> command applied to the files in the directories listed in <code>stripAllList</code>. Defaults to <code>-s</code> (i.e. <code>--strip-all</code>).</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-stripDebugList","title":"<code>stripDebugList</code>","text":"<p>List of directories to search for libraries and executables from which only debugging-related symbols should be stripped. It defaults to <code>lib lib32 lib64 libexec bin sbin</code>.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-stripDebugListTarget","title":"<code>stripDebugListTarget</code>","text":"<p>Like <code>stripDebugList</code>, but only applies to packages\u2019 target platform. By default, it\u2019s empty. Useful when supporting cross compilation.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-stripDebugFlags","title":"<code>stripDebugFlags</code>","text":"<p>Flags passed to the <code>strip</code> command applied to the files in the directories listed in <code>stripDebugList</code>. Defaults to <code>-S</code> (i.e. <code>--strip-debug</code>).</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-stripExclude","title":"<code>stripExclude</code>","text":"<p>A list of filenames or path patterns to avoid stripping. A file is excluded if its name or path (from the derivation root) matches.</p> <p>This example prevents all <code>*.rlib</code> files from being stripped:</p> <pre><code>stdenv.mkDerivation {\n  # ...\n  stripExclude = [ \"*.rlib\" ]\n}\n</code></pre> <p>This example prevents files within certain paths from being stripped:</p> <pre><code>stdenv.mkDerivation {\n  # ...\n  stripExclude = [ \"lib/modules/*/build/* ]\n}\n</code></pre>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-dontPatchELF","title":"<code>dontPatchELF</code>","text":"<p>If set, the <code>patchelf</code> command is not used to remove unnecessary <code>RPATH</code> entries. Only applies to Linux.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-dontPatchShebangs","title":"<code>dontPatchShebangs</code>","text":"<p>If set, scripts starting with <code>#!</code> do not have their interpreter paths rewritten to paths in the Nix store. See  on how patching shebangs works.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-dontPruneLibtoolFiles","title":"<code>dontPruneLibtoolFiles</code>","text":"<p>If set, libtool <code>.la</code> files associated with shared libraries won\u2019t have their <code>dependency_libs</code> field cleared.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-forceShare","title":"<code>forceShare</code>","text":"<p>The list of directories that must be moved from <code>$out</code> to <code>$out/share</code>. Defaults to <code>man doc info</code>.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-setupHook","title":"<code>setupHook</code>","text":"<p>A package can export a setup hook by setting this variable. The setup hook, if defined, is copied to <code>$out/nix-support/setup-hook</code>. Environment variables are then substituted in it using <code>substituteAll</code>.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-preFixup","title":"<code>preFixup</code>","text":"<p>Hook executed at the start of the fixup phase.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-postFixup","title":"<code>postFixup</code>","text":"<p>Hook executed at the end of the fixup phase.</p>"},{"location":"stdenv/stdenv.chapter.html#stdenv-separateDebugInfo","title":"<code>separateDebugInfo</code>","text":"<p>If set to <code>true</code>, the standard environment will enable debug information in C/C++ builds. After installation, the debug information will be separated from the executables and stored in the output named <code>debug</code>. (This output is enabled automatically; you don\u2019t need to set the <code>outputs</code> attribute explicitly.) To be precise, the debug information is stored in <code>debug/lib/debug/.build-id/XX/YYYY\u2026</code>, where \\&lt;XXYYYY\u2026&gt; is the \\&lt;build ID&gt; of the binary \u2014 a SHA-1 hash of the contents of the binary. Debuggers like GDB use the build ID to look up the separated debug information.</p> <p>:::{.example #ex-gdb-debug-symbols-socat}</p>"},{"location":"stdenv/stdenv.chapter.html#enable-debug-symbols-for-use-with-gdb","title":"Enable debug symbols for use with GDB","text":"<p>To make GDB find debug information for the <code>socat</code> package and its dependencies, you can use the following <code>shell.nix</code>:</p> <pre><code>let\n  pkgs = import ./. {\n    config = {};\n    overlays = [\n      (final: prev: {\n        ncurses = prev.ncurses.overrideAttrs { separateDebugInfo = true; };\n        readline = prev.readline.overrideAttrs { separateDebugInfo = true; };\n      })\n    ];\n  };\n\n  myDebugInfoDirs = pkgs.symlinkJoin {\n    name = \"myDebugInfoDirs\";\n    paths = with pkgs; [\n      glibc.debug\n      ncurses.debug\n      openssl.debug\n      readline.debug\n    ];\n  };\nin\n  pkgs.mkShell {\n\n    NIX_DEBUG_INFO_DIRS = \"${pkgs.lib.getLib myDebugInfoDirs}/lib/debug\";\n\n    packages = [\n      pkgs.gdb\n      pkgs.socat\n    ];\n\n    shellHook = ''\n      ${pkgs.lib.getBin pkgs.gdb}/bin/gdb ${pkgs.lib.getBin pkgs.socat}/bin/socat\n    '';\n  }\n</code></pre> <p>This setup works as follows: - Add <code>overlays</code> to the package set, since debug symbols are disabled for <code>ncurses</code> and <code>readline</code> by default. - Create a derivation to combine all required debug symbols under one path with <code>symlinkJoin</code>. - Set the environment variable <code>NIX_DEBUG_INFO_DIRS</code> in the shell. Nixpkgs patches <code>gdb</code> to use it for looking up debug symbols. - Run <code>gdb</code> on the <code>socat</code> binary on shell startup in the <code>shellHook</code>. Here we use <code>lib.getBin</code> to ensure that the correct derivation output is selected rather than the default one.</p> <p>:::</p>"},{"location":"stdenv/stdenv.chapter.html#ssec-installCheck-phase","title":"The installCheck phase","text":"<p>The installCheck phase checks whether the package was installed correctly by running its test suite against the installed directories. The default <code>installCheck</code> calls <code>make installcheck</code>.</p> <p>It is often better to add tests that are not part of the source distribution to <code>passthru.tests</code> (see ). This avoids adding overhead to every build and enables us to run them independently.</p>"},{"location":"stdenv/stdenv.chapter.html#variables-controlling-the-installcheck-phase","title":"Variables controlling the installCheck phase","text":""},{"location":"stdenv/stdenv.chapter.html#var-stdenv-doInstallCheck","title":"<code>doInstallCheck</code>","text":"<p>Controls whether the installCheck phase is executed. By default it is skipped, but if <code>doInstallCheck</code> is set to true, the installCheck phase is usually executed. Thus you should set</p> <pre><code>doInstallCheck = true;\n</code></pre> <p>in the derivation to enable install checks. The exception is cross compilation. Cross compiled builds never run tests, no matter how <code>doInstallCheck</code> is set, as the newly-built program won\u2019t run on the platform used to build it.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-installCheckTarget","title":"<code>installCheckTarget</code>","text":"<p>The make target that runs the install tests. Defaults to <code>installcheck</code>.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-installCheckFlags","title":"<code>installCheckFlags</code> / <code>installCheckFlagsArray</code>","text":"<p>A list of strings passed as additional flags to <code>make</code>. Like <code>makeFlags</code> and <code>makeFlagsArray</code>, but only used by the installCheck phase.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-installCheckInputs","title":"<code>installCheckInputs</code>","text":"<p>A list of host dependencies used by the phase, usually libraries linked into executables built during tests. This gets included in <code>buildInputs</code> when <code>doInstallCheck</code> is set.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-nativeInstallCheckInputs","title":"<code>nativeInstallCheckInputs</code>","text":"<p>A list of native dependencies used by the phase, notably tools needed on <code>$PATH</code>. This gets included in <code>nativeBuildInputs</code> when <code>doInstallCheck</code> is set.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-preInstallCheck","title":"<code>preInstallCheck</code>","text":"<p>Hook executed at the start of the installCheck phase.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-postInstallCheck","title":"<code>postInstallCheck</code>","text":"<p>Hook executed at the end of the installCheck phase.</p>"},{"location":"stdenv/stdenv.chapter.html#ssec-distribution-phase","title":"The distribution phase","text":"<p>The distribution phase is intended to produce a source distribution of the package. The default <code>distPhase</code> first calls <code>make dist</code>, then it copies the resulting source tarballs to <code>$out/tarballs/</code>. This phase is only executed if the attribute <code>doDist</code> is set.</p>"},{"location":"stdenv/stdenv.chapter.html#variables-controlling-the-distribution-phase","title":"Variables controlling the distribution phase","text":""},{"location":"stdenv/stdenv.chapter.html#var-stdenv-doDist","title":"<code>doDist</code>","text":"<p>If set, the distribution phase is executed.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-distTarget","title":"<code>distTarget</code>","text":"<p>The make target that produces the distribution. Defaults to <code>dist</code>.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-distFlags","title":"<code>distFlags</code> / <code>distFlagsArray</code>","text":"<p>Additional flags passed to <code>make</code>.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-tarballs","title":"<code>tarballs</code>","text":"<p>The names of the source distribution files to be copied to <code>$out/tarballs/</code>. It can contain shell wildcards. The default is <code>*.tar.gz</code>.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-dontCopyDist","title":"<code>dontCopyDist</code>","text":"<p>If set, no files are copied to <code>$out/tarballs/</code>.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-preDist","title":"<code>preDist</code>","text":"<p>Hook executed at the start of the distribution phase.</p>"},{"location":"stdenv/stdenv.chapter.html#var-stdenv-postDist","title":"<code>postDist</code>","text":"<p>Hook executed at the end of the distribution phase.</p>"},{"location":"stdenv/stdenv.chapter.html#ssec-stdenv-functions","title":"Shell functions and utilities","text":"<p>The standard environment provides a number of useful functions.</p>"},{"location":"stdenv/stdenv.chapter.html#fun-makeWrapper","title":"<code>makeWrapper</code> \\&lt;executable&gt; \\&lt;wrapperfile&gt; \\&lt;args&gt;","text":"<p>Constructs a wrapper for a program with various possible arguments. It is defined as part of 2 setup-hooks named <code>makeWrapper</code> and <code>makeBinaryWrapper</code> that implement the same bash functions. Hence, to use it you have to add <code>makeWrapper</code> to your <code>nativeBuildInputs</code>. Here's an example usage:</p> <pre><code># adds `FOOBAR=baz` to `$out/bin/foo`\u2019s environment\nmakeWrapper $out/bin/foo $wrapperfile --set FOOBAR baz\n\n# Prefixes the binary paths of `hello` and `git`\n# and suffixes the binary path of `xdg-utils`.\n# Be advised that paths often should be patched in directly\n# (via string replacements or in `configurePhase`).\nmakeWrapper $out/bin/foo $wrapperfile \\\n  --prefix PATH : ${lib.makeBinPath [ hello git ]} \\\n  --suffix PATH : ${lib.makeBinPath [ xdg-utils ]}\n</code></pre> <p>Packages may expect or require other utilities to be available at runtime. <code>makeWrapper</code> can be used to add packages to a <code>PATH</code> environment variable local to a wrapper.</p> <p>Use <code>--prefix</code> to explicitly set dependencies in <code>PATH</code>.</p> <p>::: {.note} <code>--prefix</code> essentially hard-codes dependencies into the wrapper. They cannot be overridden without rebuilding the package. :::</p> <p>If dependencies should be resolved at runtime, use <code>--suffix</code> to append fallback values to <code>PATH</code>.</p> <p>There\u2019s many more kinds of arguments, they are documented in <code>nixpkgs/pkgs/build-support/setup-hooks/make-wrapper.sh</code> for the <code>makeWrapper</code> implementation and in <code>nixpkgs/pkgs/build-support/setup-hooks/make-binary-wrapper/make-binary-wrapper.sh</code> for the <code>makeBinaryWrapper</code> implementation.</p> <p><code>wrapProgram</code> is a convenience function you probably want to use most of the time, implemented by both <code>makeWrapper</code> and <code>makeBinaryWrapper</code>.</p> <p>Using the <code>makeBinaryWrapper</code> implementation is usually preferred, as it creates a tiny compiled wrapper executable, that can be used as a shebang interpreter. This is needed mostly on Darwin, where shebangs cannot point to scripts, due to a limitation with the <code>execve</code>-syscall. Compiled wrappers generated by <code>makeBinaryWrapper</code> can be inspected with <code>less &lt;path-to-wrapper&gt;</code> - by scrolling past the binary data you should be able to see the shell command that generated the executable and there see the environment variables that were injected into the wrapper.</p>"},{"location":"stdenv/stdenv.chapter.html#fun-remove-references-to","title":"<code>remove-references-to -t</code> \\&lt;storepath&gt; [ <code>-t</code> \\&lt;storepath&gt; ... ] \\&lt;file&gt; ...","text":"<p>Removes the references of the specified files to the specified store files. This is done without changing the size of the file by replacing the hash by <code>eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee</code>, and should work on compiled executables. This is meant to be used to remove the dependency of the output on inputs that are known to be unnecessary at runtime. Of course, reckless usage will break the patched programs. To use this, add <code>removeReferencesTo</code> to <code>nativeBuildInputs</code>.</p> <p>As <code>remove-references-to</code> is an actual executable and not a shell function, it can be used with <code>find</code>. Example removing all references to the compiler in the output: <pre><code>postInstall = ''\n  find \"$out\" -type f -exec remove-references-to -t ${stdenv.cc} '{}' +\n'';\n</code></pre></p>"},{"location":"stdenv/stdenv.chapter.html#fun-substitute","title":"<code>substitute</code> \\&lt;infile&gt; \\&lt;outfile&gt; \\&lt;subs&gt;","text":"<p>Performs string substitution on the contents of \\&lt;infile&gt;, writing the result to \\&lt;outfile&gt;. The substitutions in \\&lt;subs&gt; are of the following form:</p>"},{"location":"stdenv/stdenv.chapter.html#fun-substitute-replace-fail","title":"<code>--replace-fail</code> \\&lt;s1&gt; \\&lt;s2&gt;","text":"<p>Replace every occurrence of the string \\&lt;s1&gt; by \\&lt;s2&gt;. Will error if no change is made.</p>"},{"location":"stdenv/stdenv.chapter.html#fun-substitute-replace-warn","title":"<code>--replace-warn</code> \\&lt;s1&gt; \\&lt;s2&gt;","text":"<p>Replace every occurrence of the string \\&lt;s1&gt; by \\&lt;s2&gt;. Will print a warning if no change is made.</p>"},{"location":"stdenv/stdenv.chapter.html#fun-substitute-replace-quiet","title":"<code>--replace-quiet</code> \\&lt;s1&gt; \\&lt;s2&gt;","text":"<p>Replace every occurrence of the string \\&lt;s1&gt; by \\&lt;s2&gt;. Will do nothing if no change can be made.</p>"},{"location":"stdenv/stdenv.chapter.html#fun-substitute-subst-var","title":"<code>--subst-var</code> \\&lt;varName&gt;","text":"<p>Replace every occurrence of <code>@varName@</code> by the contents of the environment variable \\&lt;varName&gt;. This is useful for generating files from templates, using <code>@...@</code> in the template as placeholders.</p>"},{"location":"stdenv/stdenv.chapter.html#fun-substitute-subst-var-by","title":"<code>--subst-var-by</code> \\&lt;varName&gt; \\&lt;s&gt;","text":"<p>Replace every occurrence of <code>@varName@</code> by the string \\&lt;s&gt;.</p> <p>Example:</p> <pre><code>substitute ./foo.in ./foo.out \\\n    --replace-fail /usr/bin/bar $bar/bin/bar \\\n    --replace-fail \"a string containing spaces\" \"some other text\" \\\n    --subst-var someVar\n</code></pre>"},{"location":"stdenv/stdenv.chapter.html#fun-substituteInPlace","title":"<code>substituteInPlace</code> \\&lt;multiple files&gt; \\&lt;subs&gt;","text":"<p>Like <code>substitute</code>, but performs the substitutions in place on the files passed.</p>"},{"location":"stdenv/stdenv.chapter.html#fun-substituteAll","title":"<code>substituteAll</code> \\&lt;infile&gt; \\&lt;outfile&gt;","text":"<p>Replaces every occurrence of <code>@varName@</code>, where \\&lt;varName&gt; is any environment variable, in \\&lt;infile&gt;, writing the result to \\&lt;outfile&gt;. For instance, if \\&lt;infile&gt; has the contents</p> <pre><code>#! @bash@/bin/sh\nPATH=@coreutils@/bin\necho @foo@\n</code></pre> <p>and the environment contains <code>bash=/nix/store/bmwp0q28cf21...-bash-3.2-p39</code> and <code>coreutils=/nix/store/68afga4khv0w...-coreutils-6.12</code>, but does not contain the variable <code>foo</code>, then the output will be</p> <pre><code>#! /nix/store/bmwp0q28cf21...-bash-3.2-p39/bin/sh\nPATH=/nix/store/68afga4khv0w...-coreutils-6.12/bin\necho @foo@\n</code></pre> <p>That is, no substitution is performed for undefined variables.</p> <p>Environment variables that start with an uppercase letter or an underscore are filtered out, to prevent global variables (like <code>HOME</code>) or private variables (like <code>__ETC_PROFILE_DONE</code>) from accidentally getting substituted. The variables also have to be valid bash \"names\", as defined in the bash manpage (alphanumeric or <code>_</code>, must not start with a number).</p>"},{"location":"stdenv/stdenv.chapter.html#fun-substituteAllInPlace","title":"<code>substituteAllInPlace</code> \\&lt;file&gt;","text":"<p>Like <code>substituteAll</code>, but performs the substitutions in place on the file \\&lt;file&gt;.</p>"},{"location":"stdenv/stdenv.chapter.html#fun-stripHash","title":"<code>stripHash</code> \\&lt;path&gt;","text":"<p>Strips the directory and hash part of a store path, outputting the name part to <code>stdout</code>. For example:</p> <pre><code># prints coreutils-8.24\nstripHash \"/nix/store/9s9r019176g7cvn2nvcw41gsp862y6b4-coreutils-8.24\"\n</code></pre> <p>If you wish to store the result in another variable, then the following idiom may be useful:</p> <pre><code>name=\"/nix/store/9s9r019176g7cvn2nvcw41gsp862y6b4-coreutils-8.24\"\nsomeVar=$(stripHash $name)\n</code></pre>"},{"location":"stdenv/stdenv.chapter.html#fun-wrapProgram","title":"<code>wrapProgram</code> \\&lt;executable&gt; \\&lt;makeWrapperArgs&gt;","text":"<p>Convenience function for <code>makeWrapper</code> that replaces <code>&lt;executable&gt;</code> with a wrapper that executes the original program. It takes all the same arguments as <code>makeWrapper</code>, except for <code>--inherit-argv0</code> (used by the <code>makeBinaryWrapper</code> implementation) and <code>--argv0</code> (used by both <code>makeWrapper</code> and <code>makeBinaryWrapper</code> wrapper implementations).</p> <p>If you will apply it multiple times, it will overwrite the wrapper file and you will end up with double wrapping, which should be avoided.</p>"},{"location":"stdenv/stdenv.chapter.html#fun-prependToVar","title":"<code>prependToVar</code> \\&lt;variableName&gt; \\&lt;elements...&gt;","text":"<p>Prepend elements to a variable.</p> <p>Example:</p> <pre><code>$ configureFlags=\"--disable-static\"\n$ prependToVar configureFlags --disable-dependency-tracking --enable-foo\n$ echo $configureFlags\n--disable-dependency-tracking --enable-foo --disable-static\n</code></pre>"},{"location":"stdenv/stdenv.chapter.html#fun-appendToVar","title":"<code>appendToVar</code> \\&lt;variableName&gt; \\&lt;elements...&gt;","text":"<p>Append elements to a variable.</p> <p>Example:</p> <pre><code>$ configureFlags=\"--disable-static\"\n$ appendToVar configureFlags --disable-dependency-tracking --enable-foo\n$ echo $configureFlags\n--disable-static --disable-dependency-tracking --enable-foo\n</code></pre>"},{"location":"stdenv/stdenv.chapter.html#ssec-setup-hooks","title":"Package setup hooks","text":"<p>Nix itself considers a build-time dependency as merely something that should previously be built and accessible at build time\u2014packages themselves are on their own to perform any additional setup. In most cases, that is fine, and the downstream derivation can deal with its own dependencies. But for a few common tasks, that would result in almost every package doing the same sort of setup work\u2014depending not on the package itself, but entirely on which dependencies were used.</p> <p>In order to alleviate this burden, the setup hook mechanism was written, where any package can include a shell script that [by convention rather than enforcement by Nix], any downstream reverse-dependency will source as part of its build process. That allows the downstream dependency to merely specify its dependencies, and lets those dependencies effectively initialize themselves. No boilerplate mirroring the list of dependencies is needed.</p> <p>The setup hook mechanism is a bit of a sledgehammer though: a powerful feature with a broad and indiscriminate area of effect. The combination of its power and implicit use may be expedient, but isn\u2019t without costs. Nix itself is unchanged, but the spirit of added dependencies being effect-free is violated even if the latter isn\u2019t. For example, if a derivation path is mentioned more than once, Nix itself doesn\u2019t care and makes sure the dependency derivation is already built just the same\u2014depending is just needing something to exist, and needing is idempotent. However, a dependency specified twice will have its setup hook run twice, and that could easily change the build environment (though a well-written setup hook will therefore strive to be idempotent so this is in fact not observable). More broadly, setup hooks are anti-modular in that multiple dependencies, whether the same or different, should not interfere and yet their setup hooks may well do so.</p> <p>The most typical use of the setup hook is actually to add other hooks which are then run (i.e. after all the setup hooks) on each dependency. For example, the C compiler wrapper\u2019s setup hook feeds itself flags for each dependency that contains relevant libraries and headers. This is done by defining a bash function, and appending its name to one of <code>envBuildBuildHooks</code>, <code>envBuildHostHooks</code>, <code>envBuildTargetHooks</code>, <code>envHostHostHooks</code>, <code>envHostTargetHooks</code>, or <code>envTargetTargetHooks</code>. These 6 bash variables correspond to the 6 sorts of dependencies by platform (there\u2019s 12 total but we ignore the propagated/non-propagated axis).</p> <p>Packages adding a hook should not hard code a specific hook, but rather choose a variable relative to how they are included. Returning to the C compiler wrapper example, if the wrapper itself is an <code>n</code> dependency, then it only wants to accumulate flags from <code>n + 1</code> dependencies, as only those ones match the compiler\u2019s target platform. The <code>hostOffset</code> variable is defined with the current dependency\u2019s host offset <code>targetOffset</code> with its target offset, before its setup hook is sourced. Additionally, since most environment hooks don\u2019t care about the target platform, that means the setup hook can append to the right bash array by doing something like</p> <pre><code>addEnvHooks \"$hostOffset\" myBashFunction\n</code></pre> <p>The existence of setups hooks has long been documented and packages inside Nixpkgs are free to use this mechanism. Other packages, however, should not rely on these mechanisms not changing between Nixpkgs versions. Because of the existing issues with this system, there\u2019s little benefit from mandating it be stable for any period of time.</p> <p>First, let\u2019s cover some setup hooks that are part of Nixpkgs default <code>stdenv</code>. This means that they are run for every package built using <code>stdenv.mkDerivation</code> or when using a custom builder that has <code>source $stdenv/setup</code>. Some of these are platform specific, so they may run on Linux but not Darwin or vice-versa.</p>"},{"location":"stdenv/stdenv.chapter.html#move-docs.sh","title":"<code>move-docs.sh</code>","text":"<p>This setup hook moves any installed documentation to the <code>/share</code> subdirectory directory. This includes the man, doc and info directories. This is needed for legacy programs that do not know how to use the <code>share</code> subdirectory.</p>"},{"location":"stdenv/stdenv.chapter.html#compress-man-pages.sh","title":"<code>compress-man-pages.sh</code>","text":"<p>This setup hook compresses any man pages that have been installed. The compression is done using the gzip program. This helps to reduce the installed size of packages.</p>"},{"location":"stdenv/stdenv.chapter.html#strip.sh","title":"<code>strip.sh</code>","text":"<p>This runs the strip command on installed binaries and libraries. This removes unnecessary information like debug symbols when they are not needed. This also helps to reduce the installed size of packages.</p>"},{"location":"stdenv/stdenv.chapter.html#patch-shebangs.sh","title":"<code>patch-shebangs.sh</code>","text":"<p>This setup hook patches installed scripts to add Nix store paths to their shebang interpreter as found in the build environment. The shebang line tells a Unix-like operating system which interpreter to use to execute the script's contents.</p> <p>::: {.note} The generic builder populates <code>PATH</code> from inputs of the derivation. :::</p>"},{"location":"stdenv/stdenv.chapter.html#patch-shebangs.sh-invocation","title":"Invocation","text":"<p>Multiple paths can be specified.</p> <pre><code>patchShebangs [--build | --host] PATH...\n</code></pre>"},{"location":"stdenv/stdenv.chapter.html#patch-shebangs.sh-invocation-flags","title":"Flags","text":"<p><code>--build</code> : Look up commands available at build time</p> <p><code>--host</code> : Look up commands available at run time</p>"},{"location":"stdenv/stdenv.chapter.html#patch-shebangs.sh-invocation-examples","title":"Examples","text":"<pre><code>patchShebangs --host /nix/store/&lt;hash&gt;-hello-1.0/bin\n</code></pre> <pre><code>patchShebangs --build configure\n</code></pre> <p><code>#!/bin/sh</code> will be rewritten to <code>#!/nix/store/&lt;hash&gt;-some-bash/bin/sh</code>.</p> <p><code>#!/usr/bin/env</code> gets special treatment: <code>#!/usr/bin/env python</code> is rewritten to <code>/nix/store/&lt;hash&gt;/bin/python</code>.</p> <p>Interpreter paths that point to a valid Nix store location are not changed.</p> <p>::: {.note} A script file must be marked as executable, otherwise it will not be considered. :::</p> <p>This mechanism ensures that the interpreter for a given script is always found and is exactly the one specified by the build.</p> <p>It can be disabled by setting <code>dontPatchShebangs</code>:</p> <pre><code>stdenv.mkDerivation {\n  # ...\n  dontPatchShebangs = true;\n  # ...\n}\n</code></pre> <p>The file <code>patch-shebangs.sh</code> defines the <code>patchShebangs</code> function. It is used to implement <code>patchShebangsAuto</code>, the setup hook that is registered to run during the fixup phase by default.</p> <p>If you need to run <code>patchShebangs</code> at build time, it must be called explicitly within one of the build phases.</p>"},{"location":"stdenv/stdenv.chapter.html#audit-tmpdir.sh","title":"<code>audit-tmpdir.sh</code>","text":"<p>This verifies that no references are left from the install binaries to the directory used to build those binaries. This ensures that the binaries do not need things outside the Nix store. This is currently supported in Linux only.</p>"},{"location":"stdenv/stdenv.chapter.html#multiple-outputs.sh","title":"<code>multiple-outputs.sh</code>","text":"<p>This setup hook adds configure flags that tell packages to install files into any one of the proper outputs listed in <code>outputs</code>. This behavior can be turned off by setting <code>setOutputFlags</code> to false in the derivation environment. See  for more information.</p>"},{"location":"stdenv/stdenv.chapter.html#move-sbin.sh","title":"<code>move-sbin.sh</code>","text":"<p>This setup hook moves any binaries installed in the <code>sbin/</code> subdirectory into <code>bin/</code>. In addition, a link is provided from <code>sbin/</code> to <code>bin/</code> for compatibility.</p>"},{"location":"stdenv/stdenv.chapter.html#move-lib64.sh","title":"<code>move-lib64.sh</code>","text":"<p>This setup hook moves any libraries installed in the <code>lib64/</code> subdirectory into <code>lib/</code>. In addition, a link is provided from <code>lib64/</code> to <code>lib/</code> for compatibility.</p>"},{"location":"stdenv/stdenv.chapter.html#move-systemd-user-units.sh","title":"<code>move-systemd-user-units.sh</code>","text":"<p>This setup hook moves any systemd user units installed in the <code>lib/</code> subdirectory into <code>share/</code>. In addition, a link is provided from <code>share/</code> to <code>lib/</code> for compatibility. This is needed for systemd to find user services when installed into the user profile.</p> <p>This hook only runs when compiling for Linux.</p>"},{"location":"stdenv/stdenv.chapter.html#set-source-date-epoch-to-latest.sh","title":"<code>set-source-date-epoch-to-latest.sh</code>","text":"<p>This sets <code>SOURCE_DATE_EPOCH</code> to the modification time of the most recent file.</p>"},{"location":"stdenv/stdenv.chapter.html#bintools-wrapper","title":"Bintools Wrapper and hook","text":"<p>The Bintools Wrapper wraps the binary utilities for a bunch of miscellaneous purposes. These are GNU Binutils when targeting Linux, and a mix of cctools and GNU binutils for Darwin. [The \u201cBintools\u201d name is supposed to be a compromise between \u201cBinutils\u201d and \u201ccctools\u201d not denoting any specific implementation.] Specifically, the underlying bintools package, and a C standard library (glibc or Darwin\u2019s libSystem, just for the dynamic loader) are all fed in, and dependency finding, hardening (see below), and purity checks for each are handled by the Bintools Wrapper. Packages typically depend on CC Wrapper, which in turn (at run time) depends on the Bintools Wrapper.</p> <p>The Bintools Wrapper was only just recently split off from CC Wrapper, so the division of labor is still being worked out. For example, it shouldn\u2019t care about the C standard library, but just take a derivation with the dynamic loader (which happens to be the glibc on linux). Dependency finding however is a task both wrappers will continue to need to share, and probably the most important to understand. It is currently accomplished by collecting directories of host-platform dependencies (i.e. <code>buildInputs</code> and <code>nativeBuildInputs</code>) in environment variables. The Bintools Wrapper\u2019s setup hook causes any <code>lib</code> and <code>lib64</code> subdirectories to be added to <code>NIX_LDFLAGS</code>. Since the CC Wrapper and the Bintools Wrapper use the same strategy, most of the Bintools Wrapper code is sparsely commented and refers to the CC Wrapper. But the CC Wrapper\u2019s code, by contrast, has quite lengthy comments. The Bintools Wrapper merely cites those, rather than repeating them, to avoid falling out of sync.</p> <p>A final task of the setup hook is defining a number of standard environment variables to tell build systems which executables fulfill which purpose. They are defined to just be the base name of the tools, under the assumption that the Bintools Wrapper\u2019s binaries will be on the path. Firstly, this helps poorly-written packages, e.g. ones that look for just <code>gcc</code> when <code>CC</code> isn\u2019t defined yet <code>clang</code> is to be used. Secondly, this helps packages not get confused when cross-compiling, in which case multiple Bintools Wrappers may simultaneously be in use. [^footnote-stdenv-per-platform-wrapper] <code>BUILD_</code>- and <code>TARGET_</code>-prefixed versions of the normal environment variable are defined for additional Bintools Wrappers, properly disambiguating them.</p> <p>A problem with this final task is that the Bintools Wrapper is honest and defines <code>LD</code> as <code>ld</code>. Most packages, however, firstly use the C compiler for linking, secondly use <code>LD</code> anyways, defining it as the C compiler, and thirdly, only so define <code>LD</code> when it is undefined as a fallback. This triple-threat means Bintools Wrapper will break those packages, as LD is already defined as the actual linker which the package won\u2019t override yet doesn\u2019t want to use. The workaround is to define, just for the problematic package, <code>LD</code> as the C compiler. A good way to do this would be <code>preConfigure = \"LD=$CC\"</code>.</p>"},{"location":"stdenv/stdenv.chapter.html#cc-wrapper","title":"CC Wrapper and hook","text":"<p>The CC Wrapper wraps a C toolchain for a bunch of miscellaneous purposes. Specifically, a C compiler (GCC or Clang), wrapped binary tools, and a C standard library (glibc or Darwin\u2019s libSystem, just for the dynamic loader) are all fed in, and dependency finding, hardening (see below), and purity checks for each are handled by the CC Wrapper. Packages typically depend on the CC Wrapper, which in turn (at run-time) depends on the Bintools Wrapper.</p> <p>Dependency finding is undoubtedly the main task of the CC Wrapper. This works just like the Bintools Wrapper, except that any <code>include</code> subdirectory of any relevant dependency is added to <code>NIX_CFLAGS_COMPILE</code>. The setup hook itself contains elaborate comments describing the exact mechanism by which this is accomplished.</p> <p>Similarly, the CC Wrapper follows the Bintools Wrapper in defining standard environment variables with the names of the tools it wraps, for the same reasons described above. Importantly, while it includes a <code>cc</code> symlink to the c compiler for portability, the <code>CC</code> will be defined using the compiler\u2019s \u201creal name\u201d (i.e. <code>gcc</code> or <code>clang</code>). This helps lousy build systems that inspect on the name of the compiler rather than run it.</p> <p>Here are some more packages that provide a setup hook. Since the list of hooks is extensible, this is not an exhaustive list. The mechanism is only to be used as a last resort, so it might cover most uses.</p>"},{"location":"stdenv/stdenv.chapter.html#stdenv-other-hooks","title":"Other hooks","text":"<p>Many other packages provide hooks, that are not part of <code>stdenv</code>. You can find these in the Hooks Reference.</p>"},{"location":"stdenv/stdenv.chapter.html#compiler-linker-wrapper-hooks","title":"Compiler and Linker wrapper hooks","text":"<p>If the file <code>${cc}/nix-support/cc-wrapper-hook</code> exists, it will be run at the end of the compiler wrapper. If the file <code>${binutils}/nix-support/post-link-hook</code> exists, it will be run at the end of the linker wrapper. These hooks allow a user to inject code into the wrappers. As an example, these hooks can be used to extract <code>extraBefore</code>, <code>params</code> and <code>extraAfter</code> which store all the command line arguments passed to the compiler and linker respectively.</p>"},{"location":"stdenv/stdenv.chapter.html#sec-purity-in-nixpkgs","title":"Purity in Nixpkgs","text":"<p>Measures taken to prevent dependencies on packages outside the store, and what you can do to prevent them.</p> <p>GCC doesn\u2019t search in locations such as <code>/usr/include</code>. In fact, attempts to add such directories through the <code>-I</code> flag are filtered out. Likewise, the linker (from GNU binutils) doesn\u2019t search in standard locations such as <code>/usr/lib</code>. Programs built on Linux are linked against a GNU C Library that likewise doesn\u2019t search in the default system locations.</p>"},{"location":"stdenv/stdenv.chapter.html#sec-hardening-in-nixpkgs","title":"Hardening in Nixpkgs","text":"<p>There are flags available to harden packages at compile or link-time. These can be toggled using the <code>stdenv.mkDerivation</code> parameters <code>hardeningDisable</code> and <code>hardeningEnable</code>.</p> <p>Both parameters take a list of flags as strings. The special <code>\"all\"</code> flag can be passed to <code>hardeningDisable</code> to turn off all hardening. These flags can also be used as environment variables for testing or development purposes.</p> <p>For more in-depth information on these hardening flags and hardening in general, refer to the Debian Wiki, Ubuntu Wiki, Gentoo Wiki, and the Arch Wiki.</p>"},{"location":"stdenv/stdenv.chapter.html#sec-hardening-flags-enabled-by-default","title":"Hardening flags enabled by default","text":"<p>The following flags are enabled by default and might require disabling with <code>hardeningDisable</code> if the program to package is incompatible.</p>"},{"location":"stdenv/stdenv.chapter.html#format","title":"<code>format</code>","text":"<p>Adds the <code>-Wformat -Wformat-security -Werror=format-security</code> compiler options. At present, this warns about calls to <code>printf</code> and <code>scanf</code> functions where the format string is not a string literal and there are no format arguments, as in <code>printf(foo);</code>. This may be a security hole if the format string came from untrusted input and contains <code>%n</code>.</p> <p>This needs to be turned off or fixed for errors similar to:</p> <pre><code>/tmp/nix-build-zynaddsubfx-2.5.2.drv-0/zynaddsubfx-2.5.2/src/UI/guimain.cpp:571:28: error: format not a string literal and no format arguments [-Werror=format-security]\n         printf(help_message);\n                            ^\ncc1plus: some warnings being treated as errors\n</code></pre>"},{"location":"stdenv/stdenv.chapter.html#stackprotector","title":"<code>stackprotector</code>","text":"<p>Adds the <code>-fstack-protector-strong --param ssp-buffer-size=4</code> compiler options. This adds safety checks against stack overwrites rendering many potential code injection attacks into aborting situations. In the best case this turns code injection vulnerabilities into denial of service or into non-issues (depending on the application).</p> <p>This needs to be turned off or fixed for errors similar to:</p> <pre><code>bin/blib.a(bios_console.o): In function `bios_handle_cup':\n/tmp/nix-build-ipxe-20141124-5cbdc41.drv-0/ipxe-5cbdc41/src/arch/i386/firmware/pcbios/bios_console.c:86: undefined reference to `__stack_chk_fail'\n</code></pre>"},{"location":"stdenv/stdenv.chapter.html#fortify","title":"<code>fortify</code>","text":"<p>Adds the <code>-O2 -D_FORTIFY_SOURCE=2</code> compiler options. During code generation the compiler knows a great deal of information about buffer sizes (where possible), and attempts to replace insecure unlimited length buffer function calls with length-limited ones. This is especially useful for old, crufty code. Additionally, format strings in writable memory that contain <code>%n</code> are blocked. If an application depends on such a format string, it will need to be worked around.</p> <p>Additionally, some warnings are enabled which might trigger build failures if compiler warnings are treated as errors in the package build. In this case, set <code>env.NIX_CFLAGS_COMPILE</code> to <code>-Wno-error=warning-type</code>.</p> <p>This needs to be turned off or fixed for errors similar to:</p> <pre><code>malloc.c:404:15: error: return type is an incomplete type\nmalloc.c:410:19: error: storage size of 'ms' isn't known\n\nstrdup.h:22:1: error: expected identifier or '(' before '__extension__'\n\nstrsep.c:65:23: error: register name not specified for 'delim'\n\ninstallwatch.c:3751:5: error: conflicting types for '__open_2'\n\nfcntl2.h:50:4: error: call to '__open_missing_mode' declared with attribute error: open with O_CREAT or O_TMPFILE in second argument needs 3 arguments\n</code></pre>"},{"location":"stdenv/stdenv.chapter.html#pic","title":"<code>pic</code>","text":"<p>Adds the <code>-fPIC</code> compiler options. This options adds support for position independent code in shared libraries and thus making ASLR possible.</p> <p>Most notably, the Linux kernel, kernel modules and other code not running in an operating system environment like boot loaders won\u2019t build with PIC enabled. The compiler will is most cases complain that PIC is not supported for a specific build.</p> <p>This needs to be turned off or fixed for assembler errors similar to:</p> <pre><code>ccbLfRgg.s: Assembler messages:\nccbLfRgg.s:33: Error: missing or invalid displacement expression `private_key_len@GOTOFF'\n</code></pre>"},{"location":"stdenv/stdenv.chapter.html#strictoverflow","title":"<code>strictoverflow</code>","text":"<p>Signed integer overflow is undefined behaviour according to the C standard. If it happens, it is an error in the program as it should check for overflow before it can happen, not afterwards. GCC provides built-in functions to perform arithmetic with overflow checking, which are correct and faster than any custom implementation. As a workaround, the option <code>-fno-strict-overflow</code> makes gcc behave as if signed integer overflows were defined.</p> <p>This flag should not trigger any build or runtime errors.</p>"},{"location":"stdenv/stdenv.chapter.html#relro","title":"<code>relro</code>","text":"<p>Adds the <code>-z relro</code> linker option. During program load, several ELF memory sections need to be written to by the linker, but can be turned read-only before turning over control to the program. This prevents some GOT (and .dtors) overwrite attacks, but at least the part of the GOT used by the dynamic linker (.got.plt) is still vulnerable.</p> <p>This flag can break dynamic shared object loading. For instance, the module systems of Xorg and OpenCV are incompatible with this flag. In almost all cases the <code>bindnow</code> flag must also be disabled and incompatible programs typically fail with similar errors at runtime.</p>"},{"location":"stdenv/stdenv.chapter.html#bindnow","title":"<code>bindnow</code>","text":"<p>Adds the <code>-z now</code> linker option. During program load, all dynamic symbols are resolved, allowing for the complete GOT to be marked read-only (due to <code>relro</code>). This prevents GOT overwrite attacks. For very large applications, this can incur some performance loss during initial load while symbols are resolved, but this shouldn\u2019t be an issue for daemons.</p> <p>This flag can break dynamic shared object loading. For instance, the module systems of Xorg and PHP are incompatible with this flag. Programs incompatible with this flag often fail at runtime due to missing symbols, like:</p> <pre><code>intel_drv.so: undefined symbol: vgaHWFreeHWRec\n</code></pre>"},{"location":"stdenv/stdenv.chapter.html#sec-hardening-flags-disabled-by-default","title":"Hardening flags disabled by default","text":"<p>The following flags are disabled by default and should be enabled with <code>hardeningEnable</code> for packages that take untrusted input like network services.</p>"},{"location":"stdenv/stdenv.chapter.html#pie","title":"<code>pie</code>","text":"<p>This flag is disabled by default for normal <code>glibc</code> based NixOS package builds, but enabled by default for <code>musl</code> based package builds.</p> <p>Adds the <code>-fPIE</code> compiler and <code>-pie</code> linker options. Position Independent Executables are needed to take advantage of Address Space Layout Randomization, supported by modern kernel versions. While ASLR can already be enforced for data areas in the stack and heap (brk and mmap), the code areas must be compiled as position-independent. Shared libraries already do this with the <code>pic</code> flag, so they gain ASLR automatically, but binary .text regions need to be build with <code>pie</code> to gain ASLR. When this happens, ROP attacks are much harder since there are no static locations to bounce off of during a memory corruption attack.</p> <p>Static libraries need to be compiled with <code>-fPIE</code> so that executables can link them in with the <code>-pie</code> linker option. If the libraries lack <code>-fPIE</code>, you will get the error <code>recompile with -fPIE</code>.</p> <p>[^footnote-stdenv-ignored-build-platform]: The build platform is ignored because it is a mere implementation detail of the package satisfying the dependency: As a general programming principle, dependencies are always specified as interfaces, not concrete implementation. [^footnote-stdenv-native-dependencies-in-path]: Currently, this means for native builds all dependencies are put on the <code>PATH</code>. But in the future that may not be the case for sake of matching cross: the platforms would be assumed to be unique for native and cross builds alike, so only the <code>depsBuild*</code> and <code>nativeBuildInputs</code> would be added to the <code>PATH</code>. [^footnote-stdenv-propagated-dependencies]: Nix itself already takes a package\u2019s transitive dependencies into account, but this propagation ensures nixpkgs-specific infrastructure like setup hooks also are run as if it were a propagated dependency. [^footnote-stdenv-find-inputs-location]: The <code>findInputs</code> function, currently residing in <code>pkgs/stdenv/generic/setup.sh</code>, implements the propagation logic. [^footnote-stdenv-sys-lib-search-path]: It clears the <code>sys_lib_*search_path</code> variables in the Libtool script to prevent Libtool from using libraries in <code>/usr/lib</code> and such. [^footnote-stdenv-build-time-guessing-impurity]: Eventually these will be passed building natively as well, to improve determinism: build-time guessing, as is done today, is a risk of impurity. [^footnote-stdenv-per-platform-wrapper]: Each wrapper targets a single platform, so if binaries for multiple platforms are needed, the underlying binaries must be wrapped multiple times. As this is a property of the wrapper itself, the multiple wrappings are needed whether or not the same underlying binaries can target multiple platforms.</p>"},{"location":"using/configuration.chapter.html","title":"Global configuration","text":"<p>Nix comes with certain defaults about what packages can and cannot be installed, based on a package's metadata. By default, Nix will prevent installation if any of the following criteria are true:</p> <ul> <li> <p>The package is thought to be broken, and has had its <code>meta.broken</code> set to <code>true</code>.</p> </li> <li> <p>The package isn't intended to run on the given system, as none of its <code>meta.platforms</code> match the given system.</p> </li> <li> <p>The package's <code>meta.license</code> is set to a license which is considered to be unfree.</p> </li> <li> <p>The package has known security vulnerabilities but has not or can not be updated for some reason, and a list of issues has been entered in to the package's <code>meta.knownVulnerabilities</code>.</p> </li> </ul> <p>Note that all this is checked during evaluation already, and the check includes any package that is evaluated. In particular, all build-time dependencies are checked. <code>nix-env -qa</code> will (attempt to) hide any packages that would be refused.</p> <p>Each of these criteria can be altered in the nixpkgs configuration.</p> <p>The nixpkgs configuration for a NixOS system is set in the <code>configuration.nix</code>, as in the following example:</p> <pre><code>{\n  nixpkgs.config = {\n    allowUnfree = true;\n  };\n}\n</code></pre> <p>However, this does not allow unfree software for individual users. Their configurations are managed separately.</p> <p>A user's nixpkgs configuration is stored in a user-specific configuration file located at <code>~/.config/nixpkgs/config.nix</code>. For example:</p> <pre><code>{\n  allowUnfree = true;\n}\n</code></pre> <p>Note that we are not able to test or build unfree software on Hydra due to policy. Most unfree licenses prohibit us from either executing or distributing the software.</p>"},{"location":"using/configuration.chapter.html#sec-allow-broken","title":"Installing broken packages","text":"<p>There are two ways to try compiling a package which has been marked as broken.</p> <ul> <li> <p>For allowing the build of a broken package once, you can use an environment variable for a single invocation of the nix tools:</p> <pre><code>$ export NIXPKGS_ALLOW_BROKEN=1\n</code></pre> </li> <li> <p>For permanently allowing broken packages to be built, you may add <code>allowBroken = true;</code> to your user's configuration file, like this:</p> <pre><code>{\n  allowBroken = true;\n}\n</code></pre> </li> </ul>"},{"location":"using/configuration.chapter.html#sec-allow-unsupported-system","title":"Installing packages on unsupported systems","text":"<p>There are also two ways to try compiling a package which has been marked as unsupported for the given system.</p> <ul> <li> <p>For allowing the build of an unsupported package once, you can use an environment variable for a single invocation of the nix tools:</p> <pre><code>$ export NIXPKGS_ALLOW_UNSUPPORTED_SYSTEM=1\n</code></pre> </li> <li> <p>For permanently allowing unsupported packages to be built, you may add <code>allowUnsupportedSystem = true;</code> to your user's configuration file, like this:</p> <pre><code>{\n  allowUnsupportedSystem = true;\n}\n</code></pre> </li> </ul> <p>The difference between a package being unsupported on some system and being broken is admittedly a bit fuzzy. If a program ought to work on a certain platform, but doesn't, the platform should be included in <code>meta.platforms</code>, but marked as broken with e.g.  <code>meta.broken = !hostPlatform.isWindows</code>. Of course, this begs the question of what \"ought\" means exactly. That is left to the package maintainer.</p>"},{"location":"using/configuration.chapter.html#sec-allow-unfree","title":"Installing unfree packages","text":"<p>All users of Nixpkgs are free software users, and many users (and developers) of Nixpkgs want to limit and tightly control their exposure to unfree software. At the same time, many users need (or want) to run some specific pieces of proprietary software. Nixpkgs includes some expressions for unfree software packages. By default unfree software cannot be installed and doesn\u2019t show up in searches.</p> <p>There are several ways to tweak how Nix handles a package which has been marked as unfree.</p> <ul> <li> <p>To temporarily allow all unfree packages, you can use an environment variable for a single invocation of the nix tools:</p> <pre><code>$ export NIXPKGS_ALLOW_UNFREE=1\n</code></pre> </li> <li> <p>It is possible to permanently allow individual unfree packages, while still blocking unfree packages by default using the <code>allowUnfreePredicate</code> configuration option in the user configuration file.</p> <p>This option is a function which accepts a package as a parameter, and returns a boolean. The following example configuration accepts a package and always returns false:</p> <pre><code>{\n  allowUnfreePredicate = (pkg: false);\n}\n</code></pre> <p>For a more useful example, try the following. This configuration only allows unfree packages named roon-server and visual studio code:</p> <pre><code>{\n  allowUnfreePredicate = pkg: builtins.elem (lib.getName pkg) [\n    \"roon-server\"\n    \"vscode\"\n  ];\n}\n</code></pre> </li> <li> <p>It is also possible to allow and block licenses that are specifically acceptable or not acceptable, using <code>allowlistedLicenses</code> and <code>blocklistedLicenses</code>, respectively.</p> <p>The following example configuration allowlists the licenses <code>amd</code> and <code>wtfpl</code>:</p> <pre><code>{\n  allowlistedLicenses = with lib.licenses; [ amd wtfpl ];\n}\n</code></pre> <p>The following example configuration blocklists the <code>gpl3Only</code> and <code>agpl3Only</code> licenses:</p> <pre><code>{\n  blocklistedLicenses = with lib.licenses; [ agpl3Only gpl3Only ];\n}\n</code></pre> <p>Note that <code>allowlistedLicenses</code> only applies to unfree licenses unless <code>allowUnfree</code> is enabled. It is not a generic allowlist for all types of licenses. <code>blocklistedLicenses</code> applies to all licenses.</p> </li> </ul> <p>A complete list of licenses can be found in the file <code>lib/licenses.nix</code> of the nixpkgs tree.</p>"},{"location":"using/configuration.chapter.html#sec-allow-insecure","title":"Installing insecure packages","text":"<p>There are several ways to tweak how Nix handles a package which has been marked as insecure.</p> <ul> <li> <p>To temporarily allow all insecure packages, you can use an environment variable for a single invocation of the nix tools:</p> <pre><code>$ export NIXPKGS_ALLOW_INSECURE=1\n</code></pre> </li> <li> <p>It is possible to permanently allow individual insecure packages, while still blocking other insecure packages by default using the <code>permittedInsecurePackages</code> configuration option in the user configuration file.</p> <p>The following example configuration permits the installation of the hypothetically insecure package <code>hello</code>, version <code>1.2.3</code>:</p> <pre><code>{\n  permittedInsecurePackages = [\n    \"hello-1.2.3\"\n  ];\n}\n</code></pre> </li> <li> <p>It is also possible to create a custom policy around which insecure packages to allow and deny, by overriding the <code>allowInsecurePredicate</code> configuration option.</p> <p>The <code>allowInsecurePredicate</code> option is a function which accepts a package and returns a boolean, much like <code>allowUnfreePredicate</code>.</p> <p>The following configuration example only allows insecure packages with very short names:</p> <pre><code>{\n  allowInsecurePredicate = pkg: builtins.stringLength (lib.getName pkg) &lt;= 5;\n}\n</code></pre> <p>Note that <code>permittedInsecurePackages</code> is only checked if <code>allowInsecurePredicate</code> is not specified.</p> </li> </ul>"},{"location":"using/configuration.chapter.html#sec-modify-via-packageOverrides","title":"Modify packages via <code>packageOverrides</code>","text":"<p>You can define a function called <code>packageOverrides</code> in your local <code>~/.config/nixpkgs/config.nix</code> to override Nix packages. It must be a function that takes pkgs as an argument and returns a modified set of packages.</p> <pre><code>{\n  packageOverrides = pkgs: rec {\n    foo = pkgs.foo.override { ... };\n  };\n}\n</code></pre>"},{"location":"using/configuration.chapter.html#sec-config-options-reference","title":"<code>config</code> Options Reference","text":"<p>The following attributes can be passed in <code>config</code>.</p> <p>```{=include=} options id-prefix: opt- list-id: configuration-variable-list source: ../config-options.json <pre><code>## Declarative Package Management {#sec-declarative-package-management}\n\n### Build an environment {#sec-building-environment}\n\nUsing `packageOverrides`, it is possible to manage packages declaratively. This means that we can list all of our desired packages within a declarative Nix expression. For example, to have `aspell`, `bc`, `ffmpeg`, `coreutils`, `gdb`, `nixUnstable`, `emscripten`, `jq`, `nox`, and `silver-searcher`, we could use the following in `~/.config/nixpkgs/config.nix`:\n\n```nix\n{\n  packageOverrides = pkgs: with pkgs; {\n    myPackages = pkgs.buildEnv {\n      name = \"my-packages\";\n      paths = [\n        aspell\n        bc\n        coreutils\n        gdb\n        ffmpeg\n        nixUnstable\n        emscripten\n        jq\n        nox\n        silver-searcher\n      ];\n    };\n  };\n}\n</code></pre></p> <p>To install it into our environment, you can just run <code>nix-env -iA nixpkgs.myPackages</code>. If you want to load the packages to be built from a working copy of <code>nixpkgs</code> you just run <code>nix-env -f. -iA myPackages</code>. To explore what's been installed, just look through <code>~/.nix-profile/</code>. You can see that a lot of stuff has been installed. Some of this stuff is useful some of it isn't. Let's tell Nixpkgs to only link the stuff that we want:</p> <pre><code>{\n  packageOverrides = pkgs: with pkgs; {\n    myPackages = pkgs.buildEnv {\n      name = \"my-packages\";\n      paths = [\n        aspell\n        bc\n        coreutils\n        gdb\n        ffmpeg\n        nixUnstable\n        emscripten\n        jq\n        nox\n        silver-searcher\n      ];\n      pathsToLink = [ \"/share\" \"/bin\" ];\n    };\n  };\n}\n</code></pre> <p><code>pathsToLink</code> tells Nixpkgs to only link the paths listed which gets rid of the extra stuff in the profile. <code>/bin</code> and <code>/share</code> are good defaults for a user environment, getting rid of the clutter. If you are running on Nix on MacOS, you may want to add another path as well, <code>/Applications</code>, that makes GUI apps available.</p>"},{"location":"using/configuration.chapter.html#sec-getting-documentation","title":"Getting documentation","text":"<p>After building that new environment, look through <code>~/.nix-profile</code> to make sure everything is there that we wanted. Discerning readers will note that some files are missing. Look inside <code>~/.nix-profile/share/man/man1/</code> to verify this. There are no man pages for any of the Nix tools! This is because some packages like Nix have multiple outputs for things like documentation (see section 4). Let's make Nix install those as well.</p> <pre><code>{\n  packageOverrides = pkgs: with pkgs; {\n    myPackages = pkgs.buildEnv {\n      name = \"my-packages\";\n      paths = [\n        aspell\n        bc\n        coreutils\n        ffmpeg\n        nixUnstable\n        emscripten\n        jq\n        nox\n        silver-searcher\n      ];\n      pathsToLink = [ \"/share/man\" \"/share/doc\" \"/bin\" ];\n      extraOutputsToInstall = [ \"man\" \"doc\" ];\n    };\n  };\n}\n</code></pre> <p>This provides us with some useful documentation for using our packages.  However, if we actually want those manpages to be detected by man, we need to set up our environment. This can also be managed within Nix expressions.</p> <pre><code>{\n  packageOverrides = pkgs: with pkgs; rec {\n    myProfile = writeText \"my-profile\" ''\n      export PATH=$HOME/.nix-profile/bin:/nix/var/nix/profiles/default/bin:/sbin:/bin:/usr/sbin:/usr/bin\n      export MANPATH=$HOME/.nix-profile/share/man:/nix/var/nix/profiles/default/share/man:/usr/share/man\n    '';\n    myPackages = pkgs.buildEnv {\n      name = \"my-packages\";\n      paths = [\n        (runCommand \"profile\" {} ''\n          mkdir -p $out/etc/profile.d\n          cp ${myProfile} $out/etc/profile.d/my-profile.sh\n        '')\n        aspell\n        bc\n        coreutils\n        ffmpeg\n        man\n        nixUnstable\n        emscripten\n        jq\n        nox\n        silver-searcher\n      ];\n      pathsToLink = [ \"/share/man\" \"/share/doc\" \"/bin\" \"/etc\" ];\n      extraOutputsToInstall = [ \"man\" \"doc\" ];\n    };\n  };\n}\n</code></pre> <p>For this to work fully, you must also have this script sourced when you are logged in. Try adding something like this to your <code>~/.profile</code> file:</p> <pre><code>#!/bin/sh\nif [ -d \"${HOME}/.nix-profile/etc/profile.d\" ]; then\n  for i in \"${HOME}/.nix-profile/etc/profile.d/\"*.sh; do\n    if [ -r \"$i\" ]; then\n      . \"$i\"\n    fi\n  done\nfi\n</code></pre> <p>Now just run <code>. \"${HOME}/.profile\"</code> and you can start loading man pages from your environment.</p>"},{"location":"using/configuration.chapter.html#sec-gnu-info-setup","title":"GNU info setup","text":"<p>Configuring GNU info is a little bit trickier than man pages. To work correctly, info needs a database to be generated. This can be done with some small modifications to our environment scripts.</p> <pre><code>{\n  packageOverrides = pkgs: with pkgs; rec {\n    myProfile = writeText \"my-profile\" ''\n      export PATH=$HOME/.nix-profile/bin:/nix/var/nix/profiles/default/bin:/sbin:/bin:/usr/sbin:/usr/bin\n      export MANPATH=$HOME/.nix-profile/share/man:/nix/var/nix/profiles/default/share/man:/usr/share/man\n      export INFOPATH=$HOME/.nix-profile/share/info:/nix/var/nix/profiles/default/share/info:/usr/share/info\n    '';\n    myPackages = pkgs.buildEnv {\n      name = \"my-packages\";\n      paths = [\n        (runCommand \"profile\" {} ''\n          mkdir -p $out/etc/profile.d\n          cp ${myProfile} $out/etc/profile.d/my-profile.sh\n        '')\n        aspell\n        bc\n        coreutils\n        ffmpeg\n        man\n        nixUnstable\n        emscripten\n        jq\n        nox\n        silver-searcher\n        texinfoInteractive\n      ];\n      pathsToLink = [ \"/share/man\" \"/share/doc\" \"/share/info\" \"/bin\" \"/etc\" ];\n      extraOutputsToInstall = [ \"man\" \"doc\" \"info\" ];\n      postBuild = ''\n        if [ -x $out/bin/install-info -a -w $out/share/info ]; then\n          shopt -s nullglob\n          for i in $out/share/info/*.info $out/share/info/*.info.gz; do\n              $out/bin/install-info $i $out/share/info/dir\n          done\n        fi\n      '';\n    };\n  };\n}\n</code></pre> <p><code>postBuild</code> tells Nixpkgs to run a command after building the environment. In this case, <code>install-info</code> adds the installed info pages to <code>dir</code> which is GNU info's default root node. Note that <code>texinfoInteractive</code> is added to the environment to give the <code>install-info</code> command.</p>"},{"location":"using/overlays.chapter.html","title":"Overlays","text":"<p>This chapter describes how to extend and change Nixpkgs using overlays.  Overlays are used to add layers in the fixed-point used by Nixpkgs to compose the set of all packages.</p> <p>Nixpkgs can be configured with a list of overlays, which are applied in order. This means that the order of the overlays can be significant if multiple layers override the same package.</p>"},{"location":"using/overlays.chapter.html#sec-overlays-install","title":"Installing overlays","text":"<p>The list of overlays can be set either explicitly in a Nix expression, or through <code>&lt;nixpkgs-overlays&gt;</code> or user configuration files.</p>"},{"location":"using/overlays.chapter.html#sec-overlays-argument","title":"Set overlays in NixOS or Nix expressions","text":"<p>On a NixOS system the value of the <code>nixpkgs.overlays</code> option, if present, is passed to the system Nixpkgs directly as an argument. Note that this does not affect the overlays for non-NixOS operations (e.g.  <code>nix-env</code>), which are looked up independently.</p> <p>The list of overlays can be passed explicitly when importing nixpkgs, for example <code>import &lt;nixpkgs&gt; { overlays = [ overlay1 overlay2 ]; }</code>.</p> <p>NOTE: DO NOT USE THIS in nixpkgs. Further overlays can be added by calling the <code>pkgs.extend</code> or <code>pkgs.appendOverlays</code>, although it is often preferable to avoid these functions, because they recompute the Nixpkgs fixpoint, which is somewhat expensive to do.</p>"},{"location":"using/overlays.chapter.html#sec-overlays-lookup","title":"Install overlays via configuration lookup","text":"<p>The list of overlays is determined as follows.</p> <ol> <li> <p>First, if an <code>overlays</code> argument to the Nixpkgs function itself is given, then that is used and no path lookup will be performed.</p> </li> <li> <p>Otherwise, if the Nix path entry <code>&lt;nixpkgs-overlays&gt;</code> exists, we look for overlays at that path, as described below.</p> <p>See the section on <code>NIX_PATH</code> in the Nix manual for more details on how to set a value for <code>&lt;nixpkgs-overlays&gt;.</code></p> </li> <li> <p>If one of <code>~/.config/nixpkgs/overlays.nix</code> and <code>~/.config/nixpkgs/overlays/</code> exists, then we look for overlays at that path, as described below. It is an error if both exist.</p> </li> </ol> <p>If we are looking for overlays at a path, then there are two cases:</p> <ul> <li> <p>If the path is a file, then the file is imported as a Nix expression and used as the list of overlays.</p> </li> <li> <p>If the path is a directory, then we take the content of the directory, order it lexicographically, and attempt to interpret each as an overlay by:</p> <ul> <li> <p>Importing the file, if it is a <code>.nix</code> file.</p> </li> <li> <p>Importing a top-level <code>default.nix</code> file, if it is a directory.</p> </li> </ul> </li> </ul> <p>Because overlays that are set in NixOS configuration do not affect non-NixOS operations such as <code>nix-env</code>, the <code>overlays.nix</code> option provides a convenient way to use the same overlays for a NixOS system configuration and user configuration: the same file can be used as <code>overlays.nix</code> and imported as the value of <code>nixpkgs.overlays</code>.</p>"},{"location":"using/overlays.chapter.html#sec-overlays-definition","title":"Defining overlays","text":"<p>Overlays are Nix functions which accept two arguments, conventionally called <code>self</code> and <code>super</code>, and return a set of packages. For example, the following is a valid overlay.</p> <pre><code>self: super:\n\n{\n  boost = super.boost.override {\n    python = self.python3;\n  };\n  rr = super.callPackage ./pkgs/rr {\n    stdenv = self.stdenv_32bit;\n  };\n}\n</code></pre> <p>The first argument (<code>self</code>) corresponds to the final package set. You should use this set for the dependencies of all packages specified in your overlay. For example, all the dependencies of <code>rr</code> in the example above come from <code>self</code>, as well as the overridden dependencies used in the <code>boost</code> override.</p> <p>The second argument (<code>super</code>) corresponds to the result of the evaluation of the previous stages of Nixpkgs. It does not contain any of the packages added by the current overlay, nor any of the following overlays. This set should be used either to refer to packages you wish to override, or to access functions defined in Nixpkgs. For example, the original recipe of <code>boost</code> in the above example, comes from <code>super</code>, as well as the <code>callPackage</code> function.</p> <p>The value returned by this function should be a set similar to <code>pkgs/top-level/all-packages.nix</code>, containing overridden and/or new packages.</p> <p>Overlays are similar to other methods for customizing Nixpkgs, in particular the <code>packageOverrides</code> attribute described in . Indeed, <code>packageOverrides</code> acts as an overlay with only the <code>super</code> argument. It is therefore appropriate for basic use, but overlays are more powerful and easier to distribute.</p>"},{"location":"using/overlays.chapter.html#sec-overlays-alternatives","title":"Using overlays to configure alternatives","text":"<p>Certain software packages have different implementations of the same interface. Other distributions have functionality to switch between these. For example, Debian provides DebianAlternatives.  Nixpkgs has what we call <code>alternatives</code>, which are configured through overlays.</p>"},{"location":"using/overlays.chapter.html#sec-overlays-alternatives-blas-lapack","title":"BLAS/LAPACK","text":"<p>In Nixpkgs, we have multiple implementations of the BLAS/LAPACK numerical linear algebra interfaces. They are:</p> <ul> <li> <p>OpenBLAS</p> <p>The Nixpkgs attribute is <code>openblas</code> for ILP64 (integer width = 64 bits) and <code>openblasCompat</code> for LP64 (integer width = 32 bits).  <code>openblasCompat</code> is the default.</p> </li> <li> <p>LAPACK reference (also provides BLAS and CBLAS)</p> <p>The Nixpkgs attribute is <code>lapack-reference</code>.</p> </li> <li> <p>Intel MKL (only works on the x86_64 architecture, unfree)</p> <p>The Nixpkgs attribute is <code>mkl</code>.</p> </li> <li> <p>BLIS</p> <p>BLIS, available through the attribute <code>blis</code>, is a framework for linear algebra kernels. In addition, it implements the BLAS interface.</p> </li> <li> <p>AMD BLIS/LIBFLAME (optimized for modern AMD x86_64 CPUs)</p> <p>The AMD fork of the BLIS library, with attribute <code>amd-blis</code>, extends BLIS with optimizations for modern AMD CPUs. The changes are usually submitted to the upstream BLIS project after some time. However, AMD BLIS typically provides some performance improvements on AMD Zen CPUs. The complementary AMD LIBFLAME library, with attribute <code>amd-libflame</code>, provides a LAPACK implementation.</p> </li> </ul> <p>Introduced in PR #83888, we are able to override the <code>blas</code> and <code>lapack</code> packages to use different implementations, through the <code>blasProvider</code> and <code>lapackProvider</code> argument. This can be used to select a different provider. BLAS providers will have symlinks in <code>$out/lib/libblas.so.3</code> and <code>$out/lib/libcblas.so.3</code> to their respective BLAS libraries.  Likewise, LAPACK providers will have symlinks in <code>$out/lib/liblapack.so.3</code> and <code>$out/lib/liblapacke.so.3</code> to their respective LAPACK libraries. For example, Intel MKL is both a BLAS and LAPACK provider. An overlay can be created to use Intel MKL that looks like:</p> <pre><code>self: super:\n\n{\n  blas = super.blas.override {\n    blasProvider = self.mkl;\n  };\n\n  lapack = super.lapack.override {\n    lapackProvider = self.mkl;\n  };\n}\n</code></pre> <p>This overlay uses Intel's MKL library for both BLAS and LAPACK interfaces. Note that the same can be accomplished at runtime using <code>LD_LIBRARY_PATH</code> of <code>libblas.so.3</code> and <code>liblapack.so.3</code>. For instance:</p> <pre><code>$ LD_LIBRARY_PATH=$(nix-build -A mkl)/lib${LD_LIBRARY_PATH:+:}$LD_LIBRARY_PATH nix-shell -p octave --run octave\n</code></pre> <p>Intel MKL requires an <code>openmp</code> implementation when running with multiple processors. By default, <code>mkl</code> will use Intel's <code>iomp</code> implementation if no other is specified, but this is a runtime-only dependency and binary compatible with the LLVM implementation. To use that one instead, Intel recommends users set it with <code>LD_PRELOAD</code>. Note that <code>mkl</code> is only available on <code>x86_64-linux</code> and <code>x86_64-darwin</code>. Moreover, Hydra is not building and distributing pre-compiled binaries using it.</p> <p>To override <code>blas</code> and <code>lapack</code> with its reference implementations (i.e. for development purposes), one can use the following overlay:</p> <pre><code>self: super:\n\n{\n  blas = super.blas.override {\n    blasProvider = self.lapack-reference;\n  };\n\n  lapack = super.lapack.override {\n    lapackProvider = self.lapack-reference;\n  };\n}\n</code></pre> <p>For BLAS/LAPACK switching to work correctly, all packages must depend on <code>blas</code> or <code>lapack</code>. This ensures that only one BLAS/LAPACK library is used at one time. There are two versions of BLAS/LAPACK currently in the wild, <code>LP64</code> (integer size = 32 bits) and <code>ILP64</code> (integer size = 64 bits). The attributes <code>blas</code> and <code>lapack</code> are <code>LP64</code> by default. Their <code>ILP64</code> version are provided through the attributes <code>blas-ilp64</code> and <code>lapack-ilp64</code>. Some software needs special flags or patches to work with <code>ILP64</code>. You can check if <code>ILP64</code> is used in Nixpkgs with <code>blas.isILP64</code> and <code>lapack.isILP64</code>. Some software does NOT work with <code>ILP64</code>, and derivations need to specify an assertion to prevent this. You can prevent <code>ILP64</code> from being used with the following:</p> <pre><code>{ stdenv, blas, lapack, ... }:\n\nassert (!blas.isILP64) &amp;&amp; (!lapack.isILP64);\n\nstdenv.mkDerivation {\n  ...\n}\n</code></pre>"},{"location":"using/overlays.chapter.html#sec-overlays-alternatives-mpi","title":"Switching the MPI implementation","text":"<p>All programs that are built with MPI support use the generic attribute <code>mpi</code> as an input. At the moment Nixpkgs natively provides two different MPI implementations:</p> <ul> <li> <p>Open MPI (default), attribute name     <code>openmpi</code></p> </li> <li> <p>MPICH, attribute name <code>mpich</code></p> </li> <li> <p>MVAPICH, attribute name <code>mvapich</code></p> </li> </ul> <p>To provide MPI enabled applications that use <code>MPICH</code>, instead of the default <code>Open MPI</code>, use the following overlay:</p> <pre><code>self: super:\n\n{\n  mpi = self.mpich;\n}\n</code></pre>"},{"location":"using/overrides.chapter.html","title":"Overriding","text":"<p>Sometimes one wants to override parts of <code>nixpkgs</code>, e.g. derivation attributes, the results of derivations.</p> <p>These functions are used to make changes to packages, returning only single packages. Overlays, on the other hand, can be used to combine the overridden packages across the entire package set of Nixpkgs.</p>"},{"location":"using/overrides.chapter.html#sec-pkg-override","title":"&lt;pkg&gt;.override","text":"<p>The function <code>override</code> is usually available for all the derivations in the nixpkgs expression (<code>pkgs</code>).</p> <p>It is used to override the arguments passed to a function.</p> <p>Example usages:</p> <pre><code>pkgs.foo.override { arg1 = val1; arg2 = val2; ... }\n</code></pre> <p>It's also possible to access the previous arguments.</p> <pre><code>pkgs.foo.override (previous: { arg1 = previous.arg1; ... })\n</code></pre> <pre><code>import pkgs.path { overlays = [ (self: super: {\n  foo = super.foo.override { barSupport = true ; };\n  })]};\n</code></pre> <pre><code>mypkg = pkgs.callPackage ./mypkg.nix {\n  mydep = pkgs.mydep.override { ... };\n  }\n</code></pre> <p>In the first example, <code>pkgs.foo</code> is the result of a function call with some default arguments, usually a derivation. Using <code>pkgs.foo.override</code> will call the same function with the given new arguments.</p>"},{"location":"using/overrides.chapter.html#sec-pkg-overrideAttrs","title":"&lt;pkg&gt;.overrideAttrs","text":"<p>The function <code>overrideAttrs</code> allows overriding the attribute set passed to a <code>stdenv.mkDerivation</code> call, producing a new derivation based on the original one. This function is available on all derivations produced by the <code>stdenv.mkDerivation</code> function, which is most packages in the nixpkgs expression <code>pkgs</code>.</p> <p>Example usages:</p> <pre><code>helloBar = pkgs.hello.overrideAttrs (finalAttrs: previousAttrs: {\n  pname = previousAttrs.pname + \"-bar\";\n});\n</code></pre> <p>In the above example, \"-bar\" is appended to the pname attribute, while all other attributes will be retained from the original <code>hello</code> package.</p> <p>The argument <code>previousAttrs</code> is conventionally used to refer to the attr set originally passed to <code>stdenv.mkDerivation</code>.</p> <p>The argument <code>finalAttrs</code> refers to the final attributes passed to <code>mkDerivation</code>, plus the <code>finalPackage</code> attribute which is equal to the result of <code>mkDerivation</code> or subsequent <code>overrideAttrs</code> calls.</p> <p>If only a one-argument function is written, the argument has the meaning of <code>previousAttrs</code>.</p> <p>Function arguments can be omitted entirely if there is no need to access <code>previousAttrs</code> or <code>finalAttrs</code>.</p> <pre><code>helloWithDebug = pkgs.hello.overrideAttrs {\n  separateDebugInfo = true;\n};\n</code></pre> <p>In the above example, the <code>separateDebugInfo</code> attribute is overridden to be true, thus building debug info for <code>helloWithDebug</code>.</p> <p>::: {.note} Note that <code>separateDebugInfo</code> is processed only by the <code>stdenv.mkDerivation</code> function, not the generated, raw Nix derivation. Thus, using <code>overrideDerivation</code> will not work in this case, as it overrides only the attributes of the final derivation. It is for this reason that <code>overrideAttrs</code> should be preferred in (almost) all cases to <code>overrideDerivation</code>, i.e. to allow using <code>stdenv.mkDerivation</code> to process input arguments, as well as the fact that it is easier to use (you can use the same attribute names you see in your Nix code, instead of the ones generated (e.g. <code>buildInputs</code> vs <code>nativeBuildInputs</code>), and it involves less typing). :::</p>"},{"location":"using/overrides.chapter.html#sec-pkg-overrideDerivation","title":"&lt;pkg&gt;.overrideDerivation","text":"<p>::: {.warning} You should prefer <code>overrideAttrs</code> in almost all cases, see its documentation for the reasons why. <code>overrideDerivation</code> is not deprecated and will continue to work, but is less nice to use and does not have as many abilities as <code>overrideAttrs</code>. :::</p> <p>::: {.warning} Do not use this function in Nixpkgs as it evaluates a derivation before modifying it, which breaks package abstraction. In addition, this evaluation-per-function application incurs a performance penalty, which can become a problem if many overrides are used. It is only intended for ad-hoc customisation, such as in <code>~/.config/nixpkgs/config.nix</code>. :::</p> <p>The function <code>overrideDerivation</code> creates a new derivation based on an existing one by overriding the original's attributes with the attribute set produced by the specified function. This function is available on all derivations defined using the <code>makeOverridable</code> function. Most standard derivation-producing functions, such as <code>stdenv.mkDerivation</code>, are defined using this function, which means most packages in the nixpkgs expression, <code>pkgs</code>, have this function.</p> <p>Example usage:</p> <pre><code>mySed = pkgs.gnused.overrideDerivation (oldAttrs: {\n  name = \"sed-4.2.2-pre\";\n  src = fetchurl {\n    url = \"ftp://alpha.gnu.org/gnu/sed/sed-4.2.2-pre.tar.bz2\";\n    hash = \"sha256-MxBJRcM2rYzQYwJ5XKxhXTQByvSg5jZc5cSHEZoB2IY=\";\n  };\n  patches = [];\n});\n</code></pre> <p>In the above example, the <code>name</code>, <code>src</code>, and <code>patches</code> of the derivation will be overridden, while all other attributes will be retained from the original derivation.</p> <p>The argument <code>oldAttrs</code> is used to refer to the attribute set of the original derivation.</p> <p>::: {.note} A package's attributes are evaluated before being modified by the <code>overrideDerivation</code> function. For example, the <code>name</code> attribute reference in <code>url = \"mirror://gnu/hello/${name}.tar.gz\";</code> is filled-in before the <code>overrideDerivation</code> function modifies the attribute set. This means that overriding the <code>name</code> attribute, in this example, will not change the value of the <code>url</code> attribute. Instead, we need to override both the <code>name</code> and <code>url</code> attributes. :::</p>"},{"location":"using/overrides.chapter.html#sec-lib-makeOverridable","title":"lib.makeOverridable","text":"<p>The function <code>lib.makeOverridable</code> is used to make the result of a function easily customizable. This utility only makes sense for functions that accept an argument set and return an attribute set.</p> <p>Example usage:</p> <pre><code>f = { a, b }: { result = a+b; };\nc = lib.makeOverridable f { a = 1; b = 2; };\n</code></pre> <p>The variable <code>c</code> is the value of the <code>f</code> function applied with some default arguments. Hence the value of <code>c.result</code> is <code>3</code>, in this example.</p> <p>The variable <code>c</code> however also has some additional functions, like c.override which can be used to override the default arguments. In this example the value of <code>(c.override { a = 4; }).result</code> is 6.</p>"},{"location":"using/platform-support.chapter.html","title":"Platform Support","text":"<p>Packages receive varying degrees of support, both in terms of maintainer attention and available computation resources for continuous integration (CI).</p> <p>Below is the list of the best supported platforms:</p> <ul> <li><code>x86_64-linux</code>: Highest level of support.</li> <li><code>aarch64-linux</code>: Well supported, with most packages building successfully in CI.</li> <li><code>aarch64-darwin</code>: Receives better support than <code>x86_64-darwin</code>.</li> <li><code>x86_64-darwin</code>: Receives some support.</li> </ul> <p>There are many other platforms with varying levels of support. The provisional platform list in Appendix A of RFC046, while not up to date, can be used as guidance.</p> <p>A more formal definition of the platform support tiers is provided in RFC046, but has not been fully implemented yet.</p>"}]}